{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jun 24 2024 15:23:59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.HoverAviary import HoverAviary\n",
    "from gym_pybullet_drones.envs.MultiHoverAviary import MultiHoverAviary\n",
    "from gym_pybullet_drones.utils.utils import sync, str2bool\n",
    "from gym_pybullet_drones.utils.enums import ObservationType, ActionType, Physics\n",
    "\n",
    "from policies import GaussianMLPPolicy\n",
    "from server import Federated_RL\n",
    "\n",
    "DEFAULT_GUI = True\n",
    "DEFAULT_RECORD_VIDEO = True\n",
    "DEFAULT_OUTPUT_FOLDER = 'results'\n",
    "DEFAULT_COLAB = False\n",
    "DEFAULT_DYNAMICS = Physics('pyb') # pyb: Pybullet dynamics; dyn: Explicit Dynamics specified in BaseAviary.py\n",
    "DEFAULT_WIND = np.array([0, 0.05, 0]) # units are in induced newtons\n",
    "DEFAULT_OBS = ObservationType('kin') # 'kin' or 'rgb'\n",
    "DEFAULT_ACT = ActionType('rpm') # 'rpm' or 'pid' or 'vel' or 'one_d_rpm' or 'one_d_pid'\n",
    "DEFAULT_AGENTS = 2\n",
    "DEFAULT_MA = False\n",
    "\n",
    "DR = True\n",
    "MASS_RANGE = [0.027, 0.042] # Maximum recommended payload is 15g\n",
    "WIND_RANGE = 0.005 # Inspired by literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 72\n",
      "Action size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "algorithms = ['FedSVRPG-M', 'PPO', 'SAC', 'TD3']\n",
    "num_agents = len(algorithms)\n",
    "envs = [HoverAviary for _ in range(num_agents)]\n",
    "env_kwargs = [dict(obs = DEFAULT_OBS, act = DEFAULT_ACT) for _ in range(num_agents)]\n",
    "agent_names = algorithms\n",
    "if DR == True:\n",
    "    domain_randomizations = [DR for _ in range(num_agents)]\n",
    "    DR_episode_thresholds = [.5 for _ in range(num_agents)] # Probability of DR at each episode\n",
    "    DR_step_thresholds = [.3 for _ in range(num_agents)] # If DR episode, probability of wind at each step\n",
    "\n",
    "mass_ranges = [MASS_RANGE for _ in range(num_agents)]\n",
    "wind_ranges = [WIND_RANGE for _ in range(num_agents)]\n",
    "env_example = HoverAviary(**env_kwargs[0])\n",
    "# Get the state size\n",
    "state_space = env_example.observation_space\n",
    "state_size = state_space.shape[1]\n",
    "# Get the action size\n",
    "action_space = env_example.action_space\n",
    "action_size = action_space.shape[1]\n",
    "\n",
    "layers = [512, 512, 256, 128]\n",
    "value_layers = [32, 32]\n",
    "# Maintain consistent network structures\n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=dict(pi=layers, qf=value_layers))\n",
    "\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Action size:\", action_size)\n",
    "\n",
    "policy = GaussianMLPPolicy(input_size=state_size, output_size=action_size, hidden_layers=layers) # Will need some smarter way to initialize the policy within the model in the future\n",
    "# ASSUMING ONE ALGORITHM SO FAR. WILL IMPLEMENT GENERAL STRUCTURE FOR DIVERSIFIED ALGORITHMS LATER\n",
    "\n",
    "#### Train the model #######################################\n",
    "model = Federated_RL(policy = policy,\n",
    "                     envs = envs,\n",
    "                     env_kwargs = env_kwargs,\n",
    "                     num_agents = num_agents,\n",
    "                     global_iterations = 50,\n",
    "                     state_size = state_size,\n",
    "                     action_size = action_size,\n",
    "                     local_step_size = 1e-3,\n",
    "                     policy_kwargs = policy_kwargs,\n",
    "                     critic_net_aggregation = True,\n",
    "                     critic_net = value_layers,\n",
    "                     local_iterations = 50,\n",
    "                     max_episode_length=2048,\n",
    "                     agent_names = agent_names,\n",
    "                     DR = domain_randomizations,\n",
    "                     DR_episode_th = DR_episode_thresholds,\n",
    "                     DR_step_th = DR_step_thresholds,\n",
    "                     mass_ranges = mass_ranges,\n",
    "                     wind_ranges = wind_ranges,\n",
    "                     algorithms = algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034346594379458965\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 42.76640574785146\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 79.25004502368917\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 68.0884672993511\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02934030353653905\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 24.480286385966796\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 22.641294199484406\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 43.74493176217848\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03135181622683355\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 61.99607444843282\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031789006280130844\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 79.083302772741\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 22.983543846878437\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 56.082279544317764\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037627496556653824\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 10\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m serverModel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/server.py:366\u001b[0m, in \u001b[0;36mFederated_RL.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_models()\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_iterations):\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/server.py:120\u001b[0m, in \u001b[0;36mFederated_RL.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining agent\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classification \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNonSB3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     episode_mean_rewards \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_rewards()\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_rewards\u001b[38;5;241m.\u001b[39mappend(episode_mean_rewards)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/algorithms/fedsvrpg_m.py:174\u001b[0m, in \u001b[0;36mFEDSVRPG_M.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLOBAL ITERATION: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_iter_num))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCAL ITERATION: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_rollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_trajectory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_rollout_buffer, policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_policy, display \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_rollout_buffer\u001b[38;5;241m.\u001b[39mreturns[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/algorithms/fedsvrpg_m.py:134\u001b[0m, in \u001b[0;36mFEDSVRPG_M.sample_trajectory\u001b[0;34m(self, rollout_buffer, policy, display)\u001b[0m\n\u001b[1;32m    132\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m    133\u001b[0m action, mean, std \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mget_action(state_tensor)\n\u001b[0;32m--> 134\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m action_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(log_prob\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    136\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/policies.py:40\u001b[0m, in \u001b[0;36mGaussianMLPPolicy.get_log_prob\u001b[0;34m(self, x, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m mu, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     39\u001b[0m std \u001b[38;5;241m=\u001b[39m std \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m  \u001b[38;5;66;03m# Adding small value to std to prevent division by zero\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(action) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(action\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(mu\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m     43\u001b[0m     mu \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/torch/fx/traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/traceback.py:232\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 232\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/traceback.py:395\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/traceback.py:423\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    420\u001b[0m filename \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_filename\n\u001b[1;32m    421\u001b[0m name \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_name\n\u001b[0;32m--> 423\u001b[0m \u001b[43mfnames\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m linecache\u001b[38;5;241m.\u001b[39mlazycache(filename, f\u001b[38;5;241m.\u001b[39mf_globals)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Must defer line lookups until we have called checkcache.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "serverModel = model.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
