{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.HoverAviary import HoverAviary\n",
    "from gym_pybullet_drones.envs.MultiHoverAviary import MultiHoverAviary\n",
    "from gym_pybullet_drones.utils.utils import sync, str2bool\n",
    "from gym_pybullet_drones.utils.enums import ObservationType, ActionType, Physics\n",
    "\n",
    "from policies import GaussianMLPPolicy\n",
    "from server import Federated_RL\n",
    "\n",
    "DEFAULT_GUI = True\n",
    "DEFAULT_RECORD_VIDEO = True\n",
    "DEFAULT_OUTPUT_FOLDER = 'results'\n",
    "DEFAULT_COLAB = False\n",
    "DEFAULT_DYNAMICS = Physics('pyb_gnd_drag_dw') # pyb: Pybullet dynamics; dyn: Explicit Dynamics specified in BaseAviary.py\n",
    "DEFAULT_WIND = np.array([0, 0.05, 0]) # units are in induced newtons\n",
    "DEFAULT_OBS = ObservationType('kin') # 'kin' or 'rgb'\n",
    "DEFAULT_ACT = ActionType('rpm') # 'rpm' or 'pid' or 'vel' or 'one_d_rpm' or 'one_d_pid'\n",
    "DEFAULT_AGENTS = 2\n",
    "DEFAULT_MA = False\n",
    "\n",
    "DR = True\n",
    "MASS_RANGE = [0.027, 0.042] # Maximum recommended payload is 15g\n",
    "WIND_RANGE = 0.005 # Inspired by literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 72\n",
      "Action size: 4\n"
     ]
    }
   ],
   "source": [
    "algorithms = ['FedSVRPG-M', 'PPO', 'SAC', 'TD3']\n",
    "num_agents = len(algorithms)\n",
    "envs = [HoverAviary for _ in range(num_agents)]\n",
    "env_kwargs = [dict(obs = DEFAULT_OBS, act = DEFAULT_ACT) for _ in range(num_agents)]\n",
    "agent_names = algorithms\n",
    "if DR == True:\n",
    "    domain_randomizations = [DR for _ in range(num_agents)]\n",
    "    DR_episode_thresholds = [.5 for _ in range(num_agents)] # Probability of DR at each episode\n",
    "    DR_step_thresholds = [.3 for _ in range(num_agents)] # If DR episode, probability of wind at each step\n",
    "\n",
    "mass_ranges = [MASS_RANGE for _ in range(num_agents)]\n",
    "wind_ranges = [WIND_RANGE for _ in range(num_agents)]\n",
    "env_example = HoverAviary(**env_kwargs[0])\n",
    "# Get the state size\n",
    "state_space = env_example.observation_space\n",
    "state_size = state_space.shape[1]\n",
    "# Get the action size\n",
    "action_space = env_example.action_space\n",
    "action_size = action_space.shape[1]\n",
    "\n",
    "layers = [512, 512, 256, 128]\n",
    "value_layers = [32, 32]\n",
    "# Maintain consistent network structures\n",
    "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
    "                     net_arch=dict(pi=layers, qf=value_layers))\n",
    "\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Action size:\", action_size)\n",
    "\n",
    "policy = GaussianMLPPolicy(input_size=state_size, output_size=action_size, hidden_layers=layers) # Will need some smarter way to initialize the policy within the model in the future\n",
    "# ASSUMING ONE ALGORITHM SO FAR. WILL IMPLEMENT GENERAL STRUCTURE FOR DIVERSIFIED ALGORITHMS LATER\n",
    "\n",
    "#### Train the model #######################################\n",
    "model = Federated_RL(policy = policy,\n",
    "                     envs = envs,\n",
    "                     env_kwargs = env_kwargs,\n",
    "                     num_agents = num_agents,\n",
    "                     global_iterations = 20,\n",
    "                     state_size = state_size,\n",
    "                     action_size = action_size,\n",
    "                     local_step_size = 1e-3,\n",
    "                     policy_kwargs = policy_kwargs,\n",
    "                     critic_net_aggregation = False,\n",
    "                     critic_net = value_layers,\n",
    "                     local_iterations = 10,\n",
    "                     max_episode_length=2048,\n",
    "                     agent_names = agent_names,\n",
    "                     DR = domain_randomizations,\n",
    "                     DR_episode_th = DR_episode_thresholds,\n",
    "                     DR_step_th = DR_step_thresholds,\n",
    "                     mass_ranges = mass_ranges,\n",
    "                     wind_ranges = wind_ranges,\n",
    "                     algorithms = algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035672848348311186\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 43.6539754487209\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 73.76353250134098\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033335569654417904\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 72.88044028650097\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028158931993396037\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 19.302217416812077\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 23.590517906776203\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 76.44638426843431\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030616031012781188\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 68.17780198212961\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 69.85604887664209\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 57.0561221458557\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 27.041816469762086\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03819761419180931\n",
      "Eval num_timesteps=2048, episode_reward=192.03 +/- 0.06\n",
      "Episode length: 173.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029029299092106338\n",
      "Eval num_timesteps=4096, episode_reward=46.41 +/- 3.10\n",
      "Episode length: 56.80 +/- 3.12\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03072935800638314\n",
      "Eval num_timesteps=6144, episode_reward=60.43 +/- 4.67\n",
      "Episode length: 130.40 +/- 19.17\n",
      "Eval num_timesteps=8192, episode_reward=59.70 +/- 25.10\n",
      "Episode length: 72.20 +/- 25.53\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02809087973726165\n",
      "Eval num_timesteps=10240, episode_reward=51.60 +/- 10.45\n",
      "Episode length: 63.60 +/- 6.83\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033876194904680655\n",
      "Eval num_timesteps=12288, episode_reward=57.79 +/- 9.89\n",
      "Episode length: 63.40 +/- 6.95\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027485203127609617\n",
      "Eval num_timesteps=14336, episode_reward=50.79 +/- 5.22\n",
      "Episode length: 55.00 +/- 3.85\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03563631320816822\n",
      "Eval num_timesteps=16384, episode_reward=45.72 +/- 5.20\n",
      "Episode length: 57.20 +/- 6.85\n",
      "Eval num_timesteps=18432, episode_reward=50.07 +/- 4.51\n",
      "Episode length: 62.60 +/- 5.24\n",
      "Eval num_timesteps=20480, episode_reward=51.22 +/- 16.23\n",
      "Episode length: 73.80 +/- 5.95\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137674aa0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=22.25 +/- 0.53\n",
      "Episode length: 16.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02956938218724693\n",
      "Eval num_timesteps=4096, episode_reward=24.64 +/- 0.00\n",
      "Episode length: 18.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039798587683300836\n",
      "Eval num_timesteps=6144, episode_reward=26.03 +/- 0.01\n",
      "Episode length: 19.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=31.55 +/- 0.00\n",
      "Episode length: 23.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038966393228548606\n",
      "Eval num_timesteps=10240, episode_reward=52.85 +/- 0.06\n",
      "Episode length: 42.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034922217988563486\n",
      "Eval num_timesteps=12288, episode_reward=32.56 +/- 0.01\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=29.93 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=28.66 +/- 0.00\n",
      "Episode length: 21.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=30.01 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=32.68 +/- 0.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03631137670644008\n",
      "Eval num_timesteps=2048, episode_reward=42.63 +/- 0.01\n",
      "Episode length: 33.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=42.63 +/- 0.00\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=16.57 +/- 0.00\n",
      "Episode length: 12.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=16.58 +/- 0.00\n",
      "Episode length: 12.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=9.96 +/- 0.55\n",
      "Episode length: 7.20 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=9.68 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038649269774267754\n",
      "Eval num_timesteps=14336, episode_reward=7.46 +/- 0.68\n",
      "Episode length: 5.40 +/- 0.49\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030754877279901626\n",
      "Eval num_timesteps=16384, episode_reward=6.91 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040442953666482616\n",
      "Eval num_timesteps=18432, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 24.04413620705011\n",
      "PPO: 49.98303156\n",
      "SAC: 39.211360160000005\n",
      "TD3: 5.57259316\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 77.24336969529834\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 37.302618581283305\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 37.54435628326323\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03582889126432443\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 38.71230264258654\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033517956516011915\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 37.781450221355186\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 37.2969138368513\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033501449462994375\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 38.428070436981066\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 37.537466553120915\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 37.25763067111087\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027092523313344878\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 38.04093731889808\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038960117002049315\n",
      "Eval num_timesteps=2048, episode_reward=41.83 +/- 0.41\n",
      "Episode length: 32.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=36.77 +/- 0.93\n",
      "Episode length: 27.60 +/- 0.80\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03781103297466294\n",
      "Eval num_timesteps=6144, episode_reward=36.74 +/- 1.62\n",
      "Episode length: 50.80 +/- 5.38\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035834466685822804\n",
      "Eval num_timesteps=8192, episode_reward=39.54 +/- 1.53\n",
      "Episode length: 64.80 +/- 9.83\n",
      "Eval num_timesteps=10240, episode_reward=50.76 +/- 7.55\n",
      "Episode length: 97.60 +/- 38.04\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028957978022006507\n",
      "Eval num_timesteps=12288, episode_reward=50.90 +/- 3.38\n",
      "Episode length: 59.00 +/- 6.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14336, episode_reward=72.88 +/- 4.56\n",
      "Episode length: 60.40 +/- 5.85\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03926358948526423\n",
      "Eval num_timesteps=16384, episode_reward=69.36 +/- 9.01\n",
      "Episode length: 55.00 +/- 6.42\n",
      "Eval num_timesteps=18432, episode_reward=70.43 +/- 3.45\n",
      "Episode length: 53.20 +/- 2.04\n",
      "Eval num_timesteps=20480, episode_reward=72.29 +/- 11.26\n",
      "Episode length: 54.20 +/- 7.81\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13742ae40>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=41.83 +/- 0.41\n",
      "Episode length: 32.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=37.27 +/- 0.00\n",
      "Episode length: 28.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=33.55 +/- 0.01\n",
      "Episode length: 25.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=31.08 +/- 0.00\n",
      "Episode length: 23.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=29.83 +/- 0.01\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=29.91 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03211406489473048\n",
      "Eval num_timesteps=14336, episode_reward=28.69 +/- 0.00\n",
      "Episode length: 21.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03478060344604218\n",
      "Eval num_timesteps=16384, episode_reward=29.02 +/- 0.51\n",
      "Episode length: 21.20 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041624717938109485\n",
      "Eval num_timesteps=18432, episode_reward=30.11 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04181033230610573\n",
      "Eval num_timesteps=20480, episode_reward=31.47 +/- 0.00\n",
      "Episode length: 23.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=41.83 +/- 0.41\n",
      "Episode length: 32.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=41.63 +/- 0.00\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=25.13 +/- 1.06\n",
      "Episode length: 18.40 +/- 0.80\n",
      "Eval num_timesteps=8192, episode_reward=24.60 +/- 0.00\n",
      "Episode length: 18.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=12.67 +/- 1.34\n",
      "Episode length: 9.20 +/- 0.98\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028296279828319358\n",
      "Eval num_timesteps=12288, episode_reward=11.03 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.45 +/- 0.67\n",
      "Episode length: 5.40 +/- 0.49\n",
      "Eval num_timesteps=16384, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03775508773919445\n",
      "Eval num_timesteps=18432, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.17516903801543\n",
      "PPO: 81.75206444\n",
      "SAC: 33.96235252\n",
      "TD3: 5.62752656\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 38.5111543958855\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028171511129439154\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 39.73260485258804\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03670026399374628\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 39.92194608225738\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02943402948253405\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 38.185628436678066\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 38.637427157137935\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 38.66454314020081\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034154202539854184\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 38.149694416726994\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034112641209898945\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 39.178735527792334\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 38.49140641564273\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03894645279460642\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 39.38108892354081\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030150026004264852\n",
      "Eval num_timesteps=2048, episode_reward=44.33 +/- 0.16\n",
      "Episode length: 35.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039100206996954545\n",
      "Eval num_timesteps=4096, episode_reward=28.58 +/- 0.50\n",
      "Episode length: 21.20 +/- 0.40\n",
      "Eval num_timesteps=6144, episode_reward=23.07 +/- 0.00\n",
      "Episode length: 17.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=15.07 +/- 0.01\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03387818381955592\n",
      "Eval num_timesteps=10240, episode_reward=13.46 +/- 0.54\n",
      "Episode length: 9.80 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=12.38 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029460778499646716\n",
      "Eval num_timesteps=14336, episode_reward=12.70 +/- 1.03\n",
      "Episode length: 9.20 +/- 0.75\n",
      "Eval num_timesteps=16384, episode_reward=16.31 +/- 1.35\n",
      "Episode length: 11.80 +/- 0.98\n",
      "Eval num_timesteps=18432, episode_reward=18.17 +/- 1.31\n",
      "Episode length: 13.20 +/- 0.98\n",
      "Eval num_timesteps=20480, episode_reward=19.45 +/- 1.63\n",
      "Episode length: 14.40 +/- 1.36\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13760b080>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=44.34 +/- 0.16\n",
      "Episode length: 35.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=43.88 +/- 0.38\n",
      "Episode length: 33.80 +/- 0.40\n",
      "Eval num_timesteps=6144, episode_reward=42.78 +/- 0.41\n",
      "Episode length: 32.80 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028824476545506482\n",
      "Eval num_timesteps=8192, episode_reward=41.55 +/- 0.10\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038730671769257326\n",
      "Eval num_timesteps=10240, episode_reward=42.43 +/- 0.39\n",
      "Episode length: 51.00 +/- 1.10\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03095139695144679\n",
      "Eval num_timesteps=12288, episode_reward=44.11 +/- 0.22\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027196336897731672\n",
      "Eval num_timesteps=14336, episode_reward=51.95 +/- 0.30\n",
      "Episode length: 43.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16384, episode_reward=65.13 +/- 0.17\n",
      "Episode length: 64.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18432, episode_reward=52.96 +/- 2.20\n",
      "Episode length: 62.40 +/- 5.75\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03866775307026389\n",
      "Eval num_timesteps=20480, episode_reward=45.16 +/- 0.19\n",
      "Episode length: 46.20 +/- 0.75\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030145023309252818\n",
      "Eval num_timesteps=2048, episode_reward=44.34 +/- 0.16\n",
      "Episode length: 35.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=44.26 +/- 0.00\n",
      "Episode length: 35.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=14.04 +/- 1.34\n",
      "Episode length: 10.20 +/- 0.98\n",
      "Eval num_timesteps=8192, episode_reward=12.40 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=7.17 +/- 0.55\n",
      "Episode length: 5.20 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027168680864321552\n",
      "Eval num_timesteps=14336, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03905358498988486\n",
      "Eval num_timesteps=16384, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03774867371663895\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.24175574467953\n",
      "PPO: 19.671273160000002\n",
      "SAC: 44.84961859999999\n",
      "TD3: 5.51716376\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 38.70788548464782\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 38.66370410388537\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 38.471750089903736\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 38.66231887613208\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030259749575065643\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 37.749252137765154\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03410726019967155\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 40.00523168633317\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03975532661079166\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 38.969733260870434\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040462961976445076\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 39.06897501588567\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 38.827861194542706\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 38.533937893031045\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=45.17 +/- 0.29\n",
      "Episode length: 52.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=41.37 +/- 0.05\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027468828337302915\n",
      "Eval num_timesteps=6144, episode_reward=55.53 +/- 3.34\n",
      "Episode length: 68.80 +/- 7.76\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030344561573454963\n",
      "Eval num_timesteps=8192, episode_reward=53.37 +/- 7.60\n",
      "Episode length: 75.40 +/- 8.80\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02902962996914464\n",
      "Eval num_timesteps=10240, episode_reward=68.45 +/- 3.60\n",
      "Episode length: 59.60 +/- 4.59\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02830380174279022\n",
      "Eval num_timesteps=12288, episode_reward=69.22 +/- 11.92\n",
      "Episode length: 90.20 +/- 38.68\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037773021456314615\n",
      "Eval num_timesteps=14336, episode_reward=74.10 +/- 17.34\n",
      "Episode length: 53.80 +/- 11.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16384, episode_reward=73.69 +/- 15.40\n",
      "Episode length: 51.20 +/- 9.52\n",
      "Eval num_timesteps=18432, episode_reward=147.77 +/- 47.58\n",
      "Episode length: 91.40 +/- 24.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20480, episode_reward=224.93 +/- 52.20\n",
      "Episode length: 139.80 +/- 27.91\n",
      "New best mean reward!\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03667646197166484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1376083e0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=45.18 +/- 0.29\n",
      "Episode length: 52.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=46.57 +/- 0.04\n",
      "Episode length: 53.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=48.51 +/- 0.02\n",
      "Episode length: 54.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=50.16 +/- 0.05\n",
      "Episode length: 56.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10240, episode_reward=50.62 +/- 0.08\n",
      "Episode length: 56.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12288, episode_reward=52.80 +/- 0.05\n",
      "Episode length: 56.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03969679666782459\n",
      "Eval num_timesteps=14336, episode_reward=50.04 +/- 0.03\n",
      "Episode length: 53.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=44.48 +/- 0.01\n",
      "Episode length: 34.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041987431176671634\n",
      "Eval num_timesteps=18432, episode_reward=39.90 +/- 0.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03936128715771957\n",
      "Eval num_timesteps=20480, episode_reward=37.44 +/- 0.00\n",
      "Episode length: 28.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035466519304844284\n",
      "Eval num_timesteps=2048, episode_reward=45.18 +/- 0.29\n",
      "Episode length: 52.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=45.04 +/- 0.00\n",
      "Episode length: 52.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=12.39 +/- 1.50\n",
      "Episode length: 9.00 +/- 1.10\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03229543901472214\n",
      "Eval num_timesteps=8192, episode_reward=11.02 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030851669887511177\n",
      "Eval num_timesteps=12288, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041885685447352994\n",
      "Eval num_timesteps=16384, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.251046464214724\n",
      "PPO: 289.38903364000004\n",
      "SAC: 37.14941676\n",
      "TD3: 5.5171513999999995\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037096830665883415\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 42.33404717630692\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03162332022675581\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 43.60836463928561\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040732100088744456\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 42.816308746596626\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03145761709675766\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 41.5230776177018\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 42.09659783467416\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 41.98685201815662\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036398639113735226\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 42.48446954136999\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 42.10512282004979\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 42.08311120979212\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 42.11281872699292\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=50.17 +/- 0.28\n",
      "Episode length: 63.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=43.10 +/- 1.21\n",
      "Episode length: 33.20 +/- 1.17\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038960581576748796\n",
      "Eval num_timesteps=6144, episode_reward=27.34 +/- 0.52\n",
      "Episode length: 20.20 +/- 0.40\n",
      "Eval num_timesteps=8192, episode_reward=23.34 +/- 0.53\n",
      "Episode length: 17.20 +/- 0.40\n",
      "Eval num_timesteps=10240, episode_reward=18.54 +/- 0.65\n",
      "Episode length: 13.60 +/- 0.49\n",
      "Eval num_timesteps=12288, episode_reward=25.29 +/- 3.87\n",
      "Episode length: 18.60 +/- 2.87\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031206288592323432\n",
      "Eval num_timesteps=14336, episode_reward=37.29 +/- 9.37\n",
      "Episode length: 28.20 +/- 7.55\n",
      "Eval num_timesteps=16384, episode_reward=63.57 +/- 9.92\n",
      "Episode length: 52.20 +/- 10.42\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03697833915181431\n",
      "Eval num_timesteps=18432, episode_reward=76.87 +/- 32.85\n",
      "Episode length: 54.80 +/- 22.27\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0347104945670417\n",
      "Eval num_timesteps=20480, episode_reward=111.95 +/- 27.23\n",
      "Episode length: 81.40 +/- 16.70\n",
      "New best mean reward!\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0415276783036873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137429400>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=50.16 +/- 0.29\n",
      "Episode length: 63.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=47.13 +/- 0.13\n",
      "Episode length: 58.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04189890483312694\n",
      "Eval num_timesteps=6144, episode_reward=44.23 +/- 0.14\n",
      "Episode length: 54.20 +/- 0.98\n",
      "Eval num_timesteps=8192, episode_reward=41.99 +/- 0.03\n",
      "Episode length: 34.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03360396938427751\n",
      "Eval num_timesteps=10240, episode_reward=37.99 +/- 0.39\n",
      "Episode length: 28.80 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=33.67 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030515494181066402\n",
      "Eval num_timesteps=14336, episode_reward=29.96 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03576493000756373\n",
      "Eval num_timesteps=16384, episode_reward=27.32 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=26.00 +/- 0.00\n",
      "Episode length: 19.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=25.99 +/- 0.00\n",
      "Episode length: 19.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=50.16 +/- 0.29\n",
      "Episode length: 63.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03453981683435156\n",
      "Eval num_timesteps=4096, episode_reward=50.00 +/- 0.05\n",
      "Episode length: 63.20 +/- 0.40\n",
      "Eval num_timesteps=6144, episode_reward=14.59 +/- 1.10\n",
      "Episode length: 10.60 +/- 0.80\n",
      "Eval num_timesteps=8192, episode_reward=13.76 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=7.17 +/- 0.55\n",
      "Episode length: 5.20 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040092543278560835\n",
      "Eval num_timesteps=12288, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031153232704533712\n",
      "Eval num_timesteps=14336, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.418123382893132\n",
      "PPO: 146.58001764\n",
      "SAC: 27.330593800000003\n",
      "TD3: 5.51663152\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 44.482222540770465\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 44.702346363379036\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03354449206929563\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 49.608280872001494\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028389619216814793\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 46.31356519110625\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 44.2103974445923\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031628784178972795\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 47.40214743526663\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 44.59668861909872\n",
      "\n",
      "Importance sampling weight: 1.0078431367874146\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 44.44718290582649\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 48.25067570794027\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 5\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 94.03650945397987\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0361987399700369\n",
      "Eval num_timesteps=2048, episode_reward=53.96 +/- 0.47\n",
      "Episode length: 69.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=60.98 +/- 0.84\n",
      "Episode length: 112.20 +/- 16.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=85.28 +/- 14.51\n",
      "Episode length: 167.00 +/- 64.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=123.91 +/- 22.82\n",
      "Episode length: 130.80 +/- 64.70\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039140122226109915\n",
      "Eval num_timesteps=10240, episode_reward=130.20 +/- 17.27\n",
      "Episode length: 96.80 +/- 11.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12288, episode_reward=101.49 +/- 20.30\n",
      "Episode length: 70.40 +/- 11.52\n",
      "Eval num_timesteps=14336, episode_reward=96.54 +/- 12.02\n",
      "Episode length: 68.60 +/- 5.46\n",
      "Eval num_timesteps=16384, episode_reward=106.97 +/- 35.43\n",
      "Episode length: 78.40 +/- 13.08\n",
      "Eval num_timesteps=18432, episode_reward=106.85 +/- 31.76\n",
      "Episode length: 74.20 +/- 17.27\n",
      "Eval num_timesteps=20480, episode_reward=135.82 +/- 53.73\n",
      "Episode length: 96.80 +/- 38.11\n",
      "New best mean reward!\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040942481940863584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x138b3f830>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=53.96 +/- 0.47\n",
      "Episode length: 69.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033696055867941666\n",
      "Eval num_timesteps=4096, episode_reward=57.76 +/- 0.13\n",
      "Episode length: 75.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0409682422068948\n",
      "Eval num_timesteps=6144, episode_reward=58.06 +/- 0.14\n",
      "Episode length: 67.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=59.07 +/- 0.05\n",
      "Episode length: 66.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02704309358573324\n",
      "Eval num_timesteps=10240, episode_reward=60.48 +/- 0.05\n",
      "Episode length: 67.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040531749717506435\n",
      "Eval num_timesteps=12288, episode_reward=62.28 +/- 0.10\n",
      "Episode length: 67.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03281855220668715\n",
      "Eval num_timesteps=14336, episode_reward=66.38 +/- 0.23\n",
      "Episode length: 77.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031489455307670076\n",
      "Eval num_timesteps=16384, episode_reward=57.83 +/- 0.19\n",
      "Episode length: 61.80 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03298521723314696\n",
      "Eval num_timesteps=18432, episode_reward=50.59 +/- 0.34\n",
      "Episode length: 53.80 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039117905766371606\n",
      "Eval num_timesteps=20480, episode_reward=43.12 +/- 0.09\n",
      "Episode length: 33.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036831338224747566\n",
      "Eval num_timesteps=2048, episode_reward=53.96 +/- 0.47\n",
      "Episode length: 69.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=53.61 +/- 0.01\n",
      "Episode length: 69.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038690969720633676\n",
      "Eval num_timesteps=6144, episode_reward=16.50 +/- 0.86\n",
      "Episode length: 12.00 +/- 0.63\n",
      "Eval num_timesteps=8192, episode_reward=15.14 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041616512057309284\n",
      "Eval num_timesteps=10240, episode_reward=7.99 +/- 1.03\n",
      "Episode length: 5.80 +/- 0.75\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04069828300500375\n",
      "Eval num_timesteps=12288, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041973385671349464\n",
      "Eval num_timesteps=14336, episode_reward=6.34 +/- 0.67\n",
      "Episode length: 4.60 +/- 0.49\n",
      "Eval num_timesteps=16384, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037622494687473126\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 23.88359270066544\n",
      "PPO: 179.84300624\n",
      "SAC: 38.800820359999996\n",
      "TD3: 5.51579676\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036162496025880876\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 109.840122388544\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 89.39424491554125\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 82.47739654027207\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03931962313230151\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 101.9449793691253\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 93.18960555110593\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03036607070166758\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 77.67907774506945\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 29.48287037043568\n",
      "\n",
      "Importance sampling weight: 0.005046489182859659\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 50.455959028876116\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 105.7733073960368\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 6\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 106.472912997394\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028305812230701662\n",
      "Eval num_timesteps=2048, episode_reward=87.28 +/- 12.03\n",
      "Episode length: 136.40 +/- 11.46\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034079603643028075\n",
      "Eval num_timesteps=4096, episode_reward=95.05 +/- 2.28\n",
      "Episode length: 83.20 +/- 2.48\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03809149005768682\n",
      "Eval num_timesteps=6144, episode_reward=110.57 +/- 1.80\n",
      "Episode length: 98.80 +/- 1.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=172.42 +/- 6.41\n",
      "Episode length: 146.00 +/- 6.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10240, episode_reward=127.17 +/- 6.58\n",
      "Episode length: 103.80 +/- 14.99\n",
      "Eval num_timesteps=12288, episode_reward=117.70 +/- 10.80\n",
      "Episode length: 119.00 +/- 34.11\n",
      "Eval num_timesteps=14336, episode_reward=126.42 +/- 8.74\n",
      "Episode length: 99.00 +/- 9.19\n",
      "Eval num_timesteps=16384, episode_reward=77.05 +/- 9.32\n",
      "Episode length: 66.00 +/- 5.83\n",
      "Eval num_timesteps=18432, episode_reward=67.07 +/- 2.39\n",
      "Episode length: 57.00 +/- 2.61\n",
      "Eval num_timesteps=20480, episode_reward=169.83 +/- 32.66\n",
      "Episode length: 109.80 +/- 18.52\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13756f050>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=83.88 +/- 5.15\n",
      "Episode length: 136.80 +/- 18.77\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027945751034272374\n",
      "Eval num_timesteps=4096, episode_reward=55.07 +/- 0.56\n",
      "Episode length: 82.20 +/- 1.47\n",
      "Eval num_timesteps=6144, episode_reward=48.38 +/- 0.27\n",
      "Episode length: 60.60 +/- 0.49\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03389824438770567\n",
      "Eval num_timesteps=8192, episode_reward=44.02 +/- 0.17\n",
      "Episode length: 58.20 +/- 1.17\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02847816956137159\n",
      "Eval num_timesteps=10240, episode_reward=40.63 +/- 0.06\n",
      "Episode length: 33.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038879504313601355\n",
      "Eval num_timesteps=12288, episode_reward=35.82 +/- 0.00\n",
      "Episode length: 27.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03229164370301196\n",
      "Eval num_timesteps=14336, episode_reward=32.41 +/- 0.00\n",
      "Episode length: 24.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040099385575969086\n",
      "Eval num_timesteps=16384, episode_reward=31.15 +/- 0.00\n",
      "Episode length: 23.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035383461350390895\n",
      "Eval num_timesteps=18432, episode_reward=29.94 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=29.95 +/- 0.00\n",
      "Episode length: 22.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029957810360001086\n",
      "Eval num_timesteps=2048, episode_reward=83.88 +/- 5.15\n",
      "Episode length: 136.80 +/- 18.77\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03863448545151424\n",
      "Eval num_timesteps=4096, episode_reward=90.00 +/- 13.03\n",
      "Episode length: 152.00 +/- 32.08\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03977627545675881\n",
      "Eval num_timesteps=6144, episode_reward=18.41 +/- 0.66\n",
      "Episode length: 13.40 +/- 0.49\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03299848172151777\n",
      "Eval num_timesteps=8192, episode_reward=17.87 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=8.82 +/- 0.67\n",
      "Episode length: 6.40 +/- 0.49\n",
      "Eval num_timesteps=12288, episode_reward=8.26 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037574536088530355\n",
      "Eval num_timesteps=14336, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036971318906539014\n",
      "Eval num_timesteps=20480, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 24.07228806734474\n",
      "PPO: 164.30990492\n",
      "SAC: 29.993947119999998\n",
      "TD3: 5.56964568\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 92.87042460462227\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 13.54036719411362\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040584729138517364\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 30.355194600856752\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 90.53292074576196\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 102.46367990768823\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030757302597922833\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 103.72901539543793\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027636784608228413\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 95.19549987632709\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 91.82123116481057\n",
      "\n",
      "Importance sampling weight: 0.0017406169790774584\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03463446922326502\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 90.63815621852875\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040742999890633944\n",
      "GLOBAL ITERATION: 7\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 22.002647072280368\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=108.36 +/- 8.08\n",
      "Episode length: 162.00 +/- 42.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=114.92 +/- 14.03\n",
      "Episode length: 241.40 +/- 1.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=139.69 +/- 20.92\n",
      "Episode length: 167.40 +/- 58.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=217.50 +/- 79.15\n",
      "Episode length: 177.80 +/- 54.82\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040217900082618385\n",
      "Eval num_timesteps=10240, episode_reward=109.66 +/- 17.90\n",
      "Episode length: 88.60 +/- 11.72\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027328515624865553\n",
      "Eval num_timesteps=12288, episode_reward=108.94 +/- 18.25\n",
      "Episode length: 100.20 +/- 29.02\n",
      "Eval num_timesteps=14336, episode_reward=121.40 +/- 12.79\n",
      "Episode length: 90.80 +/- 9.74\n",
      "Eval num_timesteps=16384, episode_reward=103.61 +/- 18.26\n",
      "Episode length: 75.40 +/- 13.00\n",
      "Eval num_timesteps=18432, episode_reward=132.85 +/- 43.48\n",
      "Episode length: 100.60 +/- 31.41\n",
      "Eval num_timesteps=20480, episode_reward=130.12 +/- 64.25\n",
      "Episode length: 123.00 +/- 76.16\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137677620>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=111.17 +/- 8.37\n",
      "Episode length: 156.80 +/- 30.97\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.032115428085806925\n",
      "Eval num_timesteps=4096, episode_reward=109.97 +/- 15.23\n",
      "Episode length: 220.80 +/- 16.65\n",
      "Eval num_timesteps=6144, episode_reward=59.42 +/- 0.30\n",
      "Episode length: 158.20 +/- 30.37\n",
      "Eval num_timesteps=8192, episode_reward=51.14 +/- 0.15\n",
      "Episode length: 108.80 +/- 13.85\n",
      "Eval num_timesteps=10240, episode_reward=45.92 +/- 0.21\n",
      "Episode length: 53.60 +/- 0.49\n",
      "Eval num_timesteps=12288, episode_reward=41.99 +/- 0.34\n",
      "Episode length: 47.80 +/- 1.60\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0281197821087625\n",
      "Eval num_timesteps=14336, episode_reward=34.79 +/- 0.48\n",
      "Episode length: 25.80 +/- 0.40\n",
      "Eval num_timesteps=16384, episode_reward=28.94 +/- 0.51\n",
      "Episode length: 21.20 +/- 0.40\n",
      "Eval num_timesteps=18432, episode_reward=27.33 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028248333133249655\n",
      "Eval num_timesteps=20480, episode_reward=26.00 +/- 0.00\n",
      "Episode length: 19.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=111.17 +/- 8.37\n",
      "Episode length: 156.80 +/- 30.97\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037265233179046044\n",
      "Eval num_timesteps=4096, episode_reward=112.47 +/- 12.57\n",
      "Episode length: 157.60 +/- 26.61\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03999017538848899\n",
      "Eval num_timesteps=6144, episode_reward=20.84 +/- 0.54\n",
      "Episode length: 15.20 +/- 0.40\n",
      "Eval num_timesteps=8192, episode_reward=20.57 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03255630087999113\n",
      "Eval num_timesteps=10240, episode_reward=9.09 +/- 1.10\n",
      "Episode length: 6.60 +/- 0.80\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037801572888642665\n",
      "Eval num_timesteps=12288, episode_reward=8.26 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03279920476329957\n",
      "Eval num_timesteps=14336, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03208155869299041\n",
      "Eval num_timesteps=18432, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030330841664851414\n",
      "Eval num_timesteps=20480, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 23.53767436464979\n",
      "PPO: 142.73560128\n",
      "SAC: 26.021713200000004\n",
      "TD3: 5.56825944\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 15.966490874959307\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 40.12338426571334\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04097881949032569\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 88.46965963388466\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040821121474812434\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 97.85906212470643\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03455465420075594\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 104.0214452638652\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 92.88184905278833\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 80.40638149562596\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03771715139055333\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 105.70616153161234\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 82.72065816917079\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 8\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 82.61314052929748\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=133.09 +/- 7.35\n",
      "Episode length: 169.80 +/- 36.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=123.83 +/- 8.45\n",
      "Episode length: 130.60 +/- 29.37\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03381812154463207\n",
      "Eval num_timesteps=6144, episode_reward=128.12 +/- 6.33\n",
      "Episode length: 183.40 +/- 48.02\n",
      "Eval num_timesteps=8192, episode_reward=158.54 +/- 5.17\n",
      "Episode length: 122.20 +/- 5.31\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035092694827657624\n",
      "Eval num_timesteps=10240, episode_reward=180.81 +/- 60.49\n",
      "Episode length: 147.40 +/- 63.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12288, episode_reward=134.20 +/- 12.81\n",
      "Episode length: 108.00 +/- 17.12\n",
      "Eval num_timesteps=14336, episode_reward=186.88 +/- 34.52\n",
      "Episode length: 197.80 +/- 55.25\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039274962326763176\n",
      "Eval num_timesteps=16384, episode_reward=94.57 +/- 9.39\n",
      "Episode length: 74.40 +/- 9.73\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03329006296087985\n",
      "Eval num_timesteps=18432, episode_reward=92.82 +/- 7.29\n",
      "Episode length: 69.00 +/- 4.77\n",
      "Eval num_timesteps=20480, episode_reward=105.22 +/- 10.45\n",
      "Episode length: 73.80 +/- 9.45\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137676390>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=132.28 +/- 5.86\n",
      "Episode length: 164.00 +/- 11.26\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038845973775824515\n",
      "Eval num_timesteps=4096, episode_reward=114.38 +/- 17.22\n",
      "Episode length: 195.60 +/- 27.75\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03929411019699246\n",
      "Eval num_timesteps=6144, episode_reward=52.61 +/- 0.27\n",
      "Episode length: 157.20 +/- 47.00\n",
      "Eval num_timesteps=8192, episode_reward=47.62 +/- 0.08\n",
      "Episode length: 67.20 +/- 1.33\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04134340930783214\n",
      "Eval num_timesteps=10240, episode_reward=45.49 +/- 0.10\n",
      "Episode length: 58.20 +/- 1.94\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030674525688297698\n",
      "Eval num_timesteps=12288, episode_reward=43.76 +/- 0.11\n",
      "Episode length: 66.40 +/- 9.31\n",
      "Eval num_timesteps=14336, episode_reward=42.84 +/- 0.10\n",
      "Episode length: 54.00 +/- 1.26\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035782574267070474\n",
      "Eval num_timesteps=16384, episode_reward=38.50 +/- 0.01\n",
      "Episode length: 29.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=35.05 +/- 0.02\n",
      "Episode length: 26.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028462607419315517\n",
      "Eval num_timesteps=20480, episode_reward=32.57 +/- 0.01\n",
      "Episode length: 24.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=132.28 +/- 5.86\n",
      "Episode length: 164.00 +/- 11.26\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0288083218851764\n",
      "Eval num_timesteps=4096, episode_reward=126.61 +/- 0.86\n",
      "Episode length: 160.00 +/- 5.62\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.032193160718121075\n",
      "Eval num_timesteps=6144, episode_reward=24.50 +/- 0.00\n",
      "Episode length: 18.00 +/- 0.00\n",
      "Eval num_timesteps=8192, episode_reward=24.50 +/- 0.00\n",
      "Episode length: 18.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=9.91 +/- 1.03\n",
      "Episode length: 7.20 +/- 0.75\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03323996169293968\n",
      "Eval num_timesteps=12288, episode_reward=8.26 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03572884381780062\n",
      "Eval num_timesteps=18432, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.587106307399008\n",
      "PPO: 348.4082208800001\n",
      "SAC: 31.321138880000003\n",
      "TD3: 5.62151536\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 102.54420596462093\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 109.98685736457833\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 89.97534199508999\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03761217739536668\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 98.29613789122064\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 95.09628205311728\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029222467018163414\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 108.912878178038\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 89.88257040083083\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0396711297212758\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 113.93089571712784\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 98.05235457659705\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037189231490201105\n",
      "GLOBAL ITERATION: 9\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 95.04756237874842\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04066572998991459\n",
      "Eval num_timesteps=2048, episode_reward=137.95 +/- 9.62\n",
      "Episode length: 145.60 +/- 49.62\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0402829989116336\n",
      "Eval num_timesteps=4096, episode_reward=147.79 +/- 6.31\n",
      "Episode length: 106.60 +/- 3.14\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04096280467172432\n",
      "Eval num_timesteps=6144, episode_reward=128.17 +/- 4.06\n",
      "Episode length: 94.60 +/- 3.01\n",
      "Eval num_timesteps=8192, episode_reward=151.12 +/- 4.43\n",
      "Episode length: 100.60 +/- 2.33\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03627605097535016\n",
      "Eval num_timesteps=10240, episode_reward=218.02 +/- 9.02\n",
      "Episode length: 142.20 +/- 5.71\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037803370600805485\n",
      "Eval num_timesteps=12288, episode_reward=298.15 +/- 23.46\n",
      "Episode length: 232.20 +/- 8.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14336, episode_reward=412.47 +/- 3.01\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16384, episode_reward=418.24 +/- 2.68\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03958201173109168\n",
      "Eval num_timesteps=18432, episode_reward=411.21 +/- 1.95\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=411.13 +/- 10.78\n",
      "Episode length: 242.00 +/- 0.00\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03617354574441056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x138b3ef00>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=139.03 +/- 6.79\n",
      "Episode length: 129.40 +/- 15.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=140.87 +/- 6.54\n",
      "Episode length: 173.40 +/- 39.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=157.76 +/- 29.41\n",
      "Episode length: 237.60 +/- 8.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=163.45 +/- 34.15\n",
      "Episode length: 215.20 +/- 41.06\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03366994913878317\n",
      "Eval num_timesteps=10240, episode_reward=133.18 +/- 31.18\n",
      "Episode length: 191.00 +/- 58.14\n",
      "Eval num_timesteps=12288, episode_reward=73.19 +/- 0.71\n",
      "Episode length: 98.00 +/- 7.67\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039165013266999366\n",
      "Eval num_timesteps=14336, episode_reward=64.56 +/- 0.06\n",
      "Episode length: 57.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=64.88 +/- 0.01\n",
      "Episode length: 53.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03638688884147144\n",
      "Eval num_timesteps=18432, episode_reward=69.70 +/- 0.49\n",
      "Episode length: 53.80 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03255909103121752\n",
      "Eval num_timesteps=20480, episode_reward=59.28 +/- 0.22\n",
      "Episode length: 45.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03773139300702474\n",
      "Eval num_timesteps=2048, episode_reward=139.03 +/- 6.79\n",
      "Episode length: 129.40 +/- 15.40\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030090331108312916\n",
      "Eval num_timesteps=4096, episode_reward=146.11 +/- 11.91\n",
      "Episode length: 128.20 +/- 17.39\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04012057553126781\n",
      "Eval num_timesteps=6144, episode_reward=38.80 +/- 0.86\n",
      "Episode length: 57.20 +/- 4.40\n",
      "Eval num_timesteps=8192, episode_reward=38.10 +/- 0.63\n",
      "Episode length: 55.80 +/- 4.96\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033801918667577834\n",
      "Eval num_timesteps=10240, episode_reward=12.38 +/- 1.50\n",
      "Episode length: 9.00 +/- 1.10\n",
      "Eval num_timesteps=12288, episode_reward=11.01 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.16 +/- 0.55\n",
      "Episode length: 5.20 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028929717521766728\n",
      "Eval num_timesteps=16384, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0341159128911849\n",
      "Eval num_timesteps=20480, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.617619494348002\n",
      "PPO: 379.40335824\n",
      "SAC: 54.68653963999999\n",
      "TD3: 6.879789399999998\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03594946185515835\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 125.2444615567073\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03786326675482095\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 128.29186465756985\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 118.30731595138359\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03735924744617633\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 107.169398663458\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030234559159060828\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 108.29856667164495\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029590689352569657\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 113.04140406910962\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 95.62115471977785\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03511848502791482\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 113.12626148391784\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 97.85434711888546\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040000939461318365\n",
      "GLOBAL ITERATION: 10\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 110.81043384245585\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=170.54 +/- 0.71\n",
      "Episode length: 122.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03578671905160688\n",
      "Eval num_timesteps=4096, episode_reward=151.04 +/- 1.36\n",
      "Episode length: 108.20 +/- 0.40\n",
      "Eval num_timesteps=6144, episode_reward=180.34 +/- 3.45\n",
      "Episode length: 222.80 +/- 14.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=144.25 +/- 8.04\n",
      "Episode length: 189.20 +/- 27.97\n",
      "Eval num_timesteps=10240, episode_reward=151.74 +/- 1.57\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=224.24 +/- 25.52\n",
      "Episode length: 202.00 +/- 33.60\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028891388489495122\n",
      "Eval num_timesteps=14336, episode_reward=323.88 +/- 33.46\n",
      "Episode length: 236.20 +/- 11.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16384, episode_reward=287.47 +/- 9.94\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029117499947663817\n",
      "Eval num_timesteps=18432, episode_reward=348.34 +/- 8.32\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037610161398024686\n",
      "Eval num_timesteps=20480, episode_reward=293.89 +/- 5.49\n",
      "Episode length: 242.00 +/- 0.00\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13b986cf0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=170.55 +/- 0.72\n",
      "Episode length: 122.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=202.16 +/- 3.53\n",
      "Episode length: 154.80 +/- 2.64\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04058706952965551\n",
      "Eval num_timesteps=6144, episode_reward=203.11 +/- 19.37\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=144.30 +/- 15.77\n",
      "Episode length: 202.00 +/- 49.01\n",
      "Eval num_timesteps=10240, episode_reward=135.01 +/- 13.67\n",
      "Episode length: 186.00 +/- 49.48\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041211721122320355\n",
      "Eval num_timesteps=12288, episode_reward=102.20 +/- 35.46\n",
      "Episode length: 166.20 +/- 46.07\n",
      "Eval num_timesteps=14336, episode_reward=55.59 +/- 0.91\n",
      "Episode length: 55.80 +/- 0.40\n",
      "Eval num_timesteps=16384, episode_reward=48.09 +/- 0.52\n",
      "Episode length: 50.80 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030056878596478542\n",
      "Eval num_timesteps=18432, episode_reward=38.24 +/- 0.51\n",
      "Episode length: 28.20 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027955185148089753\n",
      "Eval num_timesteps=20480, episode_reward=33.16 +/- 0.51\n",
      "Episode length: 24.20 +/- 0.40\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=170.55 +/- 0.72\n",
      "Episode length: 122.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02850283937680121\n",
      "Eval num_timesteps=4096, episode_reward=170.88 +/- 0.02\n",
      "Episode length: 122.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=46.37 +/- 2.17\n",
      "Episode length: 122.80 +/- 12.59\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027604586623648425\n",
      "Eval num_timesteps=8192, episode_reward=46.47 +/- 0.21\n",
      "Episode length: 129.00 +/- 5.97\n",
      "Eval num_timesteps=10240, episode_reward=13.47 +/- 1.02\n",
      "Episode length: 9.80 +/- 0.75\n",
      "Eval num_timesteps=12288, episode_reward=12.38 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.43 +/- 0.67\n",
      "Episode length: 5.40 +/- 0.49\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027992923295003032\n",
      "Eval num_timesteps=16384, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.68822262157024\n",
      "PPO: 269.1685484\n",
      "SAC: 31.543261920000006\n",
      "TD3: 6.87879848\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 105.5861617596856\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041381123369605796\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 125.7404642813234\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 113.59705915086627\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 132.14336596510344\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03979755490425671\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 122.08688126302107\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040349755142932764\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 142.86268346380385\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 104.26872518495205\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03373222779371852\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 143.1173070617687\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03571140329239533\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 128.71723921007361\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03921908516772843\n",
      "GLOBAL ITERATION: 11\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 143.57421250300993\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040166186205698784\n",
      "Eval num_timesteps=2048, episode_reward=210.17 +/- 16.33\n",
      "Episode length: 149.20 +/- 8.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=318.84 +/- 14.44\n",
      "Episode length: 216.80 +/- 5.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=427.76 +/- 0.15\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030251866805968457\n",
      "Eval num_timesteps=8192, episode_reward=322.32 +/- 25.85\n",
      "Episode length: 181.80 +/- 15.09\n",
      "Eval num_timesteps=10240, episode_reward=386.22 +/- 6.39\n",
      "Episode length: 220.80 +/- 4.12\n",
      "Eval num_timesteps=12288, episode_reward=396.23 +/- 3.01\n",
      "Episode length: 229.00 +/- 2.00\n",
      "Eval num_timesteps=14336, episode_reward=423.36 +/- 4.35\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03567081245462748\n",
      "Eval num_timesteps=16384, episode_reward=424.97 +/- 1.26\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03185718228935926\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x138ae54f0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=210.25 +/- 16.19\n",
      "Episode length: 149.20 +/- 8.84\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030343992000936057\n",
      "Eval num_timesteps=4096, episode_reward=218.10 +/- 8.29\n",
      "Episode length: 154.20 +/- 4.31\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03757724406740829\n",
      "Eval num_timesteps=6144, episode_reward=187.67 +/- 2.21\n",
      "Episode length: 137.20 +/- 0.98\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03710554699754581\n",
      "Eval num_timesteps=8192, episode_reward=178.02 +/- 12.15\n",
      "Episode length: 178.60 +/- 38.74\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041826923874431286\n",
      "Eval num_timesteps=10240, episode_reward=169.35 +/- 26.72\n",
      "Episode length: 217.40 +/- 30.41\n",
      "Eval num_timesteps=12288, episode_reward=136.41 +/- 27.83\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039019074819475766\n",
      "Eval num_timesteps=14336, episode_reward=57.37 +/- 0.28\n",
      "Episode length: 94.40 +/- 1.62\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02791410212074632\n",
      "Eval num_timesteps=16384, episode_reward=49.72 +/- 0.60\n",
      "Episode length: 54.80 +/- 1.47\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04075441409918625\n",
      "Eval num_timesteps=18432, episode_reward=39.68 +/- 0.99\n",
      "Episode length: 29.40 +/- 0.80\n",
      "Eval num_timesteps=20480, episode_reward=34.11 +/- 0.00\n",
      "Episode length: 25.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=210.25 +/- 16.19\n",
      "Episode length: 149.20 +/- 8.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=215.57 +/- 10.96\n",
      "Episode length: 151.20 +/- 6.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=74.33 +/- 3.66\n",
      "Episode length: 193.80 +/- 17.54\n",
      "Eval num_timesteps=8192, episode_reward=73.59 +/- 4.05\n",
      "Episode length: 208.60 +/- 19.58\n",
      "Eval num_timesteps=10240, episode_reward=15.09 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03916760079437829\n",
      "Eval num_timesteps=12288, episode_reward=15.09 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.98 +/- 1.02\n",
      "Episode length: 5.80 +/- 0.75\n",
      "Eval num_timesteps=16384, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03457764603080047\n",
      "Eval num_timesteps=18432, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.701170971967887\n",
      "PPO: 5.517596999999999\n",
      "SAC: 34.086515\n",
      "TD3: 6.8787063999999996\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 116.64860441833797\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03265171827707301\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 128.3409750384811\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029367051389797973\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 141.15787227321613\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03961779265707682\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 125.17138052704215\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 123.69076816032839\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03590647152293869\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 126.41691137935135\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031045355880202238\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 142.24118755002996\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 115.88572159176692\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03032637591390921\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 131.68399373749622\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 12\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 113.02084765234365\n",
      "\n",
      "Importance sampling weight: 1.1509929895401\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=214.43 +/- 16.73\n",
      "Episode length: 151.60 +/- 8.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=389.54 +/- 0.81\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03842096810365128\n",
      "Eval num_timesteps=6144, episode_reward=328.70 +/- 32.48\n",
      "Episode length: 203.00 +/- 18.76\n",
      "Eval num_timesteps=8192, episode_reward=287.61 +/- 15.12\n",
      "Episode length: 168.20 +/- 8.49\n",
      "Eval num_timesteps=10240, episode_reward=430.59 +/- 0.72\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028468738397533777\n",
      "Eval num_timesteps=12288, episode_reward=428.05 +/- 2.54\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030377691253963703\n",
      "Eval num_timesteps=14336, episode_reward=434.06 +/- 1.15\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.032249190014876514\n",
      "Eval num_timesteps=16384, episode_reward=371.81 +/- 15.07\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02949471931247475\n",
      "Eval num_timesteps=18432, episode_reward=387.95 +/- 12.86\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.032252866343045146\n",
      "Eval num_timesteps=20480, episode_reward=360.49 +/- 5.54\n",
      "Episode length: 240.80 +/- 1.47\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036842854954375184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137677cb0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=214.17 +/- 16.32\n",
      "Episode length: 151.40 +/- 8.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=218.42 +/- 20.08\n",
      "Episode length: 159.80 +/- 12.01\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03045438335940904\n",
      "Eval num_timesteps=6144, episode_reward=249.65 +/- 17.11\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03489832536246885\n",
      "Eval num_timesteps=8192, episode_reward=193.27 +/- 13.68\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=10240, episode_reward=212.73 +/- 11.66\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028483007453839325\n",
      "Eval num_timesteps=12288, episode_reward=138.32 +/- 38.70\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=55.09 +/- 5.73\n",
      "Episode length: 116.60 +/- 46.25\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03445698332520867\n",
      "Eval num_timesteps=16384, episode_reward=46.13 +/- 0.27\n",
      "Episode length: 56.00 +/- 5.76\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03421376621895522\n",
      "Eval num_timesteps=18432, episode_reward=41.68 +/- 0.03\n",
      "Episode length: 32.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03420653966779085\n",
      "Eval num_timesteps=20480, episode_reward=38.64 +/- 0.02\n",
      "Episode length: 29.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=214.17 +/- 16.32\n",
      "Episode length: 151.40 +/- 8.55\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036064735268536534\n",
      "Eval num_timesteps=4096, episode_reward=206.24 +/- 15.48\n",
      "Episode length: 147.40 +/- 8.26\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034593494213668245\n",
      "Eval num_timesteps=6144, episode_reward=70.51 +/- 4.72\n",
      "Episode length: 173.60 +/- 34.04\n",
      "Eval num_timesteps=8192, episode_reward=68.17 +/- 4.71\n",
      "Episode length: 182.20 +/- 12.22\n",
      "Eval num_timesteps=10240, episode_reward=15.36 +/- 0.54\n",
      "Episode length: 11.20 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=15.09 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.71 +/- 0.67\n",
      "Episode length: 5.60 +/- 0.49\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031114678807905416\n",
      "Eval num_timesteps=16384, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.701202159252365\n",
      "PPO: 389.51924075999995\n",
      "SAC: 36.51522284000001\n",
      "TD3: 6.88051408\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036955915467017486\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 139.30845024173894\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.032937815340389066\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 138.91804625020882\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0384045530086336\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 140.9982966308347\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03558002032809297\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 139.28176603489817\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 142.1034512051609\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03613568249140616\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 141.67164567803385\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 142.03288256131754\n",
      "\n",
      "Importance sampling weight: 0.5787556171417236\n",
      "\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 136.11015862288988\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039510483079429476\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 139.45018744029292\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029693573728179778\n",
      "GLOBAL ITERATION: 13\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 141.9558091089822\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=378.86 +/- 22.96\n",
      "Episode length: 225.40 +/- 11.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=425.26 +/- 0.31\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=439.24 +/- 0.01\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=421.25 +/- 4.23\n",
      "Episode length: 239.00 +/- 2.53\n",
      "Eval num_timesteps=10240, episode_reward=306.91 +/- 1.83\n",
      "Episode length: 175.60 +/- 1.20\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03629021759775655\n",
      "Eval num_timesteps=12288, episode_reward=320.28 +/- 0.65\n",
      "Episode length: 186.80 +/- 0.40\n",
      "Eval num_timesteps=14336, episode_reward=330.58 +/- 0.52\n",
      "Episode length: 207.20 +/- 0.98\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03678479109166309\n",
      "Eval num_timesteps=16384, episode_reward=310.53 +/- 4.08\n",
      "Episode length: 186.80 +/- 1.17\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034770061973838126\n",
      "Eval num_timesteps=18432, episode_reward=40.27 +/- 9.11\n",
      "Episode length: 43.20 +/- 12.27\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030143384328317153\n",
      "Eval num_timesteps=20480, episode_reward=49.71 +/- 12.57\n",
      "Episode length: 58.40 +/- 14.79\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137674b30>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03370525189405489\n",
      "Eval num_timesteps=2048, episode_reward=379.32 +/- 23.40\n",
      "Episode length: 225.40 +/- 11.62\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034486263849011795\n",
      "Eval num_timesteps=4096, episode_reward=413.88 +/- 2.13\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=416.25 +/- 1.29\n",
      "Episode length: 241.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035311964609514246\n",
      "Eval num_timesteps=8192, episode_reward=382.52 +/- 11.29\n",
      "Episode length: 233.40 +/- 1.85\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035482536685687\n",
      "Eval num_timesteps=10240, episode_reward=305.78 +/- 24.29\n",
      "Episode length: 196.40 +/- 7.91\n",
      "Eval num_timesteps=12288, episode_reward=216.65 +/- 0.86\n",
      "Episode length: 146.80 +/- 0.40\n",
      "Eval num_timesteps=14336, episode_reward=145.64 +/- 3.29\n",
      "Episode length: 142.80 +/- 23.30\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0333008262993575\n",
      "Eval num_timesteps=16384, episode_reward=121.95 +/- 6.65\n",
      "Episode length: 95.20 +/- 7.00\n",
      "Eval num_timesteps=18432, episode_reward=80.86 +/- 2.03\n",
      "Episode length: 96.40 +/- 5.24\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028671304947782833\n",
      "Eval num_timesteps=20480, episode_reward=60.14 +/- 0.44\n",
      "Episode length: 67.20 +/- 0.40\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028502261463786257\n",
      "Eval num_timesteps=2048, episode_reward=379.32 +/- 23.40\n",
      "Episode length: 225.40 +/- 11.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=384.61 +/- 24.10\n",
      "Episode length: 227.80 +/- 12.02\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.041971614176820424\n",
      "Eval num_timesteps=6144, episode_reward=88.09 +/- 0.63\n",
      "Episode length: 200.40 +/- 10.89\n",
      "Eval num_timesteps=8192, episode_reward=89.49 +/- 0.38\n",
      "Episode length: 186.00 +/- 8.72\n",
      "Eval num_timesteps=10240, episode_reward=19.40 +/- 0.53\n",
      "Episode length: 14.20 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=19.13 +/- 0.00\n",
      "Episode length: 14.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.99 +/- 1.03\n",
      "Episode length: 5.80 +/- 0.75\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02715784682781953\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03127577680130053\n",
      "Eval num_timesteps=20480, episode_reward=6.88 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.808219020115843\n",
      "PPO: 32.2205462\n",
      "SAC: 54.48955024\n",
      "TD3: 5.67720296\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03764324478035841\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 143.83715430735464\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027212932000845208\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 143.0950483257851\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 133.51064382577155\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 144.00612344280816\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038255395644660226\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 138.8488935670372\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 144.0875042391559\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0357060839443147\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 143.4337825083811\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 132.8412783344279\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03234003312533575\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 138.3435486706074\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03861390256994015\n",
      "GLOBAL ITERATION: 14\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 147.06431187968465\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=386.03 +/- 14.50\n",
      "Episode length: 231.40 +/- 7.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=341.11 +/- 37.94\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=274.20 +/- 1.51\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03863881561588923\n",
      "Eval num_timesteps=8192, episode_reward=400.44 +/- 1.71\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10240, episode_reward=405.43 +/- 0.05\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03603611571633592\n",
      "Eval num_timesteps=12288, episode_reward=414.13 +/- 5.32\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034795354236340954\n",
      "Eval num_timesteps=14336, episode_reward=426.21 +/- 1.49\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03950436295237805\n",
      "Eval num_timesteps=16384, episode_reward=415.66 +/- 2.32\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=374.90 +/- 8.09\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03166041478504995\n",
      "Eval num_timesteps=20480, episode_reward=12.34 +/- 4.10\n",
      "Episode length: 9.00 +/- 3.03\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035850766156748516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137674320>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=395.74 +/- 9.11\n",
      "Episode length: 234.80 +/- 5.56\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03327176272609233\n",
      "Eval num_timesteps=4096, episode_reward=395.88 +/- 17.82\n",
      "Episode length: 235.60 +/- 8.87\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02717819253667524\n",
      "Eval num_timesteps=6144, episode_reward=343.25 +/- 21.02\n",
      "Episode length: 214.20 +/- 8.13\n",
      "Eval num_timesteps=8192, episode_reward=297.14 +/- 18.53\n",
      "Episode length: 192.60 +/- 8.48\n",
      "Eval num_timesteps=10240, episode_reward=246.29 +/- 16.46\n",
      "Episode length: 165.80 +/- 7.08\n",
      "Eval num_timesteps=12288, episode_reward=180.52 +/- 2.13\n",
      "Episode length: 130.60 +/- 1.36\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033335427189020744\n",
      "Eval num_timesteps=14336, episode_reward=111.68 +/- 0.90\n",
      "Episode length: 85.00 +/- 0.63\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.032939389834561725\n",
      "Eval num_timesteps=16384, episode_reward=71.49 +/- 11.18\n",
      "Episode length: 208.80 +/- 56.93\n",
      "Eval num_timesteps=18432, episode_reward=51.38 +/- 0.94\n",
      "Episode length: 39.60 +/- 1.20\n",
      "Eval num_timesteps=20480, episode_reward=47.03 +/- 0.01\n",
      "Episode length: 35.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=395.74 +/- 9.11\n",
      "Episode length: 234.80 +/- 5.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=384.91 +/- 25.37\n",
      "Episode length: 229.20 +/- 12.54\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03585870579984399\n",
      "Eval num_timesteps=6144, episode_reward=83.22 +/- 2.78\n",
      "Episode length: 165.40 +/- 5.61\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03311138135779295\n",
      "Eval num_timesteps=8192, episode_reward=81.93 +/- 0.30\n",
      "Episode length: 167.20 +/- 4.12\n",
      "Eval num_timesteps=10240, episode_reward=15.39 +/- 0.54\n",
      "Episode length: 11.20 +/- 0.40\n",
      "Eval num_timesteps=12288, episode_reward=15.12 +/- 0.00\n",
      "Episode length: 11.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=7.44 +/- 0.67\n",
      "Episode length: 5.40 +/- 0.49\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.811244527064563\n",
      "PPO: 9.14006344\n",
      "SAC: 46.43197759999999\n",
      "TD3: 5.56911064\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027071577809170095\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 146.34728308598326\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03286594014555573\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 143.5092219802449\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0388685404812816\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 143.9377580951086\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0400107747141739\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 140.75469279717092\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 143.8252235134699\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 143.8782191062128\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 143.78606245630124\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03599679397254954\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 145.74645328048632\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03011566005466615\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 142.4890410298074\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 15\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 133.53538875773955\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03446676007733272\n",
      "Eval num_timesteps=2048, episode_reward=382.48 +/- 22.93\n",
      "Episode length: 228.00 +/- 10.71\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.036905264770497825\n",
      "Eval num_timesteps=4096, episode_reward=330.92 +/- 0.71\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=384.85 +/- 0.30\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=426.73 +/- 0.15\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10240, episode_reward=436.27 +/- 0.17\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12288, episode_reward=435.70 +/- 0.53\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02964050891322802\n",
      "Eval num_timesteps=14336, episode_reward=400.70 +/- 3.36\n",
      "Episode length: 227.40 +/- 1.74\n",
      "Eval num_timesteps=16384, episode_reward=15.42 +/- 3.41\n",
      "Episode length: 11.20 +/- 2.48\n",
      "Eval num_timesteps=18432, episode_reward=44.40 +/- 16.04\n",
      "Episode length: 32.60 +/- 11.62\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02846503056446436\n",
      "Eval num_timesteps=20480, episode_reward=20.96 +/- 7.00\n",
      "Episode length: 15.60 +/- 5.43\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033349495162034205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137676990>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=382.57 +/- 21.18\n",
      "Episode length: 228.00 +/- 10.53\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03409092001661646\n",
      "Eval num_timesteps=4096, episode_reward=350.07 +/- 23.84\n",
      "Episode length: 215.00 +/- 10.02\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039318410940778106\n",
      "Eval num_timesteps=6144, episode_reward=324.38 +/- 16.68\n",
      "Episode length: 200.80 +/- 6.58\n",
      "Eval num_timesteps=8192, episode_reward=285.52 +/- 16.43\n",
      "Episode length: 178.80 +/- 7.25\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03295863188219151\n",
      "Eval num_timesteps=10240, episode_reward=236.56 +/- 1.85\n",
      "Episode length: 159.20 +/- 1.17\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030999583475375972\n",
      "Eval num_timesteps=12288, episode_reward=215.15 +/- 9.56\n",
      "Episode length: 159.60 +/- 20.71\n",
      "Eval num_timesteps=14336, episode_reward=157.24 +/- 0.70\n",
      "Episode length: 132.20 +/- 6.49\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027577493653994194\n",
      "Eval num_timesteps=16384, episode_reward=85.89 +/- 3.08\n",
      "Episode length: 71.00 +/- 2.53\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039303211166272736\n",
      "Eval num_timesteps=18432, episode_reward=65.74 +/- 1.76\n",
      "Episode length: 50.80 +/- 2.04\n",
      "Eval num_timesteps=20480, episode_reward=55.85 +/- 0.59\n",
      "Episode length: 41.80 +/- 0.40\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=382.57 +/- 21.18\n",
      "Episode length: 228.00 +/- 10.53\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04134790417929798\n",
      "Eval num_timesteps=4096, episode_reward=359.32 +/- 12.76\n",
      "Episode length: 216.00 +/- 4.05\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027772733496674848\n",
      "Eval num_timesteps=6144, episode_reward=78.39 +/- 3.96\n",
      "Episode length: 142.40 +/- 1.62\n",
      "Eval num_timesteps=8192, episode_reward=75.82 +/- 0.56\n",
      "Episode length: 141.20 +/- 1.60\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037286989721526456\n",
      "Eval num_timesteps=10240, episode_reward=12.39 +/- 0.87\n",
      "Episode length: 9.00 +/- 0.63\n",
      "Eval num_timesteps=12288, episode_reward=11.02 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03658562996324301\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029872421743972765\n",
      "Eval num_timesteps=18432, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=5.51 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.81125357509177\n",
      "PPO: 148.01698448\n",
      "SAC: 49.538488359999995\n",
      "TD3: 5.570402840000001\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04109083131244388\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 139.2763760904925\n",
      "\n",
      "Importance sampling weight: 0.9922179579734802\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028797003901382765\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 26.7127559551064\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030176462612957155\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 18.60382935111852\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03384196252562335\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 100.07707940882602\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 14.63085773256173\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03122013169670858\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 103.63470588040788\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 54.275269728794484\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 103.07879963254436\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 17.104707070520963\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0291295498147245\n",
      "GLOBAL ITERATION: 16\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 101.00178288959415\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=354.84 +/- 9.02\n",
      "Episode length: 210.60 +/- 5.54\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03636261321960582\n",
      "Eval num_timesteps=4096, episode_reward=21.26 +/- 4.35\n",
      "Episode length: 15.60 +/- 3.20\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03759473146416647\n",
      "Eval num_timesteps=6144, episode_reward=8.00 +/- 0.53\n",
      "Episode length: 5.80 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030506443284246537\n",
      "Eval num_timesteps=8192, episode_reward=8.81 +/- 3.08\n",
      "Episode length: 6.40 +/- 2.24\n",
      "Eval num_timesteps=10240, episode_reward=16.98 +/- 9.05\n",
      "Episode length: 12.60 +/- 6.89\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038792570668961364\n",
      "Eval num_timesteps=12288, episode_reward=19.64 +/- 10.50\n",
      "Episode length: 14.60 +/- 8.01\n",
      "Eval num_timesteps=14336, episode_reward=7.99 +/- 2.02\n",
      "Episode length: 5.80 +/- 1.47\n",
      "Eval num_timesteps=16384, episode_reward=9.64 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.037979089051732466\n",
      "Eval num_timesteps=18432, episode_reward=9.64 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=9.64 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x13760a930>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=348.25 +/- 17.83\n",
      "Episode length: 209.00 +/- 8.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=372.41 +/- 16.41\n",
      "Episode length: 221.40 +/- 9.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=387.42 +/- 16.89\n",
      "Episode length: 232.80 +/- 7.55\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029752390280610313\n",
      "Eval num_timesteps=8192, episode_reward=371.98 +/- 4.45\n",
      "Episode length: 229.20 +/- 2.32\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0300569914621096\n",
      "Eval num_timesteps=10240, episode_reward=272.07 +/- 1.47\n",
      "Episode length: 186.80 +/- 2.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03820660533481666\n",
      "Eval num_timesteps=12288, episode_reward=209.41 +/- 10.78\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034698693493159675\n",
      "Eval num_timesteps=14336, episode_reward=127.98 +/- 1.04\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=137.63 +/- 48.20\n",
      "Episode length: 206.20 +/- 46.94\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02839691903566902\n",
      "Eval num_timesteps=18432, episode_reward=58.65 +/- 0.14\n",
      "Episode length: 66.20 +/- 1.47\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04070325254422728\n",
      "Eval num_timesteps=20480, episode_reward=53.51 +/- 0.26\n",
      "Episode length: 44.20 +/- 0.40\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03341002203799434\n",
      "Eval num_timesteps=2048, episode_reward=348.25 +/- 17.83\n",
      "Episode length: 209.00 +/- 8.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=355.07 +/- 11.65\n",
      "Episode length: 209.80 +/- 6.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6144, episode_reward=88.15 +/- 5.57\n",
      "Episode length: 146.20 +/- 2.86\n",
      "Eval num_timesteps=8192, episode_reward=83.57 +/- 1.98\n",
      "Episode length: 145.60 +/- 1.36\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.034666012261482\n",
      "Eval num_timesteps=10240, episode_reward=12.40 +/- 1.50\n",
      "Episode length: 9.00 +/- 1.10\n",
      "Eval num_timesteps=12288, episode_reward=11.03 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03048111176702226\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03343356307136869\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 23.161201656964913\n",
      "PPO: 19.544547240000004\n",
      "SAC: 48.80387248000001\n",
      "TD3: 5.5710946\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 100.2835735929857\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 14.602811295079881\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02888262299369678\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 97.22358119383857\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04080117629418154\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 25.554111542534617\n",
      "\n",
      "Importance sampling weight: 0.0010000000474974513\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 141.3346712108163\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03387575634929111\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 139.7383192635067\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 134.23340397192018\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 134.25017960946917\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 134.21853572886727\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 17\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 141.45632190771244\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=368.84 +/- 23.10\n",
      "Episode length: 219.00 +/- 10.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=9.77 +/- 1.47\n",
      "Episode length: 7.20 +/- 1.17\n",
      "Eval num_timesteps=6144, episode_reward=17.39 +/- 7.76\n",
      "Episode length: 17.20 +/- 14.52\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03455403301803306\n",
      "Eval num_timesteps=8192, episode_reward=10.66 +/- 2.36\n",
      "Episode length: 7.80 +/- 1.72\n",
      "Eval num_timesteps=10240, episode_reward=11.05 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028223636572279408\n",
      "Eval num_timesteps=12288, episode_reward=9.10 +/- 2.22\n",
      "Episode length: 6.60 +/- 1.62\n",
      "Eval num_timesteps=14336, episode_reward=11.27 +/- 2.80\n",
      "Episode length: 8.20 +/- 2.04\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04102438174239513\n",
      "Eval num_timesteps=16384, episode_reward=17.14 +/- 7.72\n",
      "Episode length: 12.60 +/- 5.71\n",
      "Eval num_timesteps=18432, episode_reward=19.00 +/- 7.54\n",
      "Episode length: 19.60 +/- 16.46\n",
      "Eval num_timesteps=20480, episode_reward=21.01 +/- 8.82\n",
      "Episode length: 20.40 +/- 15.76\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040766480435560144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137677c20>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=361.61 +/- 20.27\n",
      "Episode length: 214.60 +/- 8.87\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03944756252667968\n",
      "Eval num_timesteps=4096, episode_reward=355.29 +/- 22.78\n",
      "Episode length: 214.40 +/- 10.87\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03501887452037218\n",
      "Eval num_timesteps=6144, episode_reward=336.49 +/- 0.67\n",
      "Episode length: 210.20 +/- 0.40\n",
      "Eval num_timesteps=8192, episode_reward=282.17 +/- 14.48\n",
      "Episode length: 183.00 +/- 6.45\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03277660430414321\n",
      "Eval num_timesteps=10240, episode_reward=248.39 +/- 2.05\n",
      "Episode length: 163.80 +/- 0.98\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03506315619626024\n",
      "Eval num_timesteps=12288, episode_reward=189.01 +/- 6.53\n",
      "Episode length: 156.80 +/- 42.66\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029043853834213068\n",
      "Eval num_timesteps=14336, episode_reward=132.80 +/- 5.17\n",
      "Episode length: 132.60 +/- 21.35\n",
      "Eval num_timesteps=16384, episode_reward=109.29 +/- 20.58\n",
      "Episode length: 148.00 +/- 48.30\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0389169198594239\n",
      "Eval num_timesteps=18432, episode_reward=56.81 +/- 1.71\n",
      "Episode length: 60.80 +/- 1.17\n",
      "Eval num_timesteps=20480, episode_reward=49.45 +/- 0.32\n",
      "Episode length: 38.20 +/- 0.40\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0346108954223837\n",
      "Eval num_timesteps=2048, episode_reward=361.61 +/- 20.27\n",
      "Episode length: 214.60 +/- 8.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=362.12 +/- 13.93\n",
      "Episode length: 214.80 +/- 8.42\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0405176265006152\n",
      "Eval num_timesteps=6144, episode_reward=81.32 +/- 3.84\n",
      "Episode length: 133.60 +/- 3.61\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04132442214533614\n",
      "Eval num_timesteps=8192, episode_reward=80.08 +/- 3.11\n",
      "Episode length: 136.20 +/- 5.34\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03994846585612132\n",
      "Eval num_timesteps=10240, episode_reward=11.85 +/- 1.10\n",
      "Episode length: 8.60 +/- 0.80\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0414552784357693\n",
      "Eval num_timesteps=12288, episode_reward=9.65 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03547248680980226\n",
      "Eval num_timesteps=14336, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03159806719228009\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.040344418882371225\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.831155845810645\n",
      "PPO: 11.28045608\n",
      "SAC: 43.358489199999994\n",
      "TD3: 5.5715365199999995\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02943177525387072\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 139.02395937082528\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 134.13079939718756\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 141.10577984828566\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04199598235297416\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 141.14212405798432\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03324540392571243\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 145.47360024559296\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 133.92295302449716\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03277838762568781\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 144.0406420929419\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 133.98766891831798\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 140.7864275498345\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035380164755627413\n",
      "GLOBAL ITERATION: 18\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 137.55126293464082\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0363357157623603\n",
      "Eval num_timesteps=2048, episode_reward=355.66 +/- 15.14\n",
      "Episode length: 212.00 +/- 6.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=7.45 +/- 1.10\n",
      "Episode length: 5.40 +/- 0.80\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031163138684160922\n",
      "Eval num_timesteps=8192, episode_reward=10.43 +/- 4.27\n",
      "Episode length: 7.60 +/- 3.14\n",
      "Eval num_timesteps=10240, episode_reward=10.20 +/- 1.11\n",
      "Episode length: 7.40 +/- 0.80\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027835887044341458\n",
      "Eval num_timesteps=12288, episode_reward=19.84 +/- 9.53\n",
      "Episode length: 15.20 +/- 7.55\n",
      "Eval num_timesteps=14336, episode_reward=9.64 +/- 0.05\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=9.63 +/- 0.05\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033987960598884\n",
      "Eval num_timesteps=18432, episode_reward=16.95 +/- 3.13\n",
      "Episode length: 12.40 +/- 2.33\n",
      "Eval num_timesteps=20480, episode_reward=22.37 +/- 7.81\n",
      "Episode length: 16.40 +/- 5.82\n",
      "\n",
      "Training agent SAC\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030753652061296866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x137675670>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=353.26 +/- 16.52\n",
      "Episode length: 210.60 +/- 8.04\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03806569177045957\n",
      "Eval num_timesteps=4096, episode_reward=329.15 +/- 11.43\n",
      "Episode length: 200.40 +/- 4.54\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.0341194998935455\n",
      "Eval num_timesteps=6144, episode_reward=315.23 +/- 6.53\n",
      "Episode length: 193.80 +/- 1.94\n",
      "Eval num_timesteps=8192, episode_reward=268.39 +/- 3.21\n",
      "Episode length: 173.40 +/- 0.80\n",
      "Eval num_timesteps=10240, episode_reward=245.45 +/- 15.38\n",
      "Episode length: 161.40 +/- 7.14\n",
      "Eval num_timesteps=12288, episode_reward=189.65 +/- 14.30\n",
      "Episode length: 184.60 +/- 49.53\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02785449393514884\n",
      "Eval num_timesteps=14336, episode_reward=138.45 +/- 1.37\n",
      "Episode length: 112.40 +/- 1.36\n",
      "Eval num_timesteps=16384, episode_reward=59.78 +/- 0.14\n",
      "Episode length: 63.20 +/- 0.40\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04017899105196465\n",
      "Eval num_timesteps=18432, episode_reward=49.97 +/- 0.40\n",
      "Episode length: 37.80 +/- 0.40\n",
      "Eval num_timesteps=20480, episode_reward=44.35 +/- 0.01\n",
      "Episode length: 33.00 +/- 0.00\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Eval num_timesteps=2048, episode_reward=353.26 +/- 16.52\n",
      "Episode length: 210.60 +/- 8.04\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03613080462878169\n",
      "Eval num_timesteps=4096, episode_reward=351.89 +/- 20.13\n",
      "Episode length: 211.40 +/- 9.50\n",
      "Eval num_timesteps=6144, episode_reward=83.06 +/- 6.36\n",
      "Episode length: 133.20 +/- 3.76\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03371393890570725\n",
      "Eval num_timesteps=8192, episode_reward=78.41 +/- 1.83\n",
      "Episode length: 128.00 +/- 3.63\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02902220342635565\n",
      "Eval num_timesteps=10240, episode_reward=11.58 +/- 2.05\n",
      "Episode length: 8.40 +/- 1.50\n",
      "Eval num_timesteps=12288, episode_reward=9.65 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02899332542801963\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.035029659738146354\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.028365441481719385\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.83200545468118\n",
      "PPO: 18.70617824\n",
      "SAC: 40.424347159999996\n",
      "TD3: 5.51682548\n",
      "\n",
      "Training agent FedSVRPG-M\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Episode Reward: 140.9251220648763\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030144382059254918\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Episode Reward: 142.9825759804971\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.027422805539381655\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Episode Reward: 143.32920506557886\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Episode Reward: 134.66554085211575\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Episode Reward: 134.57289823290077\n",
      "\n",
      "Importance sampling weight: 1.0157476663589478\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Episode Reward: 138.823039720837\n",
      "\n",
      "Importance sampling weight: 0.0012610869016498327\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Episode Reward: 134.5643010317232\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Episode Reward: 141.10504761062086\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Episode Reward: 134.4742919318881\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04121426622306747\n",
      "GLOBAL ITERATION: 19\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Episode Reward: 146.12836964441772\n",
      "\n",
      "Importance sampling weight: 1.0\n",
      "\n",
      "\n",
      "Training agent PPO\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03141963822526741\n",
      "Eval num_timesteps=2048, episode_reward=355.23 +/- 14.65\n",
      "Episode length: 211.40 +/- 7.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=6144, episode_reward=23.61 +/- 9.12\n",
      "Episode length: 18.20 +/- 7.44\n",
      "Eval num_timesteps=8192, episode_reward=26.16 +/- 3.66\n",
      "Episode length: 20.20 +/- 3.92\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03224889444040889\n",
      "Eval num_timesteps=10240, episode_reward=21.73 +/- 0.00\n",
      "Episode length: 16.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=19.08 +/- 0.00\n",
      "Episode length: 14.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=16.70 +/- 1.29\n",
      "Episode length: 12.20 +/- 0.98\n",
      "Eval num_timesteps=16384, episode_reward=18.30 +/- 1.59\n",
      "Episode length: 13.40 +/- 1.20\n",
      "Eval num_timesteps=18432, episode_reward=17.27 +/- 1.58\n",
      "Episode length: 12.60 +/- 1.20\n",
      "Eval num_timesteps=20480, episode_reward=18.30 +/- 1.59\n",
      "Episode length: 13.40 +/- 1.20\n",
      "\n",
      "Training agent SAC\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/stable_baselines3/common/callbacks.py:426: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x138b2a1b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x138b3e000>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2048, episode_reward=361.83 +/- 18.24\n",
      "Episode length: 214.80 +/- 7.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=385.96 +/- 28.36\n",
      "Episode length: 229.20 +/- 13.08\n",
      "New best mean reward!\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.031217581621207222\n",
      "Eval num_timesteps=6144, episode_reward=414.40 +/- 0.66\n",
      "Episode length: 242.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8192, episode_reward=411.85 +/- 0.67\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.030676564656532224\n",
      "Eval num_timesteps=10240, episode_reward=404.08 +/- 2.90\n",
      "Episode length: 242.00 +/- 0.00\n",
      "Eval num_timesteps=12288, episode_reward=268.31 +/- 32.15\n",
      "Episode length: 205.60 +/- 28.71\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04055154626124503\n",
      "Eval num_timesteps=14336, episode_reward=135.58 +/- 8.90\n",
      "Episode length: 145.40 +/- 48.89\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.039120123704691555\n",
      "Eval num_timesteps=16384, episode_reward=106.90 +/- 2.58\n",
      "Episode length: 83.80 +/- 1.47\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02976223185697596\n",
      "Eval num_timesteps=18432, episode_reward=66.45 +/- 0.85\n",
      "Episode length: 79.60 +/- 3.88\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.033545537814872975\n",
      "Eval num_timesteps=20480, episode_reward=51.18 +/- 1.50\n",
      "Episode length: 42.40 +/- 2.33\n",
      "\n",
      "Training agent TD3\n",
      "\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.038127149413787816\n",
      "Eval num_timesteps=2048, episode_reward=361.83 +/- 18.24\n",
      "Episode length: 214.80 +/- 7.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4096, episode_reward=353.73 +/- 18.60\n",
      "Episode length: 211.60 +/- 8.48\n",
      "Eval num_timesteps=6144, episode_reward=69.56 +/- 6.59\n",
      "Episode length: 121.80 +/- 8.63\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.03944211744207814\n",
      "Eval num_timesteps=8192, episode_reward=73.30 +/- 3.63\n",
      "Episode length: 119.00 +/- 3.03\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.02748874576374916\n",
      "Eval num_timesteps=10240, episode_reward=9.93 +/- 1.35\n",
      "Episode length: 7.20 +/- 0.98\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.04094189927427649\n",
      "Eval num_timesteps=12288, episode_reward=8.28 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "Eval num_timesteps=14336, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Eval num_timesteps=16384, episode_reward=6.89 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "Episode has activated Domain Randomization. Wind will be applied with a probability of 0.3 with a maximum magnitude of 0.005 Newtons.\n",
      "Mass parameter for next episode: 0.029787740591381656\n",
      "Eval num_timesteps=18432, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=20480, episode_reward=5.52 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "\n",
      "Evaluating agents...\n",
      "\n",
      "Mean rewards for the following agents:\n",
      "FedSVRPG-M: 30.831051853782036\n",
      "PPO: 17.44827384\n",
      "SAC: 45.08880719999999\n",
      "TD3: 5.517082719999999\n"
     ]
    }
   ],
   "source": [
    "serverModel = model.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 10)\n"
     ]
    }
   ],
   "source": [
    "rewards = model.get_rewards()\n",
    "np.savetxt(\"TotalRewards_NoCritic.csv\", rewards, delimiter=',')\n",
    "evaluated_rewards = np.array(rewards)\n",
    "print(evaluated_rewards.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEDSVRPG_M_rewards = []\n",
    "PPO_rewards = []\n",
    "SAC_rewards = []\n",
    "TD3_rewards = []\n",
    "reward_lists = [FEDSVRPG_M_rewards, PPO_rewards, SAC_rewards, TD3_rewards]\n",
    "for i, rewards in enumerate(evaluated_rewards):\n",
    "    reward_lists[i % 4].append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('FEDSVRPG_M_rewards_without_value_aggregation.csv',FEDSVRPG_M_rewards, delimiter=',')\n",
    "np.savetxt('PPO_rewards_without_value_aggregation.csv', PPO_rewards, delimiter=',')\n",
    "np.savetxt('SAC_rewards_without_value_aggregation.csv', SAC_rewards, delimiter=',')\n",
    "np.savetxt('TD3_rewards_without_value_aggregation.csv', TD3_rewards, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
