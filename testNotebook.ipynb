{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jun 24 2024 15:23:59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.HoverAviary import HoverAviary\n",
    "from gym_pybullet_drones.envs.MultiHoverAviary import MultiHoverAviary\n",
    "from gym_pybullet_drones.utils.utils import sync, str2bool\n",
    "from gym_pybullet_drones.utils.enums import ObservationType, ActionType, Physics\n",
    "\n",
    "from policies import GaussianMLPPolicy\n",
    "from server import Federated_RL\n",
    "\n",
    "DEFAULT_GUI = True\n",
    "DEFAULT_RECORD_VIDEO = True\n",
    "DEFAULT_OUTPUT_FOLDER = 'results'\n",
    "DEFAULT_COLAB = False\n",
    "DEFAULT_DYNAMICS = Physics('pyb') # pyb: Pybullet dynamics; dyn: Explicit Dynamics specified in BaseAviary.py\n",
    "DEFAULT_WIND = np.array([0, 0.05, 0]) # units are in induced newtons\n",
    "DEFAULT_OBS = ObservationType('kin') # 'kin' or 'rgb'\n",
    "DEFAULT_ACT = ActionType('one_d_rpm') # 'rpm' or 'pid' or 'vel' or 'one_d_rpm' or 'one_d_pid'\n",
    "DEFAULT_AGENTS = 4\n",
    "DEFAULT_MA = False\n",
    "DEFAULT_MASS = 0.037 # Actual default is 0.027\n",
    "\n",
    "DR = False\n",
    "MASS_RANGE = [0.027, 0.042] # Maximum recommended payload is 15g\n",
    "WIND_RANGE = 0.005 # Inspired by literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n",
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n",
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n",
      "[INFO] BaseAviary.__init__() loaded parameters from the drone's .urdf:\n",
      "[INFO] m 0.027000, L 0.039700,\n",
      "[INFO] ixx 0.000014, iyy 0.000014, izz 0.000022,\n",
      "[INFO] kf 0.000000, km 0.000000,\n",
      "[INFO] t2w 2.250000, max_speed_kmh 30.000000,\n",
      "[INFO] gnd_eff_coeff 11.368590, prop_radius 0.023135,\n",
      "[INFO] drag_xy_coeff 0.000001, drag_z_coeff 0.000001,\n",
      "[INFO] dw_coeff_1 2267.180000, dw_coeff_2 0.160000, dw_coeff_3 -0.110000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 27\n",
      "Action size: 1\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.677060518795864\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.755722343693469\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.023920558393001556\n",
      "\n",
      "Step size: tensor(0.0265, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.7610997690005785\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.745528357764654\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.7488756053098236\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.766497152574669\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.798901554197229\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.779241897210884\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.7048259377479553\n",
      "\n",
      "Step size: tensor(0.0107, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.770131411206457\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.817613674666399\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.748114164161091\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.809361542820995\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.5502586364746094\n",
      "\n",
      "Step size: tensor(0.0111, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.755129278142091\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.769741414418963\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.81923641743139\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.725744340751678\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7340072393417358\n",
      "\n",
      "Step size: tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.809511524658638\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.782879967247384\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.1911281943321228\n",
      "\n",
      "Step size: tensor(0.0129, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.786407861981113\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.765415400956281\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.2245245724916458\n",
      "\n",
      "Step size: tensor(0.0124, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 10\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.795595749578474\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.784975020803072\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 11\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.711327070067039\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.775496831126333\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.08818823099136353\n",
      "\n",
      "Step size: tensor(0.0131, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 12\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.84317484641503\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.791712781958172\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.03422469645738602\n",
      "\n",
      "Step size: tensor(0.0133, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 13\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.73379368587255\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.791819144449164\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.2979269325733185\n",
      "\n",
      "Step size: tensor(0.0112, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 14\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.805744376091761\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7688822734230385\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.21567733585834503\n",
      "\n",
      "Step size: tensor(0.0115, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 15\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.758306474181666\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.771806877413632\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.1988534927368164\n",
      "\n",
      "Step size: tensor(0.0114, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 16\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.825109110694834\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.755906987885231\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.13708122074604034\n",
      "\n",
      "Step size: tensor(0.0116, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 17\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.740298762714255\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.813610288757417\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.45937198400497437\n",
      "\n",
      "Step size: tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 18\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.776160116321769\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.738110558491574\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 19\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.78310687694085\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.723596357643578\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 20\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.792868670682627\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.747115819617429\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 21\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.735548302300042\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.765108714032712\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.962828516960144\n",
      "\n",
      "Step size: tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 22\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.745751128787281\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7377855302735465\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 23\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.716359671340076\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7514912512291545\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.1539018154144287\n",
      "\n",
      "Step size: tensor(0.0105, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 24\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.7479025264471675\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7266620123212455\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0073, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 25\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.734232018377141\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.795049741148068\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.0014880201779305935\n",
      "\n",
      "Step size: tensor(0.0109, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 26\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.726982029178247\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7789959624013445\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.44100096821784973\n",
      "\n",
      "Step size: tensor(0.0093, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 27\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.788661973505093\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.771819396509507\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.3433416187763214\n",
      "\n",
      "Step size: tensor(0.0095, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 28\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.787439035472358\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.791591375942673\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.041492387652397156\n",
      "\n",
      "Step size: tensor(0.0103, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 29\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.8422050363552165\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.762867479496049\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.09540306031703949\n",
      "\n",
      "Step size: tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 30\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.803209748975237\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.8111508287823295\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 31\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.710671679231508\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.72535325587536\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.36842137575149536\n",
      "\n",
      "Step size: tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 32\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.707888607022731\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.776304234017923\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.0026930654421448708\n",
      "\n",
      "Step size: tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 33\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.732840065152477\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.677573788916226\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0071, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 34\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.762806893629552\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7891473183779985\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.7272733449935913\n",
      "\n",
      "Step size: tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 35\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.779341325829207\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.788873309860305\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.1191631555557251\n",
      "\n",
      "Step size: tensor(0.0094, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 36\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.757445372027831\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.764881941012525\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.6410044431686401\n",
      "\n",
      "Step size: tensor(0.0083, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 37\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.798513126802508\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.777499488001782\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.13247142732143402\n",
      "\n",
      "Step size: tensor(0.0092, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 38\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.787412673184517\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.728830697788269\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0070, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 39\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.776201265510049\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7676882510644685\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 40\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.754623815883873\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.725860581942047\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0069, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 41\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.764770047406242\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.75267590891363\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.1206478402018547\n",
      "\n",
      "Step size: tensor(0.0090, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 42\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.783313966573651\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.738912415197614\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.031540364027023315\n",
      "\n",
      "Step size: tensor(0.0091, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 43\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.730197465724082\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.765356904401079\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.111899733543396\n",
      "\n",
      "Step size: tensor(0.0074, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 44\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.776642234900351\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.791841786964806\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 45\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.706228451027062\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.705353671384488\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.26136064529418945\n",
      "\n",
      "Step size: tensor(0.0085, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 46\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.797118949250532\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.771348665784975\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.6260313987731934\n",
      "\n",
      "Step size: tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 47\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.744690859716227\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.754960870997319\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0068, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 48\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.745314117143398\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.722797211699368\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.07357767224311829\n",
      "\n",
      "Step size: tensor(0.0087, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 49\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.781326604446001\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.784415680287668\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.635899543762207\n",
      "\n",
      "Step size: tensor(0.0078, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 50\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.756265340082763\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.772413555604123\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.025172298774123192\n",
      "\n",
      "Step size: tensor(0.0086, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 51\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.7443462947394375\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.719089667780428\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0067, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 52\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.709286220113238\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.794010921559509\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.20896340906620026\n",
      "\n",
      "Step size: tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 53\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.816878330615481\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7514686272758615\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.18150445818901062\n",
      "\n",
      "Step size: tensor(0.0082, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 54\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.751132397287537\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.731973642411085\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.5621272921562195\n",
      "\n",
      "Step size: tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 55\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.791898831958546\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.792784259696379\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.9680784940719604\n",
      "\n",
      "Step size: tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 56\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.763221129640049\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.720367170678918\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.153875470161438\n",
      "\n",
      "Step size: tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 57\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.79094689810635\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.7544581011288045\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.15544162690639496\n",
      "\n",
      "Step size: tensor(0.0081, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 58\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.832827752762598\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.718093227192268\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.2918817102909088\n",
      "\n",
      "Step size: tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 59\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.791734311728318\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.774129971259354\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.18913646042346954\n",
      "\n",
      "Step size: tensor(0.0080, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 60\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.769983154674165\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.787612289844057\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "Step size: tensor(0.0065, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 61\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.803454380421527\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.774109020707721\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.30180513858795166\n",
      "\n",
      "Step size: tensor(0.0077, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 62\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.7663293736699375\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.726471738337757\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.46939319372177124\n",
      "\n",
      "Step size: tensor(0.0075, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 63\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.781741166052125\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.767497770036368\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.08006590604782104\n",
      "\n",
      "Step size: tensor(0.0079, grad_fn=<MulBackward0>)\n",
      "GLOBAL ITERATION: 0\n",
      "TRAINING AGENT: 1\n",
      "LOCAL ITERATION: 64\n",
      "\n",
      "Acquiring global rollout\n",
      "Total Reward: 6.726537143891963\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: 6.764952704867335\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.54503333568573\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ASSUMING ONE ALGORITHM SO FAR. WILL IMPLEMENT GENERAL STRUCTURE FOR DIVERSIFIED ALGORITHMS LATER\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#### Train the model #######################################\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m Federated_RL(policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[1;32m     22\u001b[0m                     envs\u001b[38;5;241m=\u001b[39menvs,\n\u001b[1;32m     23\u001b[0m                     num_agents \u001b[38;5;241m=\u001b[39m DEFAULT_AGENTS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m                     local_step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m\n\u001b[1;32m     31\u001b[0m                     )\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/server.py:115\u001b[0m, in \u001b[0;36mFederated_RL.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_r \u001b[38;5;241m=\u001b[39m [th\u001b[38;5;241m.\u001b[39mzeros_like(param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_models()\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh_models()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/server.py:48\u001b[0m, in \u001b[0;36mFederated_RL.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Train local policies\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m---> 48\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Aggregate local policies and update global policy\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_global()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/algorithms/fedsvrpg_m.py:164\u001b[0m, in \u001b[0;36mFEDSVRPG_M.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Update local policy\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Clear buffers\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_rollout_buffer\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/algorithms/fedsvrpg_m.py:61\u001b[0m, in \u001b[0;36mFEDSVRPG_M.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance sampling weight:\u001b[39m\u001b[38;5;124m\"\u001b[39m, IS_weight\u001b[38;5;241m.\u001b[39mitem(), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Calculate gradient estimates of each policy\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m lp_grads \u001b[38;5;241m=\u001b[39m [\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pi \u001b[38;5;129;01min\u001b[39;00m local_log_probs]\n\u001b[1;32m     62\u001b[0m gp_grads \u001b[38;5;241m=\u001b[39m [th\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs\u001b[38;5;241m=\u001b[39mpi, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_policy\u001b[38;5;241m.\u001b[39mparameters(), grad_outputs\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mones_like(pi)) \u001b[38;5;28;01mfor\u001b[39;00m pi \u001b[38;5;129;01min\u001b[39;00m global_log_probs]\n\u001b[1;32m     63\u001b[0m g_curr_policy \u001b[38;5;241m=\u001b[39m multiply_and_sum_tensors(scalar_tensor\u001b[38;5;241m=\u001b[39mlocal_returns, tensor_lists\u001b[38;5;241m=\u001b[39mlp_grads)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing on classic gym control environment first\n",
    "envs = [HoverAviary(obs=DEFAULT_OBS, act=DEFAULT_ACT) for _ in range(DEFAULT_AGENTS)]\n",
    "#envs = [gym.make('MountainCarContinuous-v0') for _ in range(DEFAULT_AGENTS)]\n",
    "env = envs[0]\n",
    "# Get the state size\n",
    "state_space = env.observation_space\n",
    "state_size = state_space.shape[1]\n",
    "# Get the action size\n",
    "action_space = env.action_space\n",
    "action_size = action_space.shape[1]\n",
    "\n",
    "layers = [512, 512, 256, 128]\n",
    "\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Action size:\", action_size)\n",
    "\n",
    "policy = GaussianMLPPolicy(input_size=state_size, output_size=action_size, hidden_layers=layers) # Will need some smarter way to initialize the policy within the model in the future\n",
    "# ASSUMING ONE ALGORITHM SO FAR. WILL IMPLEMENT GENERAL STRUCTURE FOR DIVERSIFIED ALGORITHMS LATER\n",
    "\n",
    "#### Train the model #######################################\n",
    "model = Federated_RL(policy=policy,\n",
    "                    envs=envs,\n",
    "                    num_agents = DEFAULT_AGENTS,\n",
    "                    global_iterations=10,\n",
    "                    local_iterations=100,\n",
    "                    env_kwargs=[2 for _ in range(DEFAULT_AGENTS)],\n",
    "                    algorithms=['FedSVRPG-M' for _ in range(DEFAULT_AGENTS)],\n",
    "                    max_episode_length=5,\n",
    "                    global_step_size=1e-2,\n",
    "                    local_step_size=1e-2\n",
    "                    )\n",
    "\n",
    "model.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below is unit testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_bias: torch.Size([])\n",
      "fc1.weight: torch.Size([512, 27])\n",
      "fc1.bias: torch.Size([512])\n",
      "fc_layers.0.weight: torch.Size([512, 512])\n",
      "fc_layers.0.bias: torch.Size([512])\n",
      "fc_layers.1.weight: torch.Size([256, 512])\n",
      "fc_layers.1.bias: torch.Size([256])\n",
      "fc_layers.2.weight: torch.Size([128, 256])\n",
      "fc_layers.2.bias: torch.Size([128])\n",
      "fc2.weight: torch.Size([1, 128])\n",
      "fc2.bias: torch.Size([1])\n",
      "\n",
      "torch.Size([512, 27])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256])\n",
      "torch.Size([128, 256])\n",
      "torch.Size([128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "main_model = model.get_model()\n",
    "model_params = []\n",
    "for i in main_model.named_parameters():\n",
    "    print(i[0] + ': ' + str(i[1].shape))\n",
    "    if 'fc' in i[0]:\n",
    "        model_params.append(i[1])\n",
    "print()\n",
    "for i in model_params:\n",
    "    print(i.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "SAC_model = SAC.load('/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/SAC_test_run/final_model.zip')\n",
    "#for i in SAC_model.get_parameters()['policy']:\n",
    "    #print(i + ': ' + str(SAC_model.get_parameters()['policy'][i].shape))\n",
    "    #if 'actor' in i:\n",
    "params = SAC_model.get_parameters()['policy']\n",
    "p1 = [param for param in params if 'critic.qf0' in param]\n",
    "p2 = [param for param in params if 'critic.qf1' in param]\n",
    "p3 = [(params[par1] + params[par2]) / 2 for par1, par2 in zip(p1, p2)]\n",
    "state_tensor = p3[0][:, :-1] \n",
    "action_tensor = p3[0][:, -1:]  \n",
    "print(len(action_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor.mu.0.weight: torch.Size([512, 27])\n",
      "actor.mu.0.bias: torch.Size([512])\n",
      "actor.mu.2.weight: torch.Size([512, 512])\n",
      "actor.mu.2.bias: torch.Size([512])\n",
      "actor.mu.4.weight: torch.Size([256, 512])\n",
      "actor.mu.4.bias: torch.Size([256])\n",
      "actor.mu.6.weight: torch.Size([128, 256])\n",
      "actor.mu.6.bias: torch.Size([128])\n",
      "actor.mu.8.weight: torch.Size([1, 128])\n",
      "actor.mu.8.bias: torch.Size([1])\n",
      "actor_target.mu.0.weight: torch.Size([512, 27])\n",
      "actor_target.mu.0.bias: torch.Size([512])\n",
      "actor_target.mu.2.weight: torch.Size([512, 512])\n",
      "actor_target.mu.2.bias: torch.Size([512])\n",
      "actor_target.mu.4.weight: torch.Size([256, 512])\n",
      "actor_target.mu.4.bias: torch.Size([256])\n",
      "actor_target.mu.6.weight: torch.Size([128, 256])\n",
      "actor_target.mu.6.bias: torch.Size([128])\n",
      "actor_target.mu.8.weight: torch.Size([1, 128])\n",
      "actor_target.mu.8.bias: torch.Size([1])\n",
      "critic.qf0.0.weight: torch.Size([32, 28])\n",
      "critic.qf0.0.bias: torch.Size([32])\n",
      "critic.qf0.2.weight: torch.Size([32, 32])\n",
      "critic.qf0.2.bias: torch.Size([32])\n",
      "critic.qf0.4.weight: torch.Size([1, 32])\n",
      "critic.qf0.4.bias: torch.Size([1])\n",
      "critic.qf1.0.weight: torch.Size([32, 28])\n",
      "critic.qf1.0.bias: torch.Size([32])\n",
      "critic.qf1.2.weight: torch.Size([32, 32])\n",
      "critic.qf1.2.bias: torch.Size([32])\n",
      "critic.qf1.4.weight: torch.Size([1, 32])\n",
      "critic.qf1.4.bias: torch.Size([1])\n",
      "critic_target.qf0.0.weight: torch.Size([32, 28])\n",
      "critic_target.qf0.0.bias: torch.Size([32])\n",
      "critic_target.qf0.2.weight: torch.Size([32, 32])\n",
      "critic_target.qf0.2.bias: torch.Size([32])\n",
      "critic_target.qf0.4.weight: torch.Size([1, 32])\n",
      "critic_target.qf0.4.bias: torch.Size([1])\n",
      "critic_target.qf1.0.weight: torch.Size([32, 28])\n",
      "critic_target.qf1.0.bias: torch.Size([32])\n",
      "critic_target.qf1.2.weight: torch.Size([32, 32])\n",
      "critic_target.qf1.2.bias: torch.Size([32])\n",
      "critic_target.qf1.4.weight: torch.Size([1, 32])\n",
      "critic_target.qf1.4.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from utils import TD3_policy_update\n",
    "TD3_model = TD3.load('/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/TD3_test_run/final_model.zip')\n",
    "policy_params = []\n",
    "value_params = []\n",
    "for i in TD3_model.get_parameters()['policy']:\n",
    "    print(i + ': ' + str(TD3_model.get_parameters()['policy'][i].shape))\n",
    "    for i in TD3_model.get_parameters()['policy']:\n",
    "        if 'actor.mu' in i:\n",
    "            policy_params.append(TD3_model.get_parameters()['policy'][i])\n",
    "        if 'critic.qf' in i:\n",
    "            value_params.append(TD3_model.get_parameters()['policy'][i])\n",
    "test = TD3_policy_update(TD3_model, policy_params, value_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_std: torch.Size([1])\n",
      "mlp_extractor.policy_net.0.weight: torch.Size([512, 27])\n",
      "mlp_extractor.policy_net.0.bias: torch.Size([512])\n",
      "mlp_extractor.policy_net.2.weight: torch.Size([512, 512])\n",
      "mlp_extractor.policy_net.2.bias: torch.Size([512])\n",
      "mlp_extractor.policy_net.4.weight: torch.Size([256, 512])\n",
      "mlp_extractor.policy_net.4.bias: torch.Size([256])\n",
      "mlp_extractor.policy_net.6.weight: torch.Size([128, 256])\n",
      "mlp_extractor.policy_net.6.bias: torch.Size([128])\n",
      "mlp_extractor.value_net.0.weight: torch.Size([32, 27])\n",
      "mlp_extractor.value_net.0.bias: torch.Size([32])\n",
      "mlp_extractor.value_net.2.weight: torch.Size([32, 32])\n",
      "mlp_extractor.value_net.2.bias: torch.Size([32])\n",
      "action_net.weight: torch.Size([1, 128])\n",
      "action_net.bias: torch.Size([1])\n",
      "value_net.weight: torch.Size([1, 32])\n",
      "value_net.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "PPO_params = []\n",
    "PPO_model = PPO.load('/Users/kevinhan/opt/anaconda3/envs/drones/lib/python3.12/site-packages/Federated_RL/test_run/best_model.zip')\n",
    "PPO_orig_params_OG = PPO_model.get_parameters()\n",
    "PPO_orig_params = PPO_orig_params_OG['policy']\n",
    "for i in PPO_orig_params:\n",
    "    print(i + ': ' + str(PPO_model.get_parameters()['policy'][i].shape))\n",
    "    if 'mlp' and 'policy' in i or 'action' in i:\n",
    "        PPO_params.append(PPO_orig_params[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 27])\n"
     ]
    }
   ],
   "source": [
    "new_params = []\n",
    "new_params = [mp - ppop for mp, ppop in zip(model_params, PPO_params)]\n",
    "print(new_params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip([param for param in PPO_orig_params if ('mlp' in param and 'policy' in param) or 'action' in param], new_params):\n",
    "    PPO_orig_params[i] = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "PPO_orig_params_OG['policy'] = PPO_orig_params\n",
    "PPO_model.set_parameters(PPO_orig_params_OG, exact_match = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policy': OrderedDict({'log_std': tensor([0.0155]), 'mlp_extractor.policy_net.0.weight': tensor([[ 0.2983,  0.0939,  0.0582,  ..., -0.3148, -0.2203,  0.0149],\n",
      "        [ 0.1385,  0.2232, -0.0835,  ..., -0.1183, -0.1324, -0.0246],\n",
      "        [ 0.1947,  0.0075, -0.1628,  ..., -0.0387,  0.0062,  0.1038],\n",
      "        ...,\n",
      "        [-0.1292, -0.3942, -0.0594,  ...,  0.0531, -0.0747,  0.1528],\n",
      "        [-0.0552,  0.2426,  0.0551,  ..., -0.0403,  0.0743, -0.1494],\n",
      "        [ 0.0859,  0.0312,  0.1152,  ...,  0.1407, -0.0177, -0.1886]]), 'mlp_extractor.policy_net.0.bias': tensor([ 1.2528e-02, -9.4204e-03, -9.1568e-03,  3.4986e-03, -1.2750e-02,\n",
      "        -9.8455e-05,  6.7938e-03,  1.2609e-02, -1.8640e-04,  5.3353e-03,\n",
      "         8.9538e-03,  5.2348e-04,  4.0354e-03, -4.9570e-03, -1.4039e-02,\n",
      "        -8.2345e-03,  1.2211e-02,  5.4926e-03,  1.4528e-02, -1.1743e-02,\n",
      "         1.1887e-02,  3.5642e-03, -8.1238e-04,  6.3826e-03,  9.9020e-03,\n",
      "        -1.2047e-02, -8.4845e-03, -9.1807e-03,  8.9473e-03, -1.9909e-04,\n",
      "        -1.3862e-03, -2.8767e-03,  1.5590e-03,  4.7453e-04, -5.1267e-03,\n",
      "         3.1095e-03, -3.7287e-03, -8.2870e-03,  7.5786e-03,  4.8332e-03,\n",
      "         2.4426e-03,  2.7431e-03, -2.3058e-03,  1.2877e-03, -6.8854e-03,\n",
      "         2.2894e-03,  8.2826e-03, -9.4496e-03,  7.8994e-04, -1.8326e-03,\n",
      "         1.1363e-02, -1.6595e-02,  1.1566e-02,  5.0676e-03, -7.8837e-04,\n",
      "        -1.7794e-02,  1.6988e-02,  7.4127e-03, -4.4636e-03, -4.7059e-03,\n",
      "         9.7597e-04, -2.7496e-03,  7.6562e-03, -4.2431e-03, -3.5425e-03,\n",
      "        -1.2986e-03,  3.8152e-03,  2.6356e-03,  6.2915e-03, -1.2204e-02,\n",
      "        -5.7708e-03, -2.4635e-03,  7.5855e-03,  9.6984e-03,  1.9103e-02,\n",
      "         4.5806e-03,  6.3054e-03, -1.0171e-02, -3.5862e-03,  7.6316e-05,\n",
      "         7.2269e-03, -9.8502e-03,  5.3460e-03,  5.8016e-03, -3.0848e-03,\n",
      "        -1.2915e-02, -5.7651e-04, -2.8001e-03,  1.2384e-03,  1.3074e-02,\n",
      "        -3.5729e-03, -7.0617e-03, -5.0341e-03, -5.1381e-03, -4.5685e-03,\n",
      "        -4.2996e-03, -3.6399e-03,  2.2323e-03,  6.4022e-03,  1.2813e-03,\n",
      "        -5.3233e-03,  1.2788e-02, -6.1104e-03, -4.6827e-04,  1.7630e-02,\n",
      "         2.0947e-02, -6.4981e-03, -4.2883e-03,  5.9318e-03,  2.0328e-03,\n",
      "        -1.8681e-02,  3.3884e-03,  1.1717e-02,  1.4591e-02,  2.2877e-04,\n",
      "         4.7066e-03,  2.4536e-03, -2.0784e-03,  1.0994e-02,  2.3924e-02,\n",
      "        -8.4519e-03,  3.8140e-03, -1.1473e-03,  5.0110e-03, -5.8453e-03,\n",
      "         8.5899e-03,  5.9569e-04, -4.0824e-05,  2.8774e-03, -5.4461e-05,\n",
      "         2.0701e-04,  4.6310e-03,  2.0777e-02,  7.8233e-03, -3.4334e-03,\n",
      "        -7.8082e-03,  1.2162e-02, -9.2527e-03, -2.0345e-03,  1.3361e-02,\n",
      "        -1.6538e-03,  5.3469e-03,  4.7759e-03, -1.6270e-02, -2.5412e-03,\n",
      "         8.7012e-03, -1.1522e-03, -9.7467e-03, -2.7062e-03,  8.5800e-04,\n",
      "         9.0461e-04,  1.6078e-02,  1.0022e-02, -5.8783e-03,  8.4400e-04,\n",
      "         5.5626e-03,  1.0221e-02,  1.6017e-02,  1.1923e-02, -5.3632e-03,\n",
      "        -5.6350e-03,  7.3912e-03, -7.8247e-03, -7.0867e-03, -1.5337e-03,\n",
      "         2.5963e-03,  1.0289e-02,  1.5091e-02,  1.4789e-02, -1.0410e-02,\n",
      "         1.0299e-03,  9.7673e-03,  1.6809e-04,  7.4078e-03, -4.2968e-03,\n",
      "        -9.9321e-03,  8.2922e-03, -6.6901e-03,  3.1581e-03,  2.1115e-03,\n",
      "         1.4124e-02,  1.4382e-02,  7.5036e-03,  3.0761e-03, -3.1683e-03,\n",
      "         2.2415e-03, -5.3911e-03,  1.3267e-02,  9.7951e-03,  5.5767e-03,\n",
      "        -6.6847e-03,  7.2442e-03, -1.1094e-02,  1.1567e-02,  8.0390e-03,\n",
      "         6.5184e-03,  1.0763e-02, -6.0819e-04,  4.4227e-03,  3.3932e-03,\n",
      "        -4.5335e-03, -2.9963e-03,  9.2929e-03,  1.5803e-02,  8.3030e-03,\n",
      "         2.8080e-03,  2.2023e-02,  6.2208e-03, -1.4890e-03, -8.1998e-04,\n",
      "         4.7749e-03, -1.6900e-03, -1.3464e-03,  2.9489e-03,  6.1758e-04,\n",
      "         9.6238e-04,  7.8257e-03, -1.5171e-02,  1.3525e-04, -5.9691e-03,\n",
      "         8.4335e-03,  1.3482e-02,  2.8832e-03,  5.0870e-03,  1.1913e-02,\n",
      "        -8.2598e-04,  5.2135e-03, -2.1121e-03, -4.9444e-03,  2.6444e-02,\n",
      "         1.7903e-03,  9.5137e-04,  5.2150e-03,  1.5703e-02,  1.4084e-02,\n",
      "        -1.5006e-02, -1.0318e-02,  1.1417e-02, -8.8790e-03, -3.7389e-03,\n",
      "        -1.9695e-03,  5.6997e-03, -9.3504e-03, -1.0416e-03, -7.4932e-03,\n",
      "         1.3501e-02,  3.6718e-03,  9.3867e-03, -1.9399e-03, -2.2627e-03,\n",
      "        -1.6825e-03, -1.2204e-04,  1.1828e-02,  2.0592e-03, -6.1883e-03,\n",
      "        -2.0523e-04, -9.8083e-05,  1.4633e-02,  5.7629e-03, -2.6628e-03,\n",
      "        -1.5362e-03, -8.5253e-03,  7.2881e-03,  1.0215e-02, -7.4034e-03,\n",
      "         9.1288e-03,  2.2767e-03,  7.2058e-03,  4.8684e-03,  4.5226e-03,\n",
      "         6.2952e-03,  1.5849e-02, -3.2266e-03, -1.0084e-02,  4.5649e-03,\n",
      "         1.1209e-02,  9.1130e-03, -8.7106e-03, -4.3117e-03,  5.7702e-03,\n",
      "         9.1120e-03,  3.1927e-03,  8.8027e-03, -1.6053e-02,  1.6964e-03,\n",
      "        -3.9182e-04,  1.5430e-03, -8.4307e-04,  9.9953e-03,  3.7426e-03,\n",
      "         4.1246e-03,  1.3397e-02, -1.9415e-03,  9.4644e-03, -4.6937e-03,\n",
      "         1.5488e-03,  1.6656e-03, -1.0850e-02, -8.1337e-03,  1.1819e-02,\n",
      "         1.1404e-02,  3.1936e-03, -5.0179e-03,  6.6074e-03,  9.0838e-03,\n",
      "        -1.5811e-02,  1.6870e-03, -1.6395e-03,  5.2409e-03,  7.6342e-03,\n",
      "         1.0559e-02,  1.6917e-02,  1.1642e-02, -1.1676e-02,  2.1790e-03,\n",
      "        -1.7016e-03, -2.0175e-03, -3.8746e-03, -1.0253e-03,  1.8812e-03,\n",
      "        -8.9407e-04,  9.7362e-03,  7.6041e-03,  1.1901e-03, -8.8107e-04,\n",
      "         2.8021e-02,  2.4180e-02,  7.4393e-03, -1.2625e-02,  4.0879e-03,\n",
      "         1.6346e-03,  1.1499e-02,  9.0100e-03,  3.0681e-03,  1.8505e-03,\n",
      "         6.4081e-03, -6.6099e-03,  1.1091e-03,  1.3096e-02,  2.0776e-03,\n",
      "        -1.2680e-02,  1.4675e-02,  1.1542e-02,  3.3866e-03, -1.0563e-02,\n",
      "        -1.1176e-03,  7.2962e-03,  4.2722e-03,  1.4776e-02, -8.2481e-03,\n",
      "        -3.0581e-03, -1.0422e-02,  6.9112e-03,  1.4537e-02,  1.2412e-02,\n",
      "        -1.2160e-02, -2.1554e-03, -9.4664e-03, -2.3791e-03,  1.2007e-02,\n",
      "         3.3167e-03, -2.4989e-03, -1.8821e-03, -2.4161e-03, -1.3338e-02,\n",
      "         7.4335e-03, -1.2680e-04,  5.9862e-03, -5.1015e-03, -1.1493e-02,\n",
      "        -1.4847e-03, -3.1654e-03,  5.8483e-03, -2.9929e-03,  1.0306e-02,\n",
      "         5.6592e-03,  9.5218e-04,  2.9277e-03, -7.1621e-03, -6.3689e-04,\n",
      "        -4.8698e-04,  6.0204e-03,  1.2244e-02,  9.4616e-03, -3.5491e-03,\n",
      "         4.8911e-03,  9.9086e-03, -3.2820e-03, -8.0483e-04,  6.1459e-04,\n",
      "        -1.9611e-02,  6.5510e-03,  8.4507e-03,  7.4963e-04,  6.5124e-03,\n",
      "        -5.5671e-03, -4.0157e-03,  1.8865e-02,  1.4387e-03, -8.0542e-03,\n",
      "         2.8596e-03, -1.1318e-02, -4.5673e-03,  1.1711e-02, -2.1431e-03,\n",
      "         6.5784e-03,  1.2518e-02,  8.6462e-03, -6.4200e-03, -7.8961e-04,\n",
      "         8.8847e-03,  6.3195e-03, -1.0745e-02,  6.4884e-03,  9.2512e-03,\n",
      "        -8.9924e-03,  6.4681e-03,  5.4326e-03,  2.1840e-03,  9.6068e-03,\n",
      "        -1.5633e-03, -1.1633e-04,  5.8200e-03,  2.3935e-02,  6.8438e-03,\n",
      "         6.1867e-03,  4.2168e-03, -2.6132e-03, -1.2044e-02, -7.0251e-03,\n",
      "         1.0539e-02,  3.8955e-03,  2.1068e-03,  6.6072e-03, -2.5388e-03,\n",
      "        -3.8968e-03,  1.0280e-02,  1.0759e-02,  4.0229e-03,  1.6479e-03,\n",
      "         1.9747e-02,  1.0666e-02,  7.1867e-03, -7.4854e-03,  1.2331e-03,\n",
      "         1.5442e-02, -3.5891e-03,  1.7189e-02, -9.4424e-04, -4.8250e-03,\n",
      "         1.3960e-02,  2.7968e-03,  9.4963e-03,  6.7175e-03,  8.9013e-03,\n",
      "        -1.0042e-02,  5.1462e-03,  1.7594e-04,  3.3056e-03,  1.4694e-02,\n",
      "         2.1964e-03,  1.3843e-02, -1.4268e-03, -2.3159e-03,  1.1908e-04,\n",
      "         2.3944e-03, -1.0783e-02, -1.0028e-02, -8.3574e-03, -8.8036e-03,\n",
      "         9.5785e-03,  2.0487e-03, -7.5615e-03, -8.3612e-03, -1.1282e-02,\n",
      "         2.6738e-04,  8.8183e-03,  5.7102e-03,  8.0932e-03,  1.2599e-02,\n",
      "        -1.4761e-03, -7.8809e-03, -8.4202e-03,  1.7196e-03,  1.2147e-02,\n",
      "         4.9829e-03,  5.4753e-03, -5.9156e-03,  6.1700e-03,  8.7211e-03,\n",
      "        -7.4377e-03,  1.1508e-02,  1.4978e-02, -1.5097e-03,  2.7455e-03,\n",
      "        -7.0202e-03,  1.1459e-02,  1.9386e-03,  2.3790e-04,  1.4537e-02,\n",
      "         6.9854e-03, -4.2744e-03,  6.1663e-03,  6.9418e-03, -1.2400e-02,\n",
      "         9.9107e-03, -1.5875e-02,  1.5706e-03, -4.3335e-03,  1.4204e-02,\n",
      "         3.2889e-03,  1.3922e-03]), 'mlp_extractor.policy_net.2.weight': tensor([[ 0.2378, -0.0444,  0.2353,  ...,  0.1116, -0.0081, -0.0346],\n",
      "        [-0.0906,  0.1344, -0.1428,  ...,  0.0119, -0.2099, -0.0919],\n",
      "        [-0.1890,  0.0157, -0.0646,  ..., -0.2296,  0.1701, -0.0445],\n",
      "        ...,\n",
      "        [ 0.1671, -0.0209, -0.1672,  ..., -0.1338,  0.0636,  0.0486],\n",
      "        [ 0.0796,  0.0271,  0.1364,  ...,  0.0201, -0.0282,  0.0743],\n",
      "        [-0.0423, -0.1442, -0.2212,  ...,  0.0437,  0.1461, -0.2809]]), 'mlp_extractor.policy_net.2.bias': tensor([ 4.2631e-03,  1.0626e-02, -4.5470e-03,  7.7056e-03,  7.7214e-03,\n",
      "         6.6944e-03, -7.7024e-03, -3.5765e-03, -7.3594e-03, -9.9295e-03,\n",
      "        -1.9272e-02,  1.1543e-02,  4.2901e-03, -1.2957e-02, -9.9625e-03,\n",
      "         3.5324e-03, -2.0376e-03,  1.4457e-02, -1.2569e-03,  2.0577e-03,\n",
      "        -1.2043e-03, -3.3059e-03,  3.0074e-03,  7.1076e-03, -1.9967e-03,\n",
      "         9.7161e-03,  1.7382e-03,  6.0723e-03,  5.9285e-03,  1.6038e-02,\n",
      "        -4.1188e-03, -9.8875e-03, -3.9618e-03,  4.9232e-04, -6.1991e-03,\n",
      "         6.9657e-03, -1.0173e-02,  6.1703e-03, -1.6344e-02, -3.8284e-04,\n",
      "        -6.6493e-04,  1.8438e-04,  5.5214e-03,  8.3624e-04,  8.2651e-03,\n",
      "         1.7323e-03,  5.6441e-03, -7.3455e-04,  2.5944e-04, -5.0251e-04,\n",
      "         2.3609e-03,  4.9589e-03,  5.6538e-03, -4.3444e-03,  3.1843e-03,\n",
      "         1.3900e-02, -1.1030e-03, -2.2548e-03,  1.9973e-03, -3.7935e-03,\n",
      "         1.1165e-02, -1.1608e-03,  1.6496e-02,  7.2966e-03,  8.3771e-03,\n",
      "         8.0423e-03, -7.9695e-03, -4.3677e-03, -3.2324e-03,  3.8290e-03,\n",
      "         4.2005e-03, -4.5428e-03, -7.9645e-03,  9.9233e-03,  5.1775e-03,\n",
      "         4.1519e-03,  9.7795e-03, -4.5713e-03, -3.3366e-03,  4.7528e-03,\n",
      "        -3.9037e-03, -5.8306e-03,  7.6942e-03, -4.4597e-03, -7.6241e-04,\n",
      "         1.4555e-03,  3.5992e-03,  4.4592e-04,  3.8269e-03,  5.9553e-03,\n",
      "        -5.8011e-03,  2.0236e-03,  6.4528e-03, -1.2436e-02, -2.7774e-03,\n",
      "         5.6631e-03, -3.0415e-03,  1.8944e-03,  1.0801e-02,  5.5475e-03,\n",
      "         1.5736e-03,  1.0536e-02,  2.6008e-04, -9.4246e-03, -1.3187e-03,\n",
      "         3.3209e-03,  1.0870e-02, -8.3932e-03,  1.8890e-02,  3.9116e-03,\n",
      "         1.8129e-03, -5.1569e-03,  3.4334e-03,  1.2080e-02, -1.5747e-03,\n",
      "         2.0424e-03,  5.5180e-04,  8.1184e-04,  7.7845e-05,  5.2930e-03,\n",
      "        -1.0482e-02,  1.2421e-02, -1.0922e-02,  4.9064e-03,  3.0636e-03,\n",
      "        -7.5301e-03, -2.4639e-03,  2.5316e-03,  7.6459e-03,  5.3346e-03,\n",
      "         2.2487e-03,  8.7411e-04, -6.6066e-03,  1.0676e-02,  2.0609e-04,\n",
      "        -5.4243e-03,  5.3239e-03, -8.5043e-03,  3.5096e-03,  4.5388e-03,\n",
      "         5.7576e-03,  3.5095e-03, -3.0100e-03, -1.5956e-03, -4.1234e-03,\n",
      "        -4.8749e-03,  3.0866e-03, -1.0351e-02,  7.7981e-03,  1.0340e-02,\n",
      "         2.1786e-03, -1.4769e-03,  3.5306e-03,  3.2378e-03,  5.7160e-03,\n",
      "         3.9974e-03, -3.5757e-03,  1.0556e-05,  1.9151e-02, -5.7371e-03,\n",
      "         2.6330e-03, -8.5646e-04, -5.8252e-03,  1.0229e-02,  8.2263e-03,\n",
      "        -1.5838e-03,  2.7545e-03, -7.5840e-04, -3.5457e-03,  1.2199e-02,\n",
      "        -3.5036e-03,  1.4477e-02,  5.4143e-03, -1.4863e-04,  3.7393e-03,\n",
      "         5.2817e-03,  4.6960e-03,  6.1599e-04,  1.5237e-02, -2.7700e-03,\n",
      "         4.2451e-03, -1.2206e-03, -5.8238e-03,  1.1897e-02,  1.1681e-03,\n",
      "         5.8532e-03,  6.4449e-03,  7.3350e-03,  1.7265e-03, -1.9772e-03,\n",
      "        -1.9175e-04,  6.3698e-04,  8.1790e-03, -7.0016e-03, -1.1102e-03,\n",
      "        -9.6052e-03,  1.0551e-02,  1.4068e-03, -1.2468e-03, -3.9874e-03,\n",
      "         2.0561e-04,  2.9683e-03, -3.4077e-03,  5.4717e-03,  1.5496e-03,\n",
      "         6.8787e-03, -4.2074e-03,  1.0778e-03,  5.6156e-03,  3.0308e-03,\n",
      "        -7.6376e-03,  5.9123e-03,  6.8678e-03, -3.5026e-03, -9.7225e-03,\n",
      "         8.4718e-04,  4.6064e-05,  4.6733e-04, -2.0186e-03,  2.0786e-03,\n",
      "        -4.5554e-03, -4.8073e-03,  5.0901e-03,  7.7823e-05,  4.7499e-04,\n",
      "         5.5145e-03, -1.5776e-03,  9.6677e-04,  8.3659e-03,  2.1781e-03,\n",
      "        -5.8309e-03,  3.7959e-03,  2.2129e-03,  2.8978e-03, -3.9056e-03,\n",
      "        -5.3786e-03,  3.0833e-03,  4.2958e-04, -6.7238e-04,  5.0959e-03,\n",
      "         2.2404e-03, -4.9918e-03, -2.2426e-03,  1.0064e-02, -2.4679e-04,\n",
      "        -1.6164e-04,  6.7407e-04,  1.8316e-04,  5.9223e-03, -2.5014e-03,\n",
      "        -4.1788e-03,  1.4335e-03,  6.5276e-03, -2.0869e-03,  8.4164e-04,\n",
      "         1.7839e-02, -6.5804e-03,  2.1491e-04, -1.1371e-03,  8.0091e-03,\n",
      "         6.0523e-03,  4.8981e-03,  2.5061e-03,  1.4663e-03,  7.2963e-04,\n",
      "        -5.3355e-03,  3.8105e-03,  2.8189e-03,  1.0758e-03, -2.7904e-03,\n",
      "         1.1312e-02, -4.9960e-03,  1.4350e-02, -4.0482e-03, -2.1279e-03,\n",
      "         5.5798e-03, -5.9959e-03, -1.2549e-02,  2.2796e-03,  3.0417e-04,\n",
      "         2.1998e-02,  7.5055e-03, -4.2938e-03,  2.2362e-02,  1.5750e-03,\n",
      "        -4.8665e-03,  3.9783e-03,  1.2679e-02, -2.1719e-03, -4.1842e-03,\n",
      "         1.3714e-03,  2.0430e-03,  1.6399e-03, -3.4728e-03,  4.6988e-04,\n",
      "         2.1669e-02, -2.7661e-03, -9.8606e-03, -5.6583e-03, -5.7944e-03,\n",
      "         1.1762e-02, -1.1430e-03, -2.7075e-03,  3.7677e-03, -1.9673e-03,\n",
      "         4.0082e-03,  1.6736e-03,  1.1999e-03,  3.1968e-03,  5.5354e-03,\n",
      "         1.3070e-02,  9.9788e-03, -7.4179e-04, -4.1769e-03,  1.0385e-02,\n",
      "         5.7331e-03,  6.0472e-03,  1.7022e-02,  3.8821e-03,  1.6890e-02,\n",
      "        -4.8647e-03, -1.0276e-03, -1.5682e-02,  3.1104e-03,  5.2159e-03,\n",
      "        -6.8244e-03, -1.1781e-03,  1.4245e-02,  1.8257e-02,  9.5095e-03,\n",
      "         7.7034e-03, -3.0054e-03,  1.4939e-03,  7.0432e-03,  4.4814e-03,\n",
      "         2.8753e-03,  1.4770e-03,  2.8241e-03,  7.0685e-03,  8.2706e-04,\n",
      "         4.0365e-03,  2.3205e-03,  5.8386e-03,  1.2075e-02,  8.8813e-03,\n",
      "        -1.0492e-02,  4.6667e-04, -1.6520e-03,  1.4629e-02,  9.3613e-03,\n",
      "         8.2363e-04, -6.1764e-03,  1.2195e-02,  1.2358e-02,  9.1168e-03,\n",
      "         1.7202e-02,  1.1134e-02,  1.6347e-02,  6.4003e-03, -2.3845e-03,\n",
      "         1.1492e-02,  1.2233e-03,  1.5434e-02,  9.7599e-03, -6.0125e-03,\n",
      "        -1.6520e-03,  7.1637e-03,  1.1394e-03,  3.1083e-03,  3.6331e-03,\n",
      "         5.9507e-05,  1.5751e-02,  8.4711e-03,  1.7366e-03,  7.5255e-04,\n",
      "         5.9939e-03,  8.2108e-04, -1.0533e-02,  2.1803e-03,  3.3091e-03,\n",
      "         4.0329e-04, -8.9958e-03,  7.1030e-04,  8.6060e-03,  3.5882e-03,\n",
      "         1.4262e-03, -3.3093e-04, -5.5930e-03, -4.8371e-03, -3.3255e-03,\n",
      "         1.3689e-02, -2.5434e-03,  1.0503e-02, -1.2380e-04, -3.0731e-03,\n",
      "         1.8719e-03,  1.6300e-02,  9.0509e-03,  1.0254e-02, -2.6731e-03,\n",
      "         6.0854e-03, -2.2871e-03,  7.8770e-03,  1.4946e-02, -8.9390e-03,\n",
      "        -1.1073e-03, -3.9626e-03, -1.7525e-03, -1.2616e-03, -1.7062e-03,\n",
      "         5.0972e-04,  5.1232e-03, -4.0130e-04, -5.5700e-03,  6.7019e-03,\n",
      "        -1.6919e-03, -4.7992e-03,  7.2146e-03,  9.6814e-04,  2.6045e-02,\n",
      "        -1.9643e-03, -7.8794e-04,  3.1848e-03, -2.6528e-03,  3.9204e-03,\n",
      "         8.1035e-04,  3.6355e-03, -1.4247e-03,  1.0701e-02,  3.5206e-04,\n",
      "        -1.2824e-04,  1.1931e-04,  1.2770e-02,  9.9363e-03,  2.1598e-02,\n",
      "         2.0402e-02,  1.3683e-03,  3.0229e-03, -1.3264e-02,  4.5681e-04,\n",
      "        -1.6194e-03, -7.2885e-04,  6.2881e-03, -8.1926e-03,  1.2354e-02,\n",
      "        -3.2544e-03,  1.4365e-03, -7.1849e-03, -1.1460e-02, -7.2305e-03,\n",
      "         7.6375e-03, -1.6654e-02,  2.0566e-03,  3.4522e-03,  4.0310e-03,\n",
      "        -1.8189e-03,  8.2158e-03,  1.1533e-02,  1.5834e-02, -1.4752e-03,\n",
      "         4.2351e-03, -9.2286e-03, -3.8474e-03, -2.1495e-03,  7.2108e-03,\n",
      "         1.6027e-02,  8.3132e-03, -4.9671e-03,  5.8791e-03, -5.6299e-03,\n",
      "        -5.4772e-03,  9.7169e-03,  1.2173e-02,  1.5290e-02,  1.1037e-02,\n",
      "        -1.0150e-02,  6.0183e-03,  6.2723e-03, -4.1221e-04, -4.2188e-03,\n",
      "         1.1430e-02, -1.1010e-03, -1.0479e-03, -3.6798e-03,  9.5434e-03,\n",
      "        -1.3282e-03, -6.6457e-04, -1.1538e-03, -1.7915e-03,  7.6580e-03,\n",
      "         9.5513e-03, -1.5356e-03, -6.4635e-03,  1.9981e-02, -1.0430e-02,\n",
      "        -2.9741e-03, -6.6721e-03,  2.7084e-03,  1.5732e-03, -1.1620e-02,\n",
      "         1.0939e-02,  1.0654e-02, -9.4129e-04,  1.3194e-03,  2.2339e-02,\n",
      "        -1.0041e-02,  8.5590e-03, -2.0636e-03, -3.5403e-03,  6.6409e-03,\n",
      "        -3.4367e-03,  2.5244e-02]), 'mlp_extractor.policy_net.4.weight': tensor([[ 0.0014,  0.1368,  0.0590,  ...,  0.0460, -0.2280, -0.0601],\n",
      "        [-0.0515,  0.1884, -0.2946,  ...,  0.0141, -0.0877,  0.0528],\n",
      "        [ 0.1137,  0.1303, -0.0537,  ..., -0.1605, -0.1606,  0.0229],\n",
      "        ...,\n",
      "        [-0.0256,  0.1839, -0.1650,  ..., -0.0031, -0.2133,  0.0047],\n",
      "        [ 0.0657,  0.2039,  0.0521,  ...,  0.2949,  0.2687,  0.0711],\n",
      "        [-0.0165, -0.1684,  0.1821,  ...,  0.2579, -0.0965, -0.0874]]), 'mlp_extractor.policy_net.4.bias': tensor([ 0.0009,  0.0066,  0.0027,  0.0028,  0.0104,  0.0011,  0.0013,  0.0161,\n",
      "        -0.0051,  0.0059,  0.0135,  0.0064, -0.0077, -0.0027,  0.0022,  0.0142,\n",
      "         0.0015,  0.0040, -0.0122,  0.0064,  0.0039, -0.0005, -0.0008, -0.0073,\n",
      "         0.0020,  0.0094, -0.0048,  0.0028, -0.0034, -0.0008,  0.0014,  0.0027,\n",
      "         0.0071,  0.0002,  0.0087, -0.0023,  0.0021,  0.0004, -0.0038, -0.0016,\n",
      "        -0.0151,  0.0008, -0.0002,  0.0030,  0.0093, -0.0114, -0.0051, -0.0049,\n",
      "        -0.0011, -0.0056,  0.0104,  0.0135, -0.0010, -0.0026,  0.0042, -0.0028,\n",
      "         0.0207,  0.0011, -0.0029,  0.0071,  0.0027,  0.0010, -0.0004, -0.0037,\n",
      "         0.0065,  0.0094,  0.0044,  0.0085,  0.0290,  0.0133, -0.0058,  0.0048,\n",
      "         0.0061, -0.0041, -0.0036,  0.0176,  0.0001,  0.0023,  0.0106,  0.0076,\n",
      "         0.0154, -0.0068, -0.0028, -0.0013,  0.0047,  0.0060, -0.0066, -0.0015,\n",
      "         0.0004,  0.0011, -0.0049,  0.0082, -0.0064, -0.0006,  0.0100,  0.0109,\n",
      "        -0.0103,  0.0020,  0.0075, -0.0022, -0.0040,  0.0121,  0.0027, -0.0082,\n",
      "         0.0108, -0.0114, -0.0025, -0.0121, -0.0004,  0.0252,  0.0053,  0.0101,\n",
      "        -0.0135,  0.0009, -0.0009,  0.0066, -0.0028, -0.0029, -0.0030,  0.0006,\n",
      "         0.0011, -0.0003,  0.0054,  0.0024, -0.0002, -0.0002,  0.0190,  0.0040,\n",
      "         0.0111,  0.0035, -0.0049, -0.0031,  0.0054, -0.0049, -0.0018, -0.0023,\n",
      "         0.0141, -0.0027, -0.0002, -0.0085,  0.0003,  0.0083,  0.0028, -0.0155,\n",
      "         0.0019,  0.0023,  0.0003,  0.0084,  0.0051,  0.0017,  0.0020, -0.0086,\n",
      "        -0.0076,  0.0002, -0.0061,  0.0127,  0.0087, -0.0029, -0.0061,  0.0082,\n",
      "         0.0063,  0.0058,  0.0266,  0.0039,  0.0024, -0.0044, -0.0109,  0.0075,\n",
      "        -0.0034,  0.0110,  0.0119,  0.0069, -0.0044,  0.0009,  0.0023,  0.0035,\n",
      "         0.0010,  0.0051, -0.0051, -0.0034,  0.0104, -0.0045, -0.0007,  0.0095,\n",
      "        -0.0136,  0.0020, -0.0005,  0.0026,  0.0089,  0.0117,  0.0002, -0.0002,\n",
      "         0.0018,  0.0029,  0.0022,  0.0161,  0.0125,  0.0079, -0.0037,  0.0230,\n",
      "         0.0043,  0.0105,  0.0087,  0.0069, -0.0011,  0.0112,  0.0077, -0.0048,\n",
      "        -0.0023, -0.0053, -0.0009,  0.0061,  0.0224,  0.0004, -0.0011,  0.0114,\n",
      "         0.0020, -0.0105,  0.0124,  0.0098,  0.0291, -0.0035, -0.0054,  0.0029,\n",
      "         0.0053,  0.0066, -0.0012,  0.0013, -0.0077, -0.0016, -0.0011,  0.0053,\n",
      "         0.0009,  0.0053,  0.0012,  0.0146,  0.0053,  0.0125,  0.0054, -0.0027,\n",
      "         0.0043, -0.0115, -0.0107,  0.0085, -0.0023,  0.0114,  0.0141,  0.0176,\n",
      "         0.0107, -0.0082, -0.0042,  0.0067,  0.0062,  0.0008, -0.0007,  0.0003]), 'mlp_extractor.policy_net.6.weight': tensor([[ 0.1203,  0.1730, -0.2511,  ...,  0.3113, -0.0639,  0.2320],\n",
      "        [-0.2729, -0.1567,  0.0351,  ..., -0.0494,  0.1753,  0.1244],\n",
      "        [-0.3840,  0.1714, -0.4988,  ...,  0.0675, -0.0931,  0.1059],\n",
      "        ...,\n",
      "        [ 0.2129, -0.2818,  0.1650,  ...,  0.0227,  0.1458, -0.0577],\n",
      "        [ 0.0256, -0.0099, -0.1714,  ...,  0.1258, -0.0167,  0.2440],\n",
      "        [-0.1339,  0.0383,  0.1747,  ..., -0.4029, -0.3531,  0.0241]]), 'mlp_extractor.policy_net.6.bias': tensor([-1.8034e-03,  1.3186e-02, -2.6893e-03, -9.9357e-04, -1.3913e-03,\n",
      "         8.8648e-03, -6.7254e-03,  2.0956e-02,  2.0238e-02,  6.7357e-03,\n",
      "        -2.1795e-03,  1.0360e-02, -1.0627e-03,  1.4082e-02,  4.2049e-03,\n",
      "        -2.9839e-03,  7.4309e-04, -4.3241e-03,  1.1568e-02,  2.6465e-03,\n",
      "         7.2461e-04, -2.3165e-03, -3.9736e-03,  9.3940e-04,  2.3833e-02,\n",
      "        -3.2416e-03,  1.0046e-02,  6.2617e-03,  9.7119e-03, -2.3565e-03,\n",
      "         8.5916e-05,  1.5184e-03, -1.1607e-03,  8.4232e-04,  3.5998e-03,\n",
      "         6.6583e-03,  6.9177e-04,  1.9022e-03,  2.8024e-02, -2.5320e-03,\n",
      "         1.4407e-03,  3.2240e-03,  9.7891e-04, -6.1850e-04, -1.3118e-03,\n",
      "         1.1292e-02, -5.1055e-03,  1.6264e-02,  5.4779e-03, -4.3111e-03,\n",
      "         1.2087e-02, -3.2456e-03,  1.3356e-03, -2.5143e-03,  5.1259e-03,\n",
      "        -5.3084e-04,  3.8626e-03, -5.9118e-03,  8.2452e-04, -1.8678e-03,\n",
      "        -4.1565e-03,  1.4466e-03,  6.1349e-03,  3.0582e-02,  7.2701e-03,\n",
      "         5.2691e-03,  2.7915e-02, -5.3606e-04,  6.3187e-03,  2.1297e-02,\n",
      "        -1.2286e-03,  8.4114e-03,  2.4521e-03,  1.3017e-02,  1.9825e-02,\n",
      "         1.5614e-05,  1.4498e-02, -3.0919e-04,  1.5476e-02,  9.8707e-04,\n",
      "         1.8712e-02,  2.4030e-03,  1.3522e-02,  1.4617e-03,  6.9303e-03,\n",
      "        -2.7649e-03,  2.1763e-02,  6.9971e-03,  3.2947e-03,  1.9480e-02,\n",
      "         4.3529e-03,  9.3452e-03,  1.6707e-02,  7.7589e-04,  1.0424e-02,\n",
      "         1.1206e-03,  1.6839e-03,  2.7695e-04,  2.4936e-03,  4.0628e-03,\n",
      "        -9.5327e-04,  1.4402e-02, -3.7858e-03,  2.9151e-03, -8.6112e-04,\n",
      "        -6.4606e-04,  7.5286e-03,  2.9485e-02,  1.3206e-02,  2.4694e-05,\n",
      "         2.0276e-02,  2.1829e-02, -3.8588e-04,  4.7897e-03, -1.8981e-03,\n",
      "         2.0164e-03,  7.3029e-04,  2.6402e-02,  0.0000e+00,  6.1636e-04,\n",
      "         1.5476e-03, -2.2588e-03, -1.4071e-03,  1.7639e-02,  1.0718e-02,\n",
      "         1.0848e-04, -3.3679e-04, -1.6672e-03]), 'mlp_extractor.value_net.0.weight': tensor([[ 1.7527e-01,  4.1432e-01,  2.7252e-01, -7.1504e-02,  2.7963e-01,\n",
      "         -5.9814e-01,  2.6501e-01,  1.8357e-01,  3.9262e-01, -1.2474e-01,\n",
      "          7.5256e-02,  1.9789e-01, -6.3647e-03, -2.9562e-01,  3.4958e-02,\n",
      "         -3.6459e-01,  1.3873e-01, -3.1233e-01, -8.3199e-01, -4.5410e-01,\n",
      "         -1.6865e-01,  2.3214e-01, -2.8107e-01, -2.8255e-01, -2.4110e-01,\n",
      "         -1.1548e-02,  4.0992e-01],\n",
      "        [ 7.4718e-01,  2.5851e-01,  3.4157e-01, -2.0715e-01, -1.6418e-01,\n",
      "          2.5131e-01,  6.3673e-02,  2.9800e-02,  4.0551e-01,  1.2729e-01,\n",
      "         -2.2288e-01,  1.8016e-02, -4.1107e-01,  3.5085e-02,  5.8189e-01,\n",
      "          3.2564e-01, -6.3737e-02, -5.0226e-01,  1.7758e-01,  2.3362e-01,\n",
      "         -2.3174e-01, -1.2361e-01, -2.0432e-01, -2.1975e-01,  5.2079e-01,\n",
      "         -5.2855e-01,  2.5872e-01],\n",
      "        [ 3.9740e-01, -2.1006e-01,  3.4627e-01, -3.8371e-01, -1.8869e-01,\n",
      "         -1.6674e-01,  2.1990e-02,  5.0713e-01,  3.9657e-01,  6.2339e-02,\n",
      "         -2.2458e-01,  3.9438e-01,  3.4553e-01,  7.6729e-02, -1.5637e-01,\n",
      "         -9.3819e-02,  2.8710e-01, -2.7786e-01, -2.6967e-01,  3.0661e-01,\n",
      "          2.9453e-01, -3.2783e-01,  2.7804e-01,  5.1644e-01, -5.8847e-01,\n",
      "         -1.8029e-01, -1.1225e-01],\n",
      "        [ 9.6904e-03,  3.8648e-02,  4.2015e-01, -4.4581e-03, -3.2369e-01,\n",
      "          1.2349e-01,  3.7616e-01, -1.7866e-01,  6.7111e-01,  4.6570e-01,\n",
      "         -1.7966e-01, -3.4007e-01,  4.1685e-01,  4.7341e-02,  4.6719e-01,\n",
      "         -2.0015e-02,  5.5069e-01,  4.2774e-01, -1.6554e-01, -1.2564e-01,\n",
      "          3.1877e-01,  3.6182e-01, -5.4285e-01, -8.3170e-02,  1.6016e-01,\n",
      "          3.5106e-01, -2.8914e-01],\n",
      "        [-1.6565e-01,  1.5931e-01,  3.8587e-01, -3.5014e-01, -2.2277e-01,\n",
      "         -2.3912e-01,  3.5408e-01,  3.5099e-01, -3.1492e-01,  2.5978e-01,\n",
      "         -6.4348e-03, -3.2649e-01,  5.9063e-02, -3.9545e-01,  1.7869e-01,\n",
      "         -9.1284e-03,  4.9073e-02, -5.8117e-01, -5.0400e-02,  2.7309e-01,\n",
      "          1.5648e-01, -3.5209e-01, -7.2813e-02,  4.0169e-01,  2.3484e-01,\n",
      "          3.8954e-01,  4.0568e-01],\n",
      "        [ 3.2767e-02,  4.6539e-01,  6.3479e-01,  5.1012e-02, -1.4662e-01,\n",
      "          1.2069e-01,  2.0014e-01, -2.7081e-01,  1.6327e-01, -3.2461e-01,\n",
      "          2.7198e-01,  1.3387e-01,  5.2525e-02,  1.1711e-01, -2.2083e-02,\n",
      "         -1.3146e-01,  5.3427e-02,  8.3164e-02,  1.2066e-01, -1.4436e-01,\n",
      "          5.9264e-01, -2.0913e-01, -2.7338e-01,  4.1064e-01, -1.2382e-01,\n",
      "         -9.4153e-01,  2.8776e-02],\n",
      "        [ 2.2252e-01, -3.7346e-01,  1.9101e-01, -2.7990e-01,  4.7950e-01,\n",
      "         -2.1848e-01,  2.3420e-01,  1.0770e-01,  2.8766e-01,  1.9274e-01,\n",
      "         -7.7768e-02, -7.4987e-02,  1.7934e-01,  4.9942e-01, -1.6437e-01,\n",
      "         -3.6553e-01,  1.9253e-01,  1.3827e-01,  5.3262e-01, -1.9825e-01,\n",
      "         -5.6170e-02, -3.7781e-01,  1.9654e-01, -2.2338e-01,  3.4215e-01,\n",
      "          5.1196e-03,  3.4736e-02],\n",
      "        [ 2.9258e-01,  3.1214e-01, -1.5307e-02, -5.6257e-01, -4.8928e-01,\n",
      "         -5.2348e-02, -2.1473e-02,  3.8618e-01,  3.9178e-01,  3.3315e-02,\n",
      "          4.3282e-01,  1.3269e-01,  5.0656e-01,  3.3915e-01,  1.4062e-02,\n",
      "          3.6258e-01, -2.3407e-01,  5.0435e-01,  1.1025e-01, -4.2058e-01,\n",
      "          3.6488e-01, -8.1180e-02,  2.0494e-01, -6.5388e-01,  3.5439e-01,\n",
      "          7.4543e-02,  1.4437e-01],\n",
      "        [ 1.6153e-01, -3.7792e-01,  1.6167e-02,  1.4434e-02, -3.5769e-01,\n",
      "         -2.0887e-01,  2.0259e-02, -3.8713e-01,  2.2588e-01,  2.6320e-02,\n",
      "         -2.8983e-02,  8.7413e-02, -4.6332e-01,  1.4636e-01, -4.4729e-01,\n",
      "         -1.4538e-01, -4.0183e-01, -2.8743e-01,  6.7737e-02, -3.0467e-01,\n",
      "          2.6693e-01, -3.9659e-02, -2.8654e-01,  2.1148e-01,  3.5313e-01,\n",
      "          1.7171e-02,  5.2630e-01],\n",
      "        [ 1.7911e-01, -3.0398e-02,  4.5955e-01, -1.4490e-01, -1.2957e-01,\n",
      "          3.4184e-01,  3.1797e-02,  2.5893e-01,  2.6402e-01,  8.6153e-02,\n",
      "         -1.3805e-01, -3.6547e-01, -3.3713e-02,  1.0790e-01,  1.1993e-01,\n",
      "          1.3252e-01, -2.8942e-01,  4.8812e-01, -1.3633e-01, -3.7734e-01,\n",
      "          5.0713e-02,  9.2354e-02,  3.9053e-01,  2.5037e-01, -4.5892e-01,\n",
      "          8.7945e-02,  9.2982e-01],\n",
      "        [-1.8803e-01,  4.0571e-01, -1.6757e-01, -3.1428e-01,  4.7260e-01,\n",
      "          1.7879e-01,  3.8674e-02, -8.2110e-02,  3.9148e-01, -2.1074e-01,\n",
      "         -3.6083e-01,  2.2462e-01,  2.5505e-01,  1.1329e-01, -1.7567e-01,\n",
      "         -1.4813e-02,  6.6666e-02,  1.4001e-02,  1.1784e-01,  9.6027e-02,\n",
      "         -1.0525e-01, -3.6580e-02, -5.9015e-02,  6.3737e-02,  2.9057e-01,\n",
      "          4.3303e-02,  2.4652e-01],\n",
      "        [-2.3467e-01, -5.8471e-02,  1.2203e-01, -5.9049e-01, -5.0214e-02,\n",
      "         -3.7499e-01,  1.5913e-02, -7.8244e-02,  6.6361e-02, -5.0095e-01,\n",
      "          2.0768e-02, -4.3245e-01,  8.8071e-02,  3.7198e-01,  1.7360e-01,\n",
      "          3.0229e-01,  1.0325e-01, -1.5492e-01,  3.3470e-01, -1.6066e-01,\n",
      "         -6.9010e-02,  2.2871e-01, -3.3399e-01,  3.3053e-02, -3.7897e-01,\n",
      "         -1.9409e-01, -1.1685e-01],\n",
      "        [ 3.2503e-01, -3.2783e-01,  7.4597e-02,  5.0810e-02,  2.2764e-01,\n",
      "          1.6387e-01,  4.4876e-01,  3.3777e-01,  3.3278e-01, -1.4605e-01,\n",
      "          3.4125e-01, -6.7322e-02,  7.4563e-02,  6.0662e-02,  4.3858e-01,\n",
      "         -1.4718e-01, -6.3186e-01, -2.4654e-02,  8.2369e-02, -3.9001e-01,\n",
      "         -5.5694e-01,  3.4996e-01, -2.5033e-01,  3.4742e-01, -4.7325e-02,\n",
      "         -2.9533e-01, -2.2674e-01],\n",
      "        [ 3.7456e-01,  5.5756e-02, -2.4249e-01, -7.7298e-02,  4.9003e-01,\n",
      "         -2.2543e-01,  4.0247e-01, -1.5910e-01, -1.2799e-01,  1.2803e-01,\n",
      "         -1.1741e-01, -3.3995e-01,  2.3640e-01, -6.0380e-01,  1.5001e-03,\n",
      "          3.6255e-01, -3.0829e-01,  1.7122e-01,  3.2773e-01,  1.0439e-01,\n",
      "          6.7489e-01, -1.7316e-02,  1.7811e-01, -9.2948e-02, -1.3210e-01,\n",
      "         -2.3007e-01,  1.1384e-02],\n",
      "        [-1.5253e-01, -4.7581e-02, -3.0518e-01, -1.5793e-01, -1.8316e-01,\n",
      "          2.0917e-01, -1.3229e-01, -1.5189e-01, -1.0309e-01,  5.4347e-02,\n",
      "          1.5245e-01, -1.2530e-01,  5.1911e-01, -1.1255e-01, -1.9530e-02,\n",
      "         -4.1331e-01,  5.3550e-01, -2.6748e-01, -3.6212e-01, -3.9538e-01,\n",
      "         -2.7589e-01,  2.9630e-01,  5.4993e-01,  3.0165e-01,  4.9534e-01,\n",
      "         -5.1657e-01,  3.5588e-01],\n",
      "        [ 2.5077e-01,  4.1500e-01, -1.3328e-01, -2.8484e-01, -3.7817e-01,\n",
      "          3.9477e-02,  3.5981e-01, -2.4628e-01,  4.5786e-01, -1.8438e-01,\n",
      "         -1.2281e-01, -1.6284e-02, -3.6974e-01,  1.6688e-02, -3.5077e-01,\n",
      "          2.6877e-01, -4.4469e-01, -1.8130e-01, -1.9818e-01, -1.1887e-01,\n",
      "         -8.1671e-02,  1.4792e-01,  6.1770e-01,  2.8706e-01, -2.8279e-01,\n",
      "          6.2196e-01, -3.9897e-01],\n",
      "        [ 1.0558e-01,  1.7114e-03, -1.4718e-01,  3.4478e-01,  8.2239e-02,\n",
      "         -9.2884e-02, -4.3649e-01,  6.5209e-02,  2.5700e-01,  2.8719e-01,\n",
      "          5.7236e-01,  7.9010e-02,  1.3062e-01, -1.9317e-01,  4.2763e-02,\n",
      "          4.5951e-01,  1.4161e-01, -4.3124e-02,  2.3002e-01, -2.2235e-02,\n",
      "         -4.6007e-02, -4.1883e-01, -3.8013e-01,  8.0216e-02, -2.4776e-01,\n",
      "          5.6870e-02,  3.3152e-02],\n",
      "        [-7.2471e-02, -3.2195e-02,  3.0167e-01, -1.0235e-02, -3.5184e-01,\n",
      "         -2.3793e-01,  9.4087e-02,  4.2576e-01,  6.4542e-02, -1.6799e-01,\n",
      "         -1.0460e-01,  2.4358e-01, -1.9455e-01, -6.3888e-02,  2.2128e-01,\n",
      "         -5.7173e-01, -3.3640e-01,  9.9841e-02,  2.0669e-01, -4.8027e-02,\n",
      "          4.9478e-01,  7.6790e-04,  1.7459e-01, -1.1783e-01,  6.1141e-01,\n",
      "          4.0241e-02, -5.6282e-01],\n",
      "        [ 2.6308e-01,  2.0095e-01, -1.9197e-01,  4.3481e-01,  2.6040e-02,\n",
      "         -1.0773e-02,  2.5336e-01,  5.1243e-01, -6.7212e-02, -1.5152e-01,\n",
      "          2.5351e-02,  4.4467e-02, -1.6447e-01,  3.2462e-02, -1.7188e-01,\n",
      "          3.5160e-02,  3.8542e-01,  8.2318e-02,  4.9436e-01, -1.2654e-01,\n",
      "          3.4364e-02,  4.4953e-01, -1.3200e-01,  3.2390e-01,  1.3087e-01,\n",
      "          1.1676e-01,  1.8808e-01],\n",
      "        [-1.2433e-01,  1.4679e-01,  3.1816e-02, -6.1480e-02, -6.5158e-02,\n",
      "         -4.9762e-02, -3.5849e-02,  3.5826e-01,  2.6679e-01,  4.3334e-01,\n",
      "         -5.5419e-01, -1.8503e-01, -2.9419e-01,  4.9933e-02, -7.1974e-01,\n",
      "          3.0046e-01, -1.4236e-01,  3.0168e-02, -3.3327e-01, -4.4011e-01,\n",
      "         -1.6795e-01, -1.0028e-02, -2.5028e-01, -9.9003e-02,  8.7462e-02,\n",
      "         -6.7459e-01, -4.5944e-01],\n",
      "        [ 5.7791e-01,  3.5886e-02, -7.5943e-02, -1.5535e-01, -2.0447e-02,\n",
      "         -2.2500e-01, -3.9347e-01,  1.2717e-02, -1.6016e-01, -2.5621e-01,\n",
      "         -5.0603e-01, -2.1534e-02, -2.7256e-02,  2.1353e-01,  4.4551e-01,\n",
      "         -2.1392e-01,  1.0704e-02,  5.8959e-01, -5.6251e-01, -1.3945e-01,\n",
      "          3.2035e-01, -1.4144e-01, -6.9461e-01,  9.4595e-02,  2.3837e-01,\n",
      "          2.0243e-01,  1.2048e-01],\n",
      "        [ 2.6232e-02,  1.6806e-01, -2.2494e-01,  4.2481e-01, -1.3613e-01,\n",
      "          1.9131e-01,  2.9822e-01,  2.0222e-01,  1.9876e-01, -6.3499e-02,\n",
      "         -2.4972e-01, -2.2589e-01,  2.9073e-01,  6.8575e-01,  3.9200e-02,\n",
      "         -4.6395e-01, -8.4155e-02, -7.2560e-01,  4.7215e-02,  1.8399e-01,\n",
      "          3.6715e-01, -2.4668e-01, -1.9109e-01, -6.4314e-01, -3.3050e-01,\n",
      "          9.0400e-02,  1.6235e-01],\n",
      "        [ 6.7337e-02,  4.5836e-03,  5.3575e-02,  1.2222e-01, -3.9650e-01,\n",
      "         -3.5106e-01, -5.7238e-02,  1.1796e-01,  4.8910e-01, -1.9908e-02,\n",
      "         -3.1956e-01, -1.2168e-01,  7.7854e-01, -3.3956e-01,  1.3466e-01,\n",
      "         -1.4656e-01, -6.6168e-01,  2.7493e-01,  3.7532e-01,  6.7823e-01,\n",
      "         -4.8965e-01,  3.6120e-01, -7.3188e-02,  3.0671e-01, -6.6013e-02,\n",
      "         -2.2473e-01,  2.3772e-01],\n",
      "        [ 2.9957e-01,  1.0691e-02,  1.9738e-01, -4.2608e-01, -6.4969e-02,\n",
      "          5.1449e-01, -3.8322e-01,  2.6397e-01,  4.2308e-02, -2.0886e-01,\n",
      "         -1.3926e-01, -1.9182e-01, -9.7535e-02, -3.4979e-01, -1.7812e-01,\n",
      "         -3.6956e-01,  1.2834e-01, -3.8152e-01,  2.3675e-01, -6.4002e-02,\n",
      "          3.7433e-01,  4.0398e-01, -2.7393e-01, -9.1654e-02, -2.5392e-01,\n",
      "          2.2981e-01, -3.7338e-01],\n",
      "        [ 1.0768e-01,  4.0376e-02,  4.5853e-02, -3.8493e-01,  2.5212e-02,\n",
      "          2.3602e-01,  1.3854e-02, -7.8053e-02,  3.9731e-01,  1.0980e-02,\n",
      "         -5.0478e-02,  1.6005e-01,  1.1513e-01, -7.6183e-01, -5.0719e-01,\n",
      "         -6.6817e-01, -3.1654e-02, -2.8139e-01,  3.9362e-01, -7.4457e-02,\n",
      "          4.8777e-02, -1.4920e-01, -5.9676e-01, -4.4742e-02, -2.3431e-01,\n",
      "          3.0246e-01, -5.3800e-02],\n",
      "        [ 5.8623e-02, -3.5860e-01,  4.1303e-01,  1.1374e-01, -1.6521e-01,\n",
      "         -7.5280e-02, -4.0825e-02,  1.5913e-01,  3.8516e-01, -4.6731e-01,\n",
      "         -2.9863e-01, -8.6030e-03,  4.5103e-01, -5.6621e-01, -2.1690e-01,\n",
      "          5.9246e-01,  1.9091e-01, -3.1091e-01, -2.2658e-01,  1.9258e-02,\n",
      "          3.5350e-01,  5.2932e-01,  2.7333e-01, -7.5426e-01,  2.8159e-01,\n",
      "         -1.4222e-02,  2.7815e-01],\n",
      "        [ 4.5648e-01,  1.5180e-02,  1.7715e-01,  4.2959e-02,  2.0125e-01,\n",
      "         -2.1255e-02,  3.1510e-02, -2.7446e-01, -5.0414e-01,  3.3451e-01,\n",
      "          3.2293e-01,  7.1979e-03,  3.3631e-01,  3.5316e-01, -4.4022e-01,\n",
      "          5.3831e-02, -1.5454e-02, -1.5703e-01, -1.1690e-01,  1.9292e-01,\n",
      "         -6.9574e-02,  3.9608e-01, -5.2731e-02,  5.8893e-02,  7.1713e-02,\n",
      "          3.3502e-02, -7.6532e-02],\n",
      "        [-5.7397e-02,  1.6805e-01, -6.8462e-02, -9.1016e-03,  2.8399e-01,\n",
      "         -3.2680e-01, -4.8657e-01, -8.7905e-02,  2.6242e-01,  3.5939e-01,\n",
      "          1.8974e-01, -1.1451e-01, -2.8224e-01,  4.6008e-02,  2.4798e-01,\n",
      "         -2.7440e-01, -8.6133e-02, -1.7189e-01,  8.5318e-02,  1.0054e-01,\n",
      "          2.6695e-01,  5.1114e-01,  3.6534e-01, -3.6847e-02, -1.3585e-01,\n",
      "         -7.5362e-02,  4.1809e-02],\n",
      "        [-2.3645e-01,  2.2685e-02,  3.6182e-01, -1.2860e-02,  4.0863e-01,\n",
      "          3.9233e-01, -1.1053e-01,  3.0175e-01,  3.2454e-01,  4.6466e-03,\n",
      "         -5.5409e-02,  2.5029e-02,  1.7674e-01,  1.8403e-01,  4.5591e-02,\n",
      "          4.1716e-01, -8.2908e-01, -5.9037e-02, -3.4440e-01,  2.3884e-01,\n",
      "          4.4248e-01,  1.9203e-01, -1.3642e-01,  1.8710e-01,  4.8662e-01,\n",
      "          2.0758e-01, -2.4107e-04],\n",
      "        [ 2.7357e-02,  1.8577e-01,  4.7102e-01,  5.1196e-02, -5.3655e-02,\n",
      "         -5.0701e-02,  5.0695e-03, -1.8668e-01, -1.3899e-01,  3.0657e-01,\n",
      "         -5.0530e-01,  5.9788e-01,  4.0156e-01,  5.4577e-02,  4.1609e-01,\n",
      "          1.6196e-01, -2.0741e-01, -2.4805e-01,  6.4968e-01, -7.8961e-01,\n",
      "         -1.5753e-01,  1.4245e-01,  9.7900e-02, -8.8313e-02, -2.5128e-01,\n",
      "          2.6757e-01, -8.0870e-02],\n",
      "        [ 4.0009e-01,  3.6608e-01,  5.8651e-01,  2.2654e-01, -4.7562e-02,\n",
      "         -5.5460e-02, -2.3030e-01,  2.8345e-02,  1.2574e-01, -2.7197e-01,\n",
      "         -6.9130e-02, -5.4307e-01,  2.1004e-01, -1.7198e-01, -2.5408e-01,\n",
      "         -3.8523e-01, -2.5050e-01,  3.6778e-02,  1.3625e-01, -2.1837e-01,\n",
      "         -4.0256e-01, -4.2201e-01,  3.4752e-01, -1.8847e-01,  5.0667e-02,\n",
      "          3.5518e-02, -2.3683e-01],\n",
      "        [ 2.5435e-01,  6.0726e-02,  1.1417e-01,  1.2416e-01, -2.0892e-02,\n",
      "         -1.6043e-01, -1.8930e-01, -2.7000e-02,  5.1561e-01, -2.4764e-01,\n",
      "         -1.8659e-01, -1.2379e-01,  3.7513e-01,  1.7085e-01,  1.8179e-01,\n",
      "          2.9632e-01,  3.1369e-01, -5.6579e-01, -2.2359e-01, -4.9340e-01,\n",
      "          4.6562e-01, -4.5988e-01,  3.3792e-01,  7.6673e-01,  3.1443e-01,\n",
      "          3.0918e-01, -4.5953e-01]]), 'mlp_extractor.value_net.0.bias': tensor([ 0.2113,  0.2038,  0.1454,  0.1900,  0.1860,  0.2278,  0.1351,  0.2414,\n",
      "         0.0595,  0.2027, -0.0447,  0.0261,  0.2387,  0.1368,  0.1923,  0.2425,\n",
      "         0.0691,  0.2073, -0.0643,  0.1990,  0.2597,  0.2373,  0.2473,  0.1729,\n",
      "         0.2244,  0.2095, -0.1146, -0.0625,  0.2275,  0.2250,  0.0943,  0.2139]), 'mlp_extractor.value_net.2.weight': tensor([[-0.1245, -0.4831, -0.3305,  ...,  0.3989, -0.1116, -0.3841],\n",
      "        [-0.1047, -0.0526,  0.0294,  ...,  0.6511,  0.0285,  0.3728],\n",
      "        [ 0.0247, -0.0600,  0.0247,  ..., -0.0716, -0.1416,  0.0189],\n",
      "        ...,\n",
      "        [ 0.0711, -0.2907, -0.1882,  ...,  0.1257,  0.0129,  0.0729],\n",
      "        [ 0.2911,  0.0085, -0.6286,  ...,  0.0859,  0.1527,  0.0829],\n",
      "        [ 0.4855,  0.2264,  0.2915,  ...,  0.3892,  0.1242,  0.4973]]), 'mlp_extractor.value_net.2.bias': tensor([-0.0538, -0.0220, -0.0572,  0.1616, -0.0433, -0.0734, -0.0471, -0.0546,\n",
      "         0.1570, -0.0337,  0.0398, -0.0471, -0.0227,  0.1968, -0.0632,  0.1517,\n",
      "         0.2323, -0.0629, -0.0347,  0.0078,  0.1967,  0.2288, -0.0817,  0.2085,\n",
      "        -0.0565, -0.0575,  0.2046, -0.0611, -0.0679, -0.0682, -0.0676,  0.1924]), 'action_net.weight': tensor([[-1.7260e-02, -1.0693e-01, -1.4927e-02,  1.5096e-02,  4.8780e-03,\n",
      "         -4.9448e-02, -2.2289e-02,  7.5860e-02, -8.2619e-02,  5.8923e-02,\n",
      "         -2.2522e-02,  3.1201e-02, -2.7923e-02,  4.3830e-02,  2.8708e-02,\n",
      "          1.6002e-02, -4.4176e-02, -3.1320e-02,  6.6693e-02,  2.3344e-02,\n",
      "         -1.3386e-02, -4.0387e-03,  1.1654e-02,  1.8523e-02,  7.5623e-02,\n",
      "          2.9668e-04,  3.7866e-02,  4.6177e-02,  3.6501e-02, -1.5538e-02,\n",
      "         -4.9212e-03, -2.6270e-02, -1.9377e-03, -1.2775e-02, -2.1469e-02,\n",
      "          3.8968e-02,  3.1160e-03,  1.7772e-02,  9.2834e-02,  9.6147e-03,\n",
      "         -2.2282e-02, -4.1897e-02,  1.2813e-02, -7.5075e-03, -1.0951e-02,\n",
      "         -6.2677e-02, -2.0778e-02,  8.4862e-02,  3.6848e-02, -4.2419e-02,\n",
      "         -6.9475e-02, -9.5725e-03, -1.2557e-02, -1.0240e-04,  3.1661e-02,\n",
      "         -1.5203e-03, -2.6244e-02, -1.5487e-02, -8.0578e-03,  5.7131e-04,\n",
      "          1.4808e-02, -1.2768e-02, -2.9019e-02, -1.0438e-01, -6.1309e-02,\n",
      "          3.2358e-02,  8.8981e-02,  6.6849e-03,  2.8706e-02,  1.0502e-01,\n",
      "         -7.6403e-04,  5.5288e-02, -1.7042e-02,  6.4405e-02,  8.1899e-02,\n",
      "          2.1344e-03, -6.3186e-02, -6.7132e-04,  5.7759e-02,  9.5162e-03,\n",
      "          8.2155e-02, -2.7101e-02,  7.0043e-02,  1.8553e-02,  4.9833e-02,\n",
      "         -3.4369e-03, -9.7455e-02,  4.0645e-02,  1.8241e-02,  7.5539e-02,\n",
      "         -3.4125e-02,  3.5815e-02,  8.6077e-02, -1.1195e-02, -8.8716e-02,\n",
      "          9.8187e-03,  3.2714e-02,  1.5776e-03, -3.2178e-02, -2.9284e-02,\n",
      "         -7.9304e-02,  5.7308e-02, -2.9407e-02, -2.9103e-02,  7.7918e-03,\n",
      "         -3.3548e-03,  4.8545e-02,  1.1219e-01, -5.0723e-02, -2.0141e-03,\n",
      "         -5.9984e-02, -1.2580e-01, -3.7971e-03, -7.1903e-02, -1.5461e-02,\n",
      "         -2.2954e-03,  1.4894e-02,  9.0371e-02, -3.3970e-04, -5.9961e-03,\n",
      "         -3.7944e-02, -1.3329e-02,  1.9837e-03, -6.9690e-02,  2.9726e-02,\n",
      "         -1.6882e-03, -3.8784e-04, -9.4073e-03]]), 'action_net.bias': tensor([0.0177]), 'value_net.weight': tensor([[-3.2878e-01,  4.7162e-02, -1.6641e-02,  4.0214e-01,  9.4541e-05,\n",
      "         -1.9114e-01, -5.1212e-03, -5.8801e-02,  3.1741e-01,  1.4292e-02,\n",
      "          9.1458e-02, -1.1654e-02, -7.5002e-02,  3.0572e-01, -4.6800e-01,\n",
      "          1.6234e-01,  2.6220e-01, -1.1975e-01,  2.7487e-02,  4.6810e-02,\n",
      "          2.7734e-01,  3.0588e-01, -1.6915e-01,  2.6364e-01, -1.6457e-01,\n",
      "         -4.3495e-02,  2.7581e-01, -2.5734e-01, -5.1009e-02, -1.9269e-01,\n",
      "         -1.1916e-01,  4.4660e-01]]), 'value_net.bias': tensor([0.1217])}), 'policy.optimizer': {'state': {0: {'step': tensor(640.), 'exp_avg': tensor([-0.0002]), 'exp_avg_sq': tensor([4.0818e-07])}, 1: {'step': tensor(640.), 'exp_avg': tensor([[ 1.3008e-07, -2.7070e-07, -7.4028e-08,  ...,  6.0742e-07,\n",
      "          7.4199e-07,  5.2031e-07],\n",
      "        [ 2.6939e-08,  3.8329e-07,  5.7754e-07,  ..., -5.2821e-07,\n",
      "          6.1343e-07, -7.0621e-07],\n",
      "        [-5.8619e-07, -1.3632e-07, -6.5821e-07,  ...,  3.3752e-07,\n",
      "         -1.1390e-07, -1.1633e-06],\n",
      "        ...,\n",
      "        [-8.7303e-08, -2.0694e-08,  2.8994e-07,  ...,  6.8739e-07,\n",
      "          9.8456e-09, -7.2638e-07],\n",
      "        [-1.3081e-07,  2.5532e-07, -1.1221e-07,  ..., -1.0603e-07,\n",
      "          2.3955e-07,  2.1468e-07],\n",
      "        [-2.0416e-07,  2.9779e-07,  1.2582e-06,  ..., -1.8242e-07,\n",
      "          1.0700e-06,  3.5118e-08]]), 'exp_avg_sq': tensor([[2.4460e-13, 5.2183e-13, 2.0361e-12,  ..., 1.6441e-12, 1.4431e-12,\n",
      "         1.7145e-12],\n",
      "        [5.2490e-13, 7.7338e-13, 8.1381e-13,  ..., 1.5303e-12, 1.1814e-12,\n",
      "         1.4719e-12],\n",
      "        [5.2849e-13, 1.6587e-12, 1.0662e-12,  ..., 2.4215e-12, 2.3401e-12,\n",
      "         2.5917e-12],\n",
      "        ...,\n",
      "        [4.9583e-13, 2.9817e-13, 1.0538e-12,  ..., 1.2752e-12, 1.1485e-12,\n",
      "         1.2728e-12],\n",
      "        [6.0146e-13, 5.5907e-13, 1.7311e-12,  ..., 1.4853e-12, 1.8441e-12,\n",
      "         1.4327e-12],\n",
      "        [1.9666e-13, 2.9659e-13, 2.0461e-12,  ..., 1.2934e-12, 1.3011e-12,\n",
      "         1.3310e-12]])}, 2: {'step': tensor(640.), 'exp_avg': tensor([-7.0180e-07,  6.5406e-07, -1.6966e-06, -9.1089e-08,  1.6108e-06,\n",
      "         1.6498e-07, -1.0118e-08,  5.0265e-07, -1.0326e-07, -2.7342e-07,\n",
      "         1.7392e-06,  3.8227e-08,  9.6421e-07, -7.9704e-07,  9.0610e-07,\n",
      "        -7.1719e-07, -2.2450e-06,  1.4494e-06, -1.1228e-06,  1.7265e-06,\n",
      "         2.2914e-07,  9.1531e-08,  7.5116e-07, -3.6656e-07,  3.1331e-07,\n",
      "         4.5202e-07,  2.1075e-06,  8.1093e-07,  4.8902e-07,  6.5450e-07,\n",
      "         3.4589e-06, -3.0928e-07, -1.1451e-06,  1.3506e-06, -1.9873e-06,\n",
      "        -3.0230e-06, -1.1641e-07,  4.7648e-07, -1.0230e-06,  1.5384e-06,\n",
      "        -1.7893e-07, -1.4517e-06,  6.2583e-08, -1.2427e-06, -3.5534e-08,\n",
      "         1.8343e-07,  1.5074e-06,  1.3449e-06, -4.7616e-08,  1.9615e-07,\n",
      "        -1.2283e-06,  8.7103e-07,  4.6062e-07,  2.7381e-06,  1.2311e-06,\n",
      "        -2.6588e-07,  7.3865e-07, -4.8905e-08,  6.7209e-07, -1.9939e-07,\n",
      "         1.8822e-06, -1.9220e-06,  1.0923e-06,  5.0152e-07,  4.0719e-07,\n",
      "        -2.3981e-07, -6.2819e-07,  5.8097e-08, -4.1610e-06,  3.0632e-07,\n",
      "         2.8636e-07,  4.9126e-07, -4.1663e-07, -1.2281e-06, -3.2791e-06,\n",
      "         2.9464e-06, -3.3501e-06,  7.0143e-08, -2.6365e-06,  1.6486e-06,\n",
      "        -1.9718e-06,  1.5065e-06,  6.9815e-07, -2.2397e-06, -7.3369e-07,\n",
      "         1.5550e-07, -1.6494e-06,  1.9722e-07, -1.7170e-06, -4.6853e-07,\n",
      "         2.1304e-06,  1.9464e-06, -5.5855e-07,  3.3853e-07, -2.1607e-07,\n",
      "         2.1845e-06,  2.3514e-06,  6.2292e-07, -3.6142e-06, -1.6286e-06,\n",
      "         2.2876e-06, -6.5344e-07, -1.4759e-06, -9.5906e-07, -2.3405e-07,\n",
      "        -4.8770e-06, -3.7527e-06,  1.7778e-06, -1.8409e-07,  1.0153e-07,\n",
      "         1.1996e-06, -1.4342e-06,  4.1945e-07, -1.3427e-06, -8.0575e-07,\n",
      "        -1.3302e-06, -5.0512e-07,  1.8095e-06,  2.7203e-06,  3.9524e-07,\n",
      "         5.8830e-07,  2.7697e-06,  9.0763e-07, -3.9143e-07,  1.1402e-06,\n",
      "        -3.9500e-07,  1.2984e-06,  1.4994e-06, -1.8287e-06, -4.0657e-06,\n",
      "         4.6420e-07, -1.9064e-06, -2.8656e-06, -1.5773e-06,  1.3543e-06,\n",
      "        -1.5212e-07, -2.0345e-06, -2.6985e-06, -2.1210e-07, -2.1800e-06,\n",
      "        -7.4481e-08, -2.8063e-07, -3.0465e-07, -1.6072e-06,  8.4787e-07,\n",
      "        -2.0850e-06, -1.4302e-06,  1.7926e-06, -8.6876e-07, -1.1312e-06,\n",
      "         6.0202e-07,  1.3481e-06,  3.2370e-07,  2.3215e-07, -4.5015e-07,\n",
      "        -9.7758e-07, -2.6521e-06, -3.2427e-09, -2.2118e-06,  2.0886e-07,\n",
      "        -3.2020e-06, -3.1274e-07, -6.1664e-07,  8.0123e-07, -9.5019e-07,\n",
      "        -7.4510e-07, -3.8767e-07, -4.2356e-06, -2.0808e-06,  3.1663e-07,\n",
      "         1.9910e-06,  9.8577e-07,  9.2423e-07, -1.0274e-06,  2.7489e-07,\n",
      "        -9.4205e-07, -1.1391e-08,  7.4762e-07, -4.0172e-07,  2.8314e-06,\n",
      "         9.4058e-07, -3.2702e-06,  9.9408e-07, -2.0637e-06, -1.4507e-06,\n",
      "        -6.9698e-07, -1.0504e-06, -1.0963e-06,  1.3553e-07, -1.6668e-06,\n",
      "         1.0913e-06,  4.6524e-07,  4.4852e-07,  4.3654e-07,  2.3826e-06,\n",
      "         3.7584e-06,  2.2951e-06,  1.1057e-06, -2.9408e-06,  2.4875e-07,\n",
      "         1.0352e-06,  2.5479e-06, -2.8923e-06,  2.8680e-09,  3.1113e-06,\n",
      "        -1.1849e-06, -3.5653e-06, -2.1747e-06, -2.6410e-07, -1.7918e-06,\n",
      "        -4.4150e-07,  1.1373e-06,  7.9232e-07, -1.1015e-06, -3.6076e-07,\n",
      "         1.4572e-07, -1.4775e-07,  5.5836e-07,  7.3671e-07,  5.8846e-07,\n",
      "        -1.2126e-06,  1.5383e-07,  4.7515e-07, -5.9645e-07,  9.9114e-07,\n",
      "         2.6078e-07, -4.2302e-07,  3.4679e-06,  1.2933e-06, -2.1712e-06,\n",
      "         5.3704e-07,  2.7705e-07, -8.1283e-07, -3.0430e-06,  2.1050e-06,\n",
      "         2.8107e-06,  2.3749e-06, -7.4969e-07,  4.5561e-07, -3.8035e-07,\n",
      "         2.1308e-06,  5.8097e-07, -3.4316e-06,  1.5609e-06, -1.4886e-06,\n",
      "        -1.0832e-06,  1.8741e-06, -4.9678e-07,  6.0411e-07,  1.1503e-06,\n",
      "         1.8208e-07, -5.7923e-08, -1.0919e-06,  6.3564e-07, -1.7616e-06,\n",
      "         1.8000e-06,  2.2580e-07, -2.5756e-06, -5.8474e-07,  1.4573e-06,\n",
      "        -2.5016e-07, -1.2819e-06, -2.1414e-06, -7.5266e-07,  1.0398e-06,\n",
      "         4.3926e-07,  3.3765e-08,  2.3444e-07, -9.0513e-07,  1.1944e-06,\n",
      "         1.1003e-06, -1.3242e-07, -4.1455e-07, -1.2303e-07, -6.4952e-07,\n",
      "        -4.5329e-07, -3.0524e-07,  2.2964e-06, -2.9527e-07, -3.9980e-07,\n",
      "        -7.5689e-07, -5.6882e-07,  1.0825e-06,  9.8701e-07,  7.9466e-07,\n",
      "         1.0910e-06, -1.8435e-06, -3.7494e-07,  9.9746e-07, -1.2551e-07,\n",
      "         8.0931e-07,  5.5559e-07,  3.8441e-06, -1.0144e-06, -4.4666e-07,\n",
      "         3.2855e-07,  1.6397e-06,  2.9873e-06, -5.9708e-07, -3.0914e-06,\n",
      "        -1.7969e-06, -4.9502e-08,  2.5728e-06,  8.6311e-07, -1.2365e-06,\n",
      "         1.4717e-06, -8.3719e-07,  1.3638e-06,  2.3467e-06,  9.7947e-07,\n",
      "         1.4566e-06, -6.1911e-07, -1.2536e-06, -3.4683e-07, -2.7425e-07,\n",
      "        -1.9427e-07, -7.0702e-07,  5.0597e-07, -1.3524e-06, -7.9008e-07,\n",
      "         3.2644e-06,  1.2680e-06, -2.3798e-07, -1.2690e-06,  2.6932e-07,\n",
      "         1.6691e-06, -2.2937e-06, -5.9364e-07, -1.3987e-06,  2.1966e-06,\n",
      "         3.3211e-07, -1.2989e-06,  7.0755e-07,  5.2524e-07,  1.7798e-06,\n",
      "        -9.1967e-07,  1.2407e-06, -1.3587e-06, -1.2187e-06,  1.2166e-07,\n",
      "        -7.7013e-07, -1.4468e-06, -1.4696e-06, -7.0312e-07,  1.3951e-06,\n",
      "         7.4161e-07, -2.2313e-07, -1.6406e-06,  1.2570e-06,  1.8906e-06,\n",
      "         1.5150e-06, -2.0509e-09, -1.3733e-06, -1.9672e-07, -2.7383e-06,\n",
      "         5.7931e-07,  1.0555e-07,  4.8413e-07,  4.4389e-08,  1.1364e-07,\n",
      "         8.1565e-08, -1.1105e-06, -1.2169e-06, -4.9547e-07,  2.5643e-07,\n",
      "         3.4970e-07, -9.7899e-07,  2.8024e-06,  6.0343e-07,  5.2533e-06,\n",
      "         3.5037e-07,  4.8889e-07,  2.3999e-07,  1.4513e-06, -3.0711e-07,\n",
      "        -8.3674e-07, -5.1170e-07,  7.9555e-07, -9.1888e-07,  3.8924e-07,\n",
      "         1.5512e-06, -1.0839e-06, -1.0279e-06, -7.4302e-07, -6.4298e-07,\n",
      "         1.6239e-07, -1.3788e-06,  1.6700e-06, -6.9277e-07,  9.3007e-07,\n",
      "         1.4794e-06,  7.4255e-07,  8.2784e-07, -1.4519e-06, -1.6461e-06,\n",
      "        -5.8888e-07,  1.9399e-06, -2.1854e-06,  6.3531e-07, -2.8288e-08,\n",
      "         5.9507e-07, -3.9940e-07,  7.5499e-07, -4.2370e-06, -1.7716e-06,\n",
      "         1.1519e-06,  2.0992e-07, -6.4065e-07,  2.6240e-07,  4.2125e-07,\n",
      "        -2.2004e-07, -3.7009e-06, -1.4140e-06, -1.0709e-06,  1.3277e-07,\n",
      "         4.0465e-07,  3.6323e-06, -2.8748e-06, -5.8354e-07, -6.6952e-07,\n",
      "         8.7388e-08,  2.7059e-06,  4.0157e-07, -2.4033e-06, -6.6146e-07,\n",
      "        -8.6603e-07, -9.1630e-07,  8.3810e-07,  1.5954e-06,  1.6170e-06,\n",
      "        -2.4950e-06,  1.2021e-06,  6.7425e-07, -1.6379e-06,  1.7944e-06,\n",
      "         1.0106e-06, -3.2432e-06,  3.4503e-07,  7.3305e-07,  1.1678e-06,\n",
      "        -9.9221e-07, -8.1429e-07, -4.1739e-07,  1.5281e-06,  8.0165e-07,\n",
      "         5.4492e-07,  2.6661e-07, -5.0139e-07, -5.2824e-07,  1.7068e-06,\n",
      "        -2.4036e-07,  3.7165e-07, -2.2307e-07,  2.0766e-08,  4.8976e-07,\n",
      "         2.0226e-06, -1.3627e-06, -1.4318e-06, -5.4872e-07, -9.8517e-07,\n",
      "         1.8619e-06, -4.0602e-07,  1.3885e-06, -3.7260e-07, -5.6866e-07,\n",
      "         4.8997e-07, -4.5602e-07, -1.5839e-06,  9.3413e-08,  6.6386e-07,\n",
      "        -8.0435e-07,  2.0737e-06,  9.3588e-07, -2.0544e-06,  4.4839e-07,\n",
      "         8.6400e-07, -3.3722e-07,  3.7750e-07,  5.2483e-07, -3.3190e-07,\n",
      "         6.3830e-07, -1.5053e-06,  1.5819e-06, -7.5689e-07, -1.9630e-06,\n",
      "        -4.9181e-07, -6.9764e-07, -1.6361e-07, -4.3026e-07, -6.3363e-07,\n",
      "        -5.8914e-07, -7.2474e-07, -4.4354e-06, -6.6122e-08,  2.4213e-07,\n",
      "        -5.8998e-07, -3.0733e-06, -1.0870e-07, -7.4478e-07, -2.6163e-06,\n",
      "        -1.8563e-06,  6.3158e-07, -3.6707e-06,  4.7814e-07,  1.2328e-06,\n",
      "        -5.5472e-07,  1.6520e-07, -1.9671e-06, -2.9558e-07,  4.3678e-07,\n",
      "         1.8698e-07,  5.3735e-07]), 'exp_avg_sq': tensor([2.5917e-12, 2.8046e-12, 4.6335e-12, 2.4989e-12, 4.0398e-12, 2.7937e-12,\n",
      "        3.3405e-12, 3.0038e-12, 3.2830e-12, 5.6767e-12, 6.7946e-12, 5.6160e-12,\n",
      "        5.2523e-12, 3.3170e-12, 4.1976e-12, 4.5232e-12, 9.6070e-12, 4.5978e-12,\n",
      "        4.3119e-12, 3.7564e-12, 3.3631e-12, 2.3369e-12, 2.7656e-12, 3.2815e-12,\n",
      "        1.0976e-11, 3.7052e-12, 3.0653e-12, 2.0952e-12, 1.8832e-12, 4.7475e-12,\n",
      "        8.6685e-12, 6.8991e-12, 5.8609e-12, 3.4313e-12, 4.9938e-12, 1.6698e-11,\n",
      "        2.6126e-12, 2.0917e-12, 2.6638e-12, 4.7961e-12, 4.5046e-12, 2.7574e-12,\n",
      "        2.6441e-12, 2.8223e-12, 1.8666e-12, 4.1771e-12, 4.3289e-12, 6.3663e-12,\n",
      "        3.0612e-12, 3.1376e-12, 3.2046e-12, 2.3384e-12, 8.4916e-12, 6.6183e-12,\n",
      "        5.0959e-12, 2.7609e-12, 4.4209e-12, 2.7996e-12, 2.0226e-12, 2.1136e-12,\n",
      "        2.2435e-12, 8.6575e-12, 4.3930e-12, 2.4403e-12, 2.9677e-12, 2.5980e-12,\n",
      "        1.8953e-12, 2.5038e-12, 1.8816e-11, 4.3478e-12, 2.5230e-12, 2.1197e-12,\n",
      "        1.0631e-11, 3.0052e-12, 5.9868e-12, 8.0897e-12, 1.2212e-11, 4.0122e-12,\n",
      "        5.1433e-12, 1.0796e-11, 3.3393e-12, 4.0138e-12, 3.0086e-12, 5.2900e-12,\n",
      "        4.5924e-12, 2.8123e-12, 3.2127e-12, 3.3463e-12, 2.3783e-12, 3.9315e-12,\n",
      "        4.3014e-12, 3.7725e-12, 5.2329e-12, 3.6840e-12, 2.5392e-12, 1.1887e-11,\n",
      "        4.5880e-12, 2.3240e-12, 7.2144e-12, 4.1194e-12, 5.1739e-12, 3.2325e-12,\n",
      "        5.3284e-12, 2.8457e-12, 4.0299e-12, 9.2550e-12, 1.0765e-11, 6.9434e-12,\n",
      "        3.7167e-12, 2.6642e-12, 2.7058e-12, 3.3411e-12, 4.5188e-12, 4.6357e-12,\n",
      "        2.7584e-12, 4.0927e-12, 5.4169e-12, 3.6812e-12, 8.0610e-12, 6.7906e-12,\n",
      "        1.6400e-12, 7.9920e-12, 4.6656e-12, 2.7053e-12, 4.7710e-12, 2.5499e-12,\n",
      "        2.4084e-12, 4.0327e-12, 6.0669e-12, 1.2331e-11, 3.7327e-12, 3.8205e-12,\n",
      "        1.4978e-11, 1.9290e-11, 1.1930e-11, 2.1758e-12, 3.6660e-12, 7.0987e-12,\n",
      "        1.9888e-12, 3.8590e-12, 3.1292e-12, 2.7482e-12, 3.6390e-12, 4.0782e-12,\n",
      "        3.2868e-12, 3.3059e-12, 2.2656e-12, 7.3639e-12, 3.2689e-12, 1.9313e-12,\n",
      "        2.6006e-12, 5.9572e-12, 5.3031e-12, 2.1489e-12, 2.7471e-12, 2.4830e-12,\n",
      "        2.0129e-11, 3.6490e-12, 5.0053e-12, 3.4672e-12, 1.3297e-11, 3.4182e-12,\n",
      "        1.8853e-12, 2.1768e-12, 4.8835e-12, 3.2283e-12, 3.2898e-12, 1.3487e-11,\n",
      "        1.5493e-11, 2.4046e-12, 9.6075e-12, 2.6318e-12, 3.0645e-12, 2.6395e-12,\n",
      "        2.6990e-12, 6.3446e-12, 2.2987e-12, 2.5834e-12, 7.1637e-13, 1.0001e-11,\n",
      "        2.6425e-12, 8.6746e-12, 2.5207e-12, 7.1395e-12, 5.8353e-12, 3.6342e-12,\n",
      "        6.1863e-12, 3.1303e-12, 2.4190e-12, 6.5012e-12, 2.5967e-12, 2.8516e-12,\n",
      "        3.2797e-12, 2.7387e-12, 7.8064e-12, 1.1127e-11, 2.1532e-11, 2.7410e-12,\n",
      "        8.6793e-12, 2.2558e-12, 2.3892e-12, 8.6101e-12, 9.7689e-12, 3.2418e-12,\n",
      "        1.0302e-11, 2.3742e-12, 2.1444e-11, 6.9224e-12, 3.3487e-12, 2.8071e-12,\n",
      "        6.9212e-12, 3.4673e-12, 3.0043e-12, 2.3944e-12, 2.6540e-12, 2.5797e-12,\n",
      "        2.0399e-12, 3.9744e-12, 3.3545e-12, 1.9586e-12, 2.1127e-12, 4.6380e-12,\n",
      "        1.8320e-12, 1.1357e-11, 4.2905e-12, 1.7705e-12, 4.1408e-12, 7.4617e-12,\n",
      "        3.0460e-12, 4.6093e-12, 5.9289e-12, 2.7445e-12, 2.0397e-12, 1.9893e-11,\n",
      "        8.2586e-12, 1.1507e-11, 5.1233e-12, 3.5805e-12, 2.4325e-12, 3.9567e-12,\n",
      "        5.6259e-12, 3.9332e-12, 9.9524e-12, 3.5666e-12, 3.5629e-12, 3.0586e-12,\n",
      "        2.9737e-12, 2.8034e-12, 2.5406e-12, 3.0031e-12, 2.9191e-12, 2.2717e-12,\n",
      "        4.5799e-12, 3.6179e-12, 3.7892e-12, 4.3296e-12, 3.6491e-12, 9.5935e-12,\n",
      "        7.7162e-12, 3.7380e-12, 1.5552e-12, 1.2446e-11, 5.0983e-12, 3.6841e-12,\n",
      "        6.8434e-12, 3.0685e-12, 4.0987e-12, 2.0864e-12, 4.0333e-12, 2.4486e-12,\n",
      "        3.9060e-12, 4.2458e-12, 1.3596e-12, 2.7078e-12, 3.2212e-12, 3.9129e-12,\n",
      "        1.8304e-12, 4.1161e-12, 2.2563e-12, 3.4386e-12, 2.2961e-12, 2.6390e-12,\n",
      "        3.0562e-12, 1.0664e-11, 3.5684e-12, 7.1063e-12, 4.3896e-12, 1.8867e-12,\n",
      "        6.1421e-12, 4.6807e-12, 2.6271e-12, 5.1911e-12, 8.6626e-12, 3.8039e-12,\n",
      "        2.4650e-12, 4.6911e-12, 4.9317e-12, 1.0408e-11, 2.6596e-12, 6.1116e-12,\n",
      "        3.3018e-12, 1.9434e-12, 8.5042e-12, 5.7603e-12, 3.8592e-12, 3.2561e-12,\n",
      "        3.5036e-12, 1.9955e-12, 1.9091e-11, 5.7222e-12, 8.4765e-12, 4.5593e-12,\n",
      "        2.2953e-12, 2.8685e-12, 3.2165e-12, 3.5311e-12, 2.4918e-12, 2.7502e-12,\n",
      "        2.4122e-12, 1.9777e-12, 1.1243e-11, 9.5951e-12, 2.7734e-12, 2.7443e-12,\n",
      "        4.1617e-12, 7.2812e-12, 9.1360e-12, 2.6314e-12, 4.9251e-12, 6.7400e-12,\n",
      "        3.6807e-12, 6.7659e-12, 2.8644e-12, 4.1264e-12, 4.5016e-12, 4.2244e-12,\n",
      "        4.2282e-12, 3.6083e-12, 5.2837e-12, 2.5765e-12, 3.1090e-12, 4.5632e-12,\n",
      "        3.3493e-12, 1.9354e-12, 2.3531e-12, 2.9283e-12, 1.6933e-12, 4.9010e-12,\n",
      "        5.1269e-12, 4.9998e-12, 8.4878e-12, 3.2301e-12, 2.2850e-12, 2.6489e-12,\n",
      "        5.7379e-12, 4.8540e-12, 2.1535e-12, 2.7955e-12, 2.3231e-12, 3.0695e-12,\n",
      "        2.6297e-12, 2.4608e-12, 3.2370e-12, 2.6416e-12, 2.6931e-12, 3.3357e-12,\n",
      "        2.6432e-12, 8.2522e-12, 1.6794e-12, 2.1952e-11, 2.5122e-12, 1.7958e-12,\n",
      "        5.9039e-12, 2.6492e-12, 4.2409e-12, 2.4922e-12, 3.3331e-12, 1.9762e-12,\n",
      "        3.0948e-12, 3.4286e-12, 2.7763e-12, 2.2968e-12, 3.3241e-12, 3.2213e-12,\n",
      "        2.4816e-12, 1.8594e-12, 1.9861e-12, 5.4552e-12, 3.4234e-12, 2.0337e-12,\n",
      "        3.7376e-12, 6.2782e-12, 4.5383e-12, 3.7823e-12, 5.6703e-12, 2.8300e-12,\n",
      "        3.9946e-12, 7.4573e-12, 2.9717e-12, 2.3570e-12, 2.6185e-12, 2.6121e-12,\n",
      "        3.3080e-12, 1.1252e-11, 5.3897e-12, 3.3812e-12, 4.2651e-12, 5.3319e-12,\n",
      "        1.9890e-12, 2.8467e-12, 4.4363e-12, 8.8669e-12, 7.8289e-12, 8.3033e-12,\n",
      "        4.1602e-12, 2.0590e-12, 2.1441e-11, 2.0490e-11, 1.0977e-11, 2.5828e-12,\n",
      "        2.3630e-12, 5.9154e-12, 2.2363e-12, 1.4137e-11, 2.6357e-12, 5.2059e-12,\n",
      "        3.6714e-12, 3.1736e-12, 2.8906e-12, 5.8674e-12, 7.6684e-12, 4.6903e-12,\n",
      "        5.1828e-12, 3.4228e-12, 4.4332e-12, 3.0380e-12, 7.5712e-12, 4.4880e-12,\n",
      "        2.2038e-12, 5.6249e-12, 3.9138e-12, 1.9384e-11, 3.2414e-12, 5.0346e-12,\n",
      "        3.7779e-12, 6.4280e-12, 1.9575e-12, 3.8129e-12, 5.2727e-12, 7.2313e-12,\n",
      "        4.6820e-12, 2.6349e-12, 3.5157e-12, 1.9362e-12, 2.9585e-12, 3.7327e-12,\n",
      "        6.6464e-12, 2.3472e-12, 3.2339e-12, 3.6717e-12, 2.7245e-12, 1.7343e-12,\n",
      "        3.4025e-12, 2.8685e-12, 3.1158e-12, 3.6801e-12, 3.6748e-12, 8.8284e-12,\n",
      "        3.2541e-12, 2.7189e-12, 3.0445e-12, 5.8590e-12, 5.1291e-12, 1.1489e-11,\n",
      "        4.1960e-12, 3.0362e-12, 2.0953e-12, 2.0341e-12, 4.5122e-12, 7.7352e-12,\n",
      "        6.8125e-12, 3.4041e-12, 4.6382e-12, 2.3498e-12, 7.1188e-12, 2.8626e-12,\n",
      "        4.3817e-12, 2.8681e-12, 1.2457e-12, 3.9887e-12, 2.6369e-12, 9.1781e-12,\n",
      "        1.0174e-11, 1.1902e-12, 2.1524e-12, 1.6708e-12, 6.3950e-12, 3.1330e-12,\n",
      "        3.7599e-12, 7.8945e-12, 4.4018e-12, 4.2642e-12, 6.5151e-12, 2.9354e-12,\n",
      "        2.9791e-12, 3.8503e-12, 2.9326e-12, 6.1265e-12, 2.9590e-12, 2.5612e-12,\n",
      "        3.0460e-12, 2.6205e-12])}, 3: {'step': tensor(640.), 'exp_avg': tensor([[ 7.0795e-08,  9.0264e-09, -3.0524e-08,  ..., -1.9272e-08,\n",
      "         -2.3600e-08, -1.0380e-07],\n",
      "        [ 4.8897e-08, -2.8771e-08, -1.5103e-08,  ..., -7.0166e-09,\n",
      "         -4.9675e-08, -1.3816e-08],\n",
      "        [ 5.8517e-08,  1.8405e-08,  1.3523e-07,  ...,  6.1633e-08,\n",
      "          9.4690e-08,  4.0538e-08],\n",
      "        ...,\n",
      "        [ 2.1658e-07, -1.9564e-08,  4.1914e-08,  ...,  1.1740e-07,\n",
      "          1.5761e-07,  9.3402e-08],\n",
      "        [ 1.0380e-07, -1.3938e-07, -1.3329e-07,  ..., -1.7649e-07,\n",
      "         -1.7569e-07, -1.0897e-07],\n",
      "        [-4.7220e-07, -4.4804e-07, -3.3727e-07,  ..., -5.2375e-07,\n",
      "         -3.8703e-07, -4.7197e-07]]), 'exp_avg_sq': tensor([[2.9593e-14, 4.5859e-15, 2.4547e-15,  ..., 6.6028e-15, 3.7352e-15,\n",
      "         7.7125e-15],\n",
      "        [6.5478e-14, 3.1313e-14, 9.9852e-15,  ..., 2.2828e-14, 2.9194e-14,\n",
      "         1.3458e-14],\n",
      "        [1.8378e-14, 1.7835e-14, 6.9508e-15,  ..., 5.9884e-15, 2.7691e-14,\n",
      "         1.3824e-14],\n",
      "        ...,\n",
      "        [7.6804e-14, 7.9693e-14, 2.6767e-14,  ..., 3.5309e-14, 2.0398e-13,\n",
      "         4.4638e-14],\n",
      "        [6.9850e-14, 7.3266e-14, 4.4430e-14,  ..., 4.4880e-14, 1.5029e-13,\n",
      "         4.6378e-14],\n",
      "        [1.3676e-13, 8.6267e-14, 4.7744e-14,  ..., 9.7191e-14, 1.7489e-13,\n",
      "         1.7073e-13]])}, 4: {'step': tensor(640.), 'exp_avg': tensor([-3.4535e-07, -1.4550e-07,  1.0028e-06, -2.0219e-08,  1.4658e-06,\n",
      "         1.1280e-07,  4.6750e-06, -3.0391e-08, -1.1449e-06,  9.4942e-07,\n",
      "         5.1306e-06, -1.4558e-06, -7.3991e-07,  5.3260e-07,  8.4627e-07,\n",
      "         1.9685e-07, -2.3799e-06,  1.4159e-06, -5.5418e-08, -6.4202e-07,\n",
      "         3.4632e-07, -1.3752e-07,  4.6991e-07,  5.2676e-07, -2.0729e-07,\n",
      "        -1.8590e-06, -5.1168e-07,  2.2098e-06, -1.1347e-06,  3.4793e-07,\n",
      "        -3.9719e-07,  1.3020e-06, -2.5137e-07, -1.3658e-06, -5.2874e-08,\n",
      "        -1.1011e-06,  5.6580e-06,  5.1285e-07,  1.6704e-07, -9.8217e-07,\n",
      "         4.0865e-07,  1.0326e-06,  2.2973e-06,  1.2382e-06, -1.6368e-06,\n",
      "        -1.2467e-06, -5.8780e-07, -4.7316e-07, -1.5143e-06,  3.7746e-07,\n",
      "         6.9375e-07,  3.0134e-07, -2.8398e-06,  5.0886e-06, -4.1016e-07,\n",
      "        -1.9256e-06, -1.0437e-06, -8.9162e-07,  5.8768e-07, -9.4012e-07,\n",
      "        -2.2215e-06,  2.5794e-07, -4.0068e-06, -9.6151e-07,  1.8945e-06,\n",
      "        -1.2356e-06,  2.5779e-06,  1.2278e-07,  6.3216e-07, -6.3552e-07,\n",
      "        -3.9368e-06,  8.1678e-07,  3.1182e-07, -1.2817e-06, -7.9807e-08,\n",
      "        -1.0221e-07,  1.3502e-06, -1.5509e-07,  4.5653e-07, -1.2496e-06,\n",
      "         4.6459e-07, -1.6172e-07,  2.6476e-07, -1.2727e-08, -3.0272e-06,\n",
      "         4.0211e-07,  6.7537e-08, -2.1086e-07,  1.4112e-07,  2.6700e-06,\n",
      "         2.3319e-07,  1.5254e-06,  1.1261e-07,  6.8217e-07, -5.9364e-07,\n",
      "        -3.0104e-07, -3.2483e-06,  4.6429e-07,  1.9617e-06,  2.7927e-07,\n",
      "         2.1818e-06,  6.6730e-06, -6.0097e-07, -1.3285e-06,  1.4762e-07,\n",
      "        -5.5216e-07, -1.2432e-06,  5.4353e-07, -5.2275e-06, -1.1569e-06,\n",
      "        -6.8951e-07,  4.0118e-07, -9.3788e-07, -1.6162e-06,  4.0699e-07,\n",
      "        -2.9973e-07, -1.6347e-06,  2.9713e-07, -5.9536e-07, -1.7036e-06,\n",
      "         1.8226e-06, -8.2918e-07,  4.1097e-06,  1.0772e-06, -1.5811e-06,\n",
      "        -2.3899e-07, -1.4314e-06, -1.0368e-06,  6.3778e-07,  9.4631e-08,\n",
      "         8.5114e-08, -1.1846e-07,  5.5779e-07,  1.5429e-07,  3.1046e-07,\n",
      "        -6.9293e-08, -3.0832e-07,  7.5685e-07, -7.2551e-07, -1.9720e-08,\n",
      "        -4.3227e-07,  2.1399e-06,  2.2883e-06, -1.8642e-06, -8.7753e-07,\n",
      "         5.4421e-07,  1.1741e-06,  3.3223e-07, -8.2936e-07,  2.9437e-06,\n",
      "        -3.4795e-07, -8.3614e-08, -2.4582e-07, -1.1097e-06,  8.0781e-07,\n",
      "        -2.2078e-07,  3.6461e-06, -2.9282e-07, -2.6496e-06, -1.0442e-06,\n",
      "        -2.5164e-07,  2.7435e-06,  1.2124e-07,  1.4241e-06,  1.2828e-06,\n",
      "        -7.4056e-07,  9.5957e-07, -1.6331e-07, -5.3223e-07, -7.4335e-07,\n",
      "         6.9395e-07, -6.6013e-07, -9.9078e-07,  2.0293e-07, -7.7878e-07,\n",
      "         3.0238e-07, -1.4875e-07,  4.0674e-07, -1.4036e-06,  1.3944e-06,\n",
      "         2.7611e-06,  1.1393e-06,  2.6203e-07, -1.7380e-06, -2.5365e-07,\n",
      "        -2.6473e-06, -8.0488e-07, -6.1120e-06, -3.9396e-07,  8.6689e-07,\n",
      "         3.7137e-06, -4.7653e-07, -2.7754e-06,  2.6173e-06, -9.4054e-07,\n",
      "         1.1653e-06, -1.9359e-06,  1.9699e-07, -9.4214e-07, -6.9499e-07,\n",
      "         4.2471e-07,  4.5913e-07,  1.9234e-07, -4.4245e-08, -1.2174e-06,\n",
      "         1.7518e-06, -1.0276e-07, -3.5363e-07, -1.4654e-06, -1.8243e-07,\n",
      "        -6.3305e-07, -5.5040e-08,  3.1460e-07,  2.2745e-06, -4.3343e-07,\n",
      "         1.3939e-07, -1.7697e-07,  2.3979e-07,  8.7733e-07, -2.0717e-06,\n",
      "        -7.6813e-09,  1.3509e-06,  1.3950e-08, -3.2265e-07, -2.4977e-07,\n",
      "        -1.2544e-06,  1.3413e-06,  3.4852e-08, -3.1183e-07, -1.7228e-07,\n",
      "         3.0714e-07, -1.1262e-06, -1.2517e-07, -2.0554e-07,  7.5419e-07,\n",
      "         1.0056e-06, -3.5038e-07,  7.9515e-07,  2.7650e-07, -6.0369e-07,\n",
      "         1.3247e-07,  8.8540e-08, -2.3970e-07,  6.8571e-08,  1.0195e-06,\n",
      "        -6.3382e-07,  2.1176e-06, -1.6619e-06, -1.4054e-06,  4.2766e-07,\n",
      "        -2.4159e-06, -4.6981e-07, -9.5952e-07, -1.0299e-06, -1.2644e-07,\n",
      "        -1.2065e-06, -1.8008e-06,  6.8602e-10,  4.0438e-06,  2.0441e-07,\n",
      "        -1.6932e-07,  2.9606e-06,  2.1488e-07,  1.9656e-08,  5.5717e-07,\n",
      "         3.2883e-07, -4.3826e-07,  7.0680e-07,  2.2844e-07,  1.4331e-06,\n",
      "        -1.3658e-06, -3.2190e-07, -2.3071e-06,  1.2923e-06, -1.6785e-06,\n",
      "        -4.5731e-07,  1.3935e-06,  9.9874e-07, -6.5392e-07,  3.4522e-07,\n",
      "         5.3651e-06,  2.7329e-08,  2.1656e-06, -3.6394e-06, -1.3348e-06,\n",
      "         5.4762e-07,  1.9826e-07, -1.7226e-06,  2.0229e-06,  3.3581e-07,\n",
      "        -2.1209e-06,  2.9838e-06,  3.2036e-07, -4.8111e-07, -6.1771e-07,\n",
      "        -3.0570e-06,  3.3329e-08,  8.4749e-07, -1.5371e-07, -2.0482e-07,\n",
      "         2.4854e-08, -2.0120e-06,  6.3488e-07, -1.3760e-06, -9.6655e-08,\n",
      "        -1.1153e-06,  4.7674e-07, -1.4000e-07,  2.0108e-06, -9.1205e-07,\n",
      "         2.3033e-06,  1.6643e-06,  8.4327e-07, -4.8657e-08,  1.5124e-06,\n",
      "        -3.0483e-07, -5.5618e-07, -5.5512e-06, -1.5249e-06, -2.4402e-06,\n",
      "         3.3002e-06, -3.0903e-07,  4.2933e-07, -3.0882e-07, -1.1505e-06,\n",
      "         6.9480e-07,  1.2190e-07, -1.1638e-06,  1.1706e-06,  3.5177e-07,\n",
      "         4.0501e-07, -8.0948e-07, -4.7968e-07, -7.4905e-07, -6.8067e-07,\n",
      "         9.1219e-07,  1.7659e-06,  3.1566e-07, -2.2649e-06,  1.4585e-06,\n",
      "        -8.0839e-07,  6.0572e-07,  1.9050e-06,  2.0507e-06, -1.2091e-06,\n",
      "         3.3924e-07,  2.2203e-07, -1.9404e-06, -4.2163e-07, -4.1396e-08,\n",
      "         5.8395e-08,  7.4567e-07, -2.9197e-06, -3.0036e-06, -3.3154e-06,\n",
      "        -2.2658e-06,  3.3458e-07, -2.3053e-06,  3.6364e-06,  1.5100e-06,\n",
      "         1.7879e-06, -3.6804e-07, -1.6255e-06, -1.9481e-06,  4.2327e-07,\n",
      "         2.1364e-07,  4.3012e-06, -3.2189e-07,  1.1506e-06, -2.7157e-06,\n",
      "        -6.2285e-07, -4.5108e-06, -3.6689e-07, -4.0584e-07, -1.2050e-06,\n",
      "         1.3054e-06,  7.8290e-08,  9.2172e-07, -3.2310e-07, -4.5586e-07,\n",
      "         5.7790e-07,  3.3472e-07,  1.6826e-06, -2.1078e-06, -4.5596e-07,\n",
      "        -1.8162e-07, -6.1684e-07, -2.4394e-07, -9.0542e-07,  7.3975e-07,\n",
      "        -1.2227e-06,  6.2284e-08, -3.4468e-06,  1.7264e-06,  3.7499e-07,\n",
      "         1.5557e-06, -1.8558e-06, -4.6654e-06,  5.2200e-07,  1.6293e-06,\n",
      "         2.1468e-06, -1.8818e-07, -1.3857e-07,  2.3294e-06,  3.4820e-07,\n",
      "         4.2538e-07,  1.7554e-06, -3.5295e-07, -2.0199e-06, -3.6007e-07,\n",
      "        -7.0115e-07,  2.4287e-07, -9.3126e-07, -1.8035e-06, -4.3915e-07,\n",
      "         4.0732e-07, -6.9865e-07, -1.0570e-07, -8.7169e-07, -3.1397e-06,\n",
      "         1.6916e-06, -3.8264e-07, -5.3623e-07, -5.3900e-07,  4.5307e-07,\n",
      "         1.8409e-06,  5.6930e-07, -5.1169e-08, -3.2764e-07, -3.6637e-07,\n",
      "        -5.3842e-07, -2.0544e-06, -1.3714e-06,  2.5337e-06, -3.4986e-06,\n",
      "         5.1190e-06, -7.3210e-07,  3.1929e-07,  1.0521e-06, -4.2092e-07,\n",
      "        -1.0323e-06, -5.8329e-07, -6.3815e-07,  2.1288e-06,  2.5970e-06,\n",
      "         5.7781e-07, -8.1443e-07,  6.4011e-07, -3.0502e-07, -1.4811e-06,\n",
      "         1.6451e-06,  1.3052e-07, -2.6017e-07, -4.1304e-07, -1.1759e-06,\n",
      "        -9.2543e-07, -1.6861e-06, -3.0091e-06, -1.7992e-06,  2.3406e-07,\n",
      "        -1.7626e-06,  1.2053e-06, -1.0079e-06, -1.3491e-06,  2.7759e-07,\n",
      "        -5.6994e-07, -1.3854e-06,  6.1876e-07,  2.8277e-07,  6.6395e-07,\n",
      "        -1.7661e-06, -1.7602e-07,  1.5656e-06, -3.0676e-06, -4.5932e-06,\n",
      "        -9.3635e-08, -1.9195e-07, -2.6316e-06,  4.4021e-07,  4.2816e-07,\n",
      "        -1.5376e-07, -3.4506e-07, -1.4569e-06, -1.1581e-06, -2.4222e-06,\n",
      "         9.9124e-07,  2.2406e-06, -1.3657e-08,  8.2785e-07, -9.2837e-08,\n",
      "        -2.1809e-06, -8.2263e-08, -1.0984e-06, -2.9439e-07,  3.3114e-07,\n",
      "         1.1339e-06,  2.4473e-07, -1.0502e-06, -1.5812e-08, -3.3759e-07,\n",
      "        -5.7837e-07, -1.0649e-06,  7.7408e-08,  9.1517e-07, -1.8147e-06,\n",
      "        -9.2667e-07, -2.2050e-06,  2.9241e-07, -1.0888e-06,  1.6857e-06,\n",
      "        -9.1052e-07, -4.6464e-06]), 'exp_avg_sq': tensor([3.5860e-13, 2.0678e-12, 1.1983e-12, 2.4855e-12, 6.4008e-12, 1.0083e-12,\n",
      "        1.8560e-11, 1.6657e-13, 3.1733e-12, 3.1258e-12, 1.5290e-11, 4.0651e-12,\n",
      "        2.2463e-12, 1.7284e-12, 2.6345e-12, 1.6679e-12, 3.3712e-12, 3.2511e-12,\n",
      "        1.9456e-13, 6.6302e-13, 7.1942e-13, 1.2181e-12, 2.2263e-12, 1.5241e-12,\n",
      "        1.3604e-12, 4.2144e-12, 9.2956e-13, 9.9501e-12, 3.5473e-12, 3.4264e-12,\n",
      "        1.5149e-12, 3.5145e-12, 8.7812e-13, 6.0508e-12, 1.4567e-12, 3.4193e-12,\n",
      "        3.5212e-11, 1.5064e-12, 3.2376e-12, 5.0024e-12, 9.4005e-13, 1.6806e-12,\n",
      "        1.0698e-11, 2.9904e-12, 3.9764e-12, 7.4293e-12, 1.1502e-12, 9.5381e-13,\n",
      "        5.6748e-12, 3.4814e-13, 2.4494e-12, 2.6728e-12, 5.0273e-12, 2.0424e-11,\n",
      "        4.3835e-13, 5.7837e-12, 3.7821e-12, 4.4870e-12, 1.7887e-12, 2.4691e-12,\n",
      "        1.0538e-11, 3.2646e-13, 1.4057e-11, 3.2748e-12, 6.6647e-12, 1.4909e-12,\n",
      "        6.9145e-12, 8.5347e-13, 7.7091e-13, 3.1453e-12, 1.0543e-11, 4.0331e-12,\n",
      "        1.6838e-12, 2.2827e-12, 9.0559e-13, 4.2944e-12, 2.9087e-12, 3.1694e-12,\n",
      "        1.1073e-12, 5.6348e-12, 6.9885e-13, 8.9728e-13, 4.2086e-12, 1.8613e-12,\n",
      "        1.7853e-11, 1.0469e-12, 5.7714e-12, 5.7771e-13, 2.0085e-12, 6.4995e-12,\n",
      "        1.6626e-12, 5.8494e-12, 1.0959e-12, 2.3489e-12, 1.1244e-12, 1.4171e-12,\n",
      "        8.1519e-12, 2.4859e-12, 4.6142e-12, 1.0525e-12, 3.6941e-12, 4.3463e-11,\n",
      "        3.3043e-12, 1.0806e-11, 1.1575e-12, 1.0180e-12, 6.7720e-12, 1.1789e-12,\n",
      "        2.4282e-11, 5.0052e-12, 2.3144e-12, 1.9942e-12, 2.0279e-12, 4.2781e-12,\n",
      "        8.3867e-13, 1.1958e-12, 3.3275e-12, 3.2942e-12, 1.8635e-12, 5.5738e-12,\n",
      "        6.7034e-12, 9.2197e-13, 1.5563e-11, 2.3251e-12, 3.0684e-12, 1.7784e-12,\n",
      "        3.3292e-12, 3.1771e-12, 2.5336e-12, 1.2485e-12, 1.6280e-12, 1.7909e-12,\n",
      "        1.8775e-12, 1.0304e-12, 1.8697e-12, 2.3214e-12, 3.2129e-12, 2.8536e-12,\n",
      "        1.4382e-12, 3.7847e-12, 9.7341e-13, 6.8681e-12, 8.8386e-12, 3.7785e-12,\n",
      "        2.7621e-12, 8.9538e-13, 3.7809e-12, 2.2985e-12, 2.2374e-12, 1.7799e-11,\n",
      "        1.3235e-12, 3.1856e-12, 1.5739e-12, 3.7530e-12, 1.5044e-12, 1.6451e-12,\n",
      "        1.0586e-11, 2.7292e-12, 1.7101e-11, 4.0131e-12, 6.7704e-12, 8.9422e-12,\n",
      "        9.6204e-13, 4.0331e-12, 3.7620e-12, 1.9916e-12, 1.4573e-12, 4.2955e-13,\n",
      "        1.4589e-12, 3.5746e-12, 2.1294e-12, 1.2177e-12, 3.1860e-12, 7.5963e-12,\n",
      "        4.7107e-13, 7.3690e-13, 1.3135e-12, 2.5117e-12, 8.1533e-12, 4.2391e-12,\n",
      "        4.4936e-12, 1.7597e-12, 5.6131e-13, 8.2082e-12, 9.0269e-13, 2.9795e-12,\n",
      "        6.0212e-12, 4.4083e-11, 1.6663e-13, 2.4229e-12, 2.0375e-11, 3.7374e-13,\n",
      "        5.4341e-12, 1.6616e-11, 2.4950e-12, 2.8092e-12, 3.2396e-12, 9.8634e-13,\n",
      "        2.1191e-12, 5.4735e-12, 2.3913e-12, 8.7012e-13, 1.0186e-12, 2.4019e-12,\n",
      "        3.1153e-12, 6.6849e-12, 2.9739e-12, 2.4332e-13, 4.1662e-12, 2.2094e-12,\n",
      "        1.6870e-12, 2.3672e-12, 2.6288e-12, 1.1648e-11, 3.8298e-12, 3.5540e-12,\n",
      "        2.6732e-12, 8.1427e-13, 3.7488e-13, 2.1262e-12, 4.5489e-12, 5.9209e-12,\n",
      "        2.0366e-12, 2.5593e-12, 8.3148e-13, 2.0489e-12, 2.3144e-12, 1.1892e-13,\n",
      "        2.6138e-12, 2.4900e-12, 5.8747e-13, 1.5212e-12, 1.7724e-13, 5.2746e-12,\n",
      "        3.7414e-12, 8.0710e-12, 3.0301e-12, 2.6039e-12, 1.2722e-12, 1.3784e-12,\n",
      "        2.8371e-12, 2.5872e-12, 1.8054e-12, 3.0981e-12, 9.1315e-12, 8.1939e-13,\n",
      "        4.3513e-12, 6.2782e-12, 2.5415e-12, 2.8561e-12, 7.6321e-12, 1.3433e-12,\n",
      "        2.1928e-12, 5.0850e-12, 9.3685e-14, 7.0503e-12, 9.2031e-12, 2.9094e-13,\n",
      "        1.4241e-11, 9.9545e-13, 4.2187e-13, 9.1266e-12, 4.4210e-13, 1.3386e-12,\n",
      "        2.8780e-12, 3.9323e-13, 1.1082e-12, 9.8608e-12, 1.9070e-12, 2.1266e-12,\n",
      "        1.3056e-11, 1.8177e-12, 7.4836e-12, 1.4397e-12, 1.2637e-11, 1.0114e-12,\n",
      "        3.5089e-12, 3.8056e-12, 6.8024e-13, 2.3357e-12, 2.7879e-11, 2.3234e-12,\n",
      "        3.0013e-12, 1.5821e-11, 3.2606e-12, 4.1041e-12, 1.5278e-12, 6.3008e-12,\n",
      "        3.7109e-12, 1.2189e-12, 7.0825e-12, 1.9389e-11, 1.0190e-12, 1.1936e-12,\n",
      "        8.7469e-13, 1.2014e-11, 1.2471e-12, 1.3864e-12, 2.3010e-12, 2.7825e-12,\n",
      "        1.1937e-12, 3.1265e-12, 1.5057e-12, 5.9348e-12, 2.0958e-13, 2.6261e-12,\n",
      "        1.5742e-12, 1.3606e-12, 1.5815e-11, 3.4041e-12, 3.6746e-12, 5.1157e-12,\n",
      "        3.3306e-12, 9.7305e-13, 5.0775e-12, 2.0961e-12, 5.8335e-12, 1.0505e-11,\n",
      "        3.1812e-12, 9.8241e-12, 1.1589e-11, 2.0748e-12, 2.4372e-12, 4.0718e-12,\n",
      "        2.6261e-12, 3.6742e-12, 1.5257e-12, 4.3802e-12, 5.7711e-12, 4.2459e-12,\n",
      "        2.6365e-12, 2.4652e-12, 9.8240e-13, 1.5368e-12, 2.0116e-12, 3.2532e-12,\n",
      "        3.3625e-12, 1.8336e-12, 4.1239e-12, 3.0877e-12, 1.3956e-12, 3.5820e-12,\n",
      "        6.3156e-12, 4.9622e-12, 2.9078e-12, 2.2006e-12, 1.1347e-13, 2.8635e-12,\n",
      "        6.0108e-12, 2.0389e-12, 4.6220e-13, 3.5600e-12, 1.0513e-11, 6.4269e-12,\n",
      "        1.4394e-11, 7.3845e-12, 5.9646e-12, 1.4570e-11, 7.1455e-12, 2.0479e-12,\n",
      "        2.8212e-12, 2.9267e-13, 1.2356e-11, 5.7304e-12, 2.4872e-12, 1.8042e-13,\n",
      "        2.3145e-11, 2.7221e-12, 8.6090e-12, 3.7174e-12, 1.5944e-12, 2.5916e-11,\n",
      "        1.5542e-12, 2.7872e-12, 2.8742e-12, 2.8312e-12, 6.2008e-13, 3.8775e-12,\n",
      "        3.1662e-12, 6.0182e-12, 7.7216e-12, 1.3638e-12, 4.6611e-12, 5.7144e-12,\n",
      "        3.4113e-12, 4.7467e-13, 3.5413e-12, 2.0548e-12, 2.1611e-12, 1.7342e-12,\n",
      "        4.9818e-12, 3.0828e-12, 6.4243e-12, 4.6145e-12, 6.1662e-13, 2.9369e-12,\n",
      "        1.4861e-11, 1.1825e-11, 3.1939e-12, 4.1082e-12, 4.5838e-12, 1.3285e-12,\n",
      "        2.5259e-12, 6.1533e-12, 2.9198e-12, 1.8552e-12, 3.1964e-12, 3.0983e-13,\n",
      "        4.4876e-12, 1.8064e-12, 1.8280e-12, 1.6256e-12, 1.6652e-12, 4.7192e-12,\n",
      "        1.8186e-12, 2.1136e-12, 2.4127e-12, 1.6769e-12, 3.1138e-12, 9.4507e-12,\n",
      "        8.0572e-12, 6.8633e-13, 2.5643e-12, 2.6297e-12, 1.1152e-12, 4.4654e-12,\n",
      "        2.4431e-12, 5.8565e-13, 8.2211e-13, 1.9284e-12, 7.4771e-12, 5.2135e-12,\n",
      "        8.3451e-12, 1.2687e-11, 1.7784e-11, 3.8859e-11, 1.6310e-12, 9.7310e-13,\n",
      "        2.9719e-12, 9.6545e-13, 8.4758e-13, 2.4597e-12, 5.6364e-12, 5.1329e-12,\n",
      "        8.3541e-12, 2.1593e-12, 3.0651e-12, 1.9385e-12, 1.8911e-12, 5.1494e-12,\n",
      "        3.3529e-12, 2.6884e-12, 4.0612e-13, 2.0117e-12, 5.8206e-12, 2.9885e-12,\n",
      "        3.9591e-12, 1.3666e-11, 7.2108e-12, 2.6871e-12, 5.9476e-12, 2.4760e-12,\n",
      "        2.6091e-12, 2.1816e-12, 1.7278e-11, 3.7617e-12, 2.0746e-12, 5.5974e-13,\n",
      "        2.3761e-12, 1.0275e-12, 1.2184e-11, 1.7906e-12, 3.0463e-12, 8.3957e-12,\n",
      "        4.1066e-11, 1.7648e-12, 3.5790e-12, 1.0113e-11, 4.7140e-12, 1.3434e-12,\n",
      "        1.6053e-12, 6.3072e-13, 3.6675e-12, 2.8269e-12, 1.1072e-11, 4.0286e-12,\n",
      "        4.2198e-12, 2.0416e-13, 2.7120e-12, 2.5081e-12, 9.2077e-12, 1.3512e-12,\n",
      "        2.9207e-12, 4.5580e-12, 3.1997e-12, 2.5127e-12, 4.4731e-13, 5.9244e-12,\n",
      "        2.4765e-12, 3.3878e-12, 3.4832e-12, 2.5182e-12, 2.4030e-12, 5.1990e-12,\n",
      "        1.4294e-11, 2.0734e-12, 1.5655e-11, 3.5875e-13, 2.4775e-12, 4.6880e-12,\n",
      "        4.1985e-12, 8.3222e-12])}, 5: {'step': tensor(640.), 'exp_avg': tensor([[ 7.6026e-09, -1.2918e-07, -5.0211e-07,  ..., -4.9656e-07,\n",
      "         -2.8387e-07, -6.5045e-07],\n",
      "        [-1.7432e-10, -6.7847e-08,  1.5688e-08,  ..., -1.2584e-08,\n",
      "         -4.9849e-09, -9.4497e-09],\n",
      "        [-2.0240e-08, -1.2208e-07, -1.5588e-07,  ..., -1.3526e-07,\n",
      "         -9.1238e-08, -2.5905e-07],\n",
      "        ...,\n",
      "        [ 6.6191e-09, -1.6644e-09,  6.5425e-08,  ...,  3.3598e-08,\n",
      "          3.2147e-08,  9.6795e-08],\n",
      "        [ 1.3673e-09,  2.8779e-08, -7.3751e-09,  ...,  1.5037e-07,\n",
      "          1.1707e-08,  9.7469e-08],\n",
      "        [ 7.6058e-09, -1.8901e-08,  1.3049e-07,  ...,  4.2653e-08,\n",
      "          6.4364e-09,  9.1728e-08]]), 'exp_avg_sq': tensor([[1.3249e-14, 1.7108e-13, 1.1182e-13,  ..., 3.4287e-13, 6.5139e-14,\n",
      "         1.0728e-13],\n",
      "        [1.0943e-15, 6.4482e-15, 1.5798e-14,  ..., 4.4163e-14, 1.5646e-14,\n",
      "         1.8330e-14],\n",
      "        [4.3033e-15, 4.7310e-14, 3.0251e-14,  ..., 1.5208e-13, 4.0067e-14,\n",
      "         5.0691e-14],\n",
      "        ...,\n",
      "        [1.4729e-16, 8.5054e-16, 6.5614e-15,  ..., 2.3261e-14, 1.3391e-14,\n",
      "         2.0068e-14],\n",
      "        [3.2493e-16, 9.6163e-15, 7.3465e-15,  ..., 4.0783e-14, 2.5611e-14,\n",
      "         2.1909e-14],\n",
      "        [2.4297e-15, 8.4130e-15, 1.9112e-14,  ..., 4.8163e-14, 1.4164e-14,\n",
      "         3.5415e-14]])}, 6: {'step': tensor(640.), 'exp_avg': tensor([-4.7833e-06, -3.2353e-07, -1.5267e-06,  5.0738e-08, -5.6016e-06,\n",
      "        -5.3782e-07,  3.5036e-06,  5.5012e-07, -6.9557e-07, -1.8137e-07,\n",
      "        -3.4924e-06, -2.6129e-07,  2.7072e-07,  1.1396e-06, -1.0827e-07,\n",
      "        -2.2224e-06,  3.3912e-06, -1.4352e-07, -4.9058e-07, -2.2844e-06,\n",
      "        -8.1868e-08, -2.1005e-07,  2.9587e-06,  3.5256e-07, -2.7357e-07,\n",
      "        -6.0720e-06,  1.0991e-06, -6.9083e-07,  3.0139e-06, -2.5563e-07,\n",
      "         1.9743e-06,  6.8147e-08, -6.6231e-07,  1.0439e-06, -4.1203e-06,\n",
      "         1.0962e-06, -2.0875e-08, -2.6590e-07, -4.7523e-07,  1.2816e-06,\n",
      "        -1.4495e-06,  1.1511e-07,  5.2320e-07, -9.9639e-07, -3.5101e-07,\n",
      "        -1.3728e-06,  2.8671e-07,  1.9417e-07,  4.4833e-06,  2.7607e-06,\n",
      "        -3.0993e-06,  8.6396e-07,  9.2522e-07, -2.0230e-07, -3.0750e-06,\n",
      "        -3.0662e-07, -2.9987e-06, -4.3914e-07,  2.3019e-06, -3.5574e-08,\n",
      "         1.6028e-07,  4.1826e-09, -4.6993e-07, -2.2233e-07, -1.4972e-06,\n",
      "        -4.1945e-06,  6.4847e-07,  9.6960e-07, -8.3507e-06,  1.7660e-06,\n",
      "         4.7038e-06,  1.7393e-07,  1.1399e-07, -7.5592e-07,  2.5861e-06,\n",
      "         1.3927e-07, -6.4669e-07, -6.3777e-07, -3.6828e-06, -6.7995e-06,\n",
      "         6.7347e-07,  1.1872e-07,  3.8461e-07,  7.9244e-07,  3.6604e-07,\n",
      "        -1.9505e-06, -3.7547e-06, -6.0033e-08,  8.1003e-07,  5.0701e-07,\n",
      "        -8.4783e-07, -2.6519e-06,  4.7319e-07,  3.7995e-07, -4.5940e-07,\n",
      "         2.9335e-06, -1.0894e-06,  2.7014e-07, -9.2482e-07, -1.7934e-06,\n",
      "        -4.2568e-08, -1.0956e-06,  1.0818e-06, -8.0595e-07,  4.5282e-06,\n",
      "         3.6490e-06, -1.5502e-06,  2.4191e-06,  3.4037e-06, -2.2700e-07,\n",
      "         1.0078e-07, -8.5920e-07, -8.5540e-07,  7.9514e-07,  6.2677e-06,\n",
      "         1.5911e-06,  2.3719e-07,  2.1071e-07,  6.8197e-07,  1.5274e-06,\n",
      "         1.8360e-07,  1.3925e-09,  7.2885e-07, -2.1080e-06,  1.5572e-07,\n",
      "        -3.1532e-07, -2.4423e-06, -1.8668e-06, -1.5964e-06, -4.5297e-08,\n",
      "        -9.4753e-08, -3.1755e-07,  8.7396e-08, -5.7095e-07,  4.7278e-07,\n",
      "         9.0867e-08, -1.3786e-06,  2.9197e-06,  7.0018e-07,  1.6653e-06,\n",
      "        -3.4502e-07, -3.2974e-07, -4.5690e-08, -1.6364e-07, -3.2495e-07,\n",
      "        -1.6393e-06,  2.5558e-06,  2.6206e-07,  1.3764e-06, -1.0050e-08,\n",
      "        -5.9739e-07,  6.7694e-07,  5.7121e-07, -3.5522e-08, -1.0026e-06,\n",
      "        -1.2335e-06,  4.7880e-06, -1.2128e-07,  5.0705e-07, -5.2456e-07,\n",
      "        -3.3406e-06,  4.9496e-06, -2.0485e-06, -4.6047e-07,  1.7064e-06,\n",
      "         7.5301e-07,  6.3395e-06, -1.4488e-06, -1.3525e-08,  1.2986e-07,\n",
      "        -3.6625e-07, -5.1127e-07,  4.6133e-07, -1.2596e-06, -4.6773e-06,\n",
      "         3.1240e-06, -2.9475e-07, -2.6543e-06,  9.5073e-07,  1.4143e-06,\n",
      "        -6.8184e-07, -1.6842e-07, -3.6771e-08, -1.3131e-06,  3.4220e-07,\n",
      "        -6.1821e-06, -3.2215e-06,  6.4201e-07, -2.6579e-06, -1.0861e-06,\n",
      "        -9.2975e-09, -5.1764e-07,  8.9090e-07, -5.0425e-07,  8.9823e-07,\n",
      "         2.8969e-07, -1.3618e-06, -5.6467e-06,  2.6156e-07, -6.4849e-07,\n",
      "         1.6431e-06,  5.2237e-07,  1.5356e-06, -1.9240e-06,  2.5376e-07,\n",
      "        -5.5801e-06, -3.1514e-06,  2.4608e-06,  2.2867e-08,  9.6633e-08,\n",
      "        -9.7564e-07,  1.5025e-08,  2.5449e-06,  5.9119e-07,  1.6774e-08,\n",
      "         1.2484e-06, -2.5757e-07,  4.9076e-08,  2.9782e-06, -9.2639e-07,\n",
      "        -3.0315e-06,  2.2178e-07, -1.4450e-06,  8.1345e-07,  8.9194e-07,\n",
      "        -8.2176e-07,  2.3567e-07, -3.2819e-06, -3.3385e-06,  1.9286e-06,\n",
      "        -2.2541e-07, -1.1882e-06,  2.4507e-08, -3.4545e-06,  4.7593e-06,\n",
      "        -3.4174e-06, -5.4994e-07,  1.9626e-06, -4.4882e-06, -6.3679e-07,\n",
      "        -2.8556e-07, -1.8167e-06,  7.3189e-07, -1.2307e-06, -1.2708e-07,\n",
      "        -5.0568e-06,  2.0507e-06, -4.0932e-06, -9.8431e-07, -2.1278e-08,\n",
      "         3.8312e-06, -6.3261e-07, -1.0782e-06,  6.6844e-07,  8.2237e-07,\n",
      "         4.0004e-07]), 'exp_avg_sq': tensor([2.0865e-11, 2.5263e-12, 7.9540e-12, 1.7553e-12, 2.6172e-11, 8.3853e-13,\n",
      "        4.7718e-11, 2.8252e-12, 2.2960e-12, 2.7147e-13, 1.0987e-11, 8.4065e-13,\n",
      "        8.6989e-13, 1.0984e-11, 1.3126e-12, 1.3775e-11, 1.6170e-11, 1.3249e-13,\n",
      "        1.8841e-12, 7.7204e-12, 1.1033e-12, 4.6207e-12, 6.1568e-12, 1.5222e-12,\n",
      "        4.5853e-13, 5.0362e-11, 3.4671e-12, 1.7239e-12, 1.4506e-11, 1.4109e-12,\n",
      "        1.0780e-11, 4.4182e-13, 2.9638e-12, 3.7046e-12, 1.5900e-11, 1.7619e-12,\n",
      "        1.8188e-13, 2.6215e-12, 2.5022e-12, 1.1342e-11, 4.0367e-12, 3.7222e-13,\n",
      "        2.7921e-12, 1.2947e-12, 4.3705e-12, 4.1816e-12, 1.6749e-12, 3.0327e-13,\n",
      "        3.0420e-11, 1.3171e-11, 8.1480e-12, 1.4891e-12, 3.5512e-12, 3.0612e-12,\n",
      "        1.0749e-11, 5.0138e-12, 1.1013e-11, 4.2787e-12, 5.7503e-12, 1.5522e-12,\n",
      "        1.2205e-12, 1.0979e-12, 1.9718e-12, 2.1707e-12, 3.3570e-12, 1.7315e-11,\n",
      "        3.9309e-12, 2.5300e-12, 9.7501e-11, 1.0930e-11, 2.2103e-11, 5.0667e-12,\n",
      "        9.3039e-13, 1.6297e-12, 1.2941e-11, 8.2982e-12, 1.2490e-12, 2.2886e-12,\n",
      "        1.5772e-11, 7.4038e-11, 1.1842e-12, 1.3884e-12, 2.0804e-12, 1.0251e-11,\n",
      "        1.8619e-12, 3.3804e-12, 1.5315e-11, 8.5642e-13, 1.8288e-12, 3.2411e-12,\n",
      "        8.2677e-12, 1.3303e-11, 7.9046e-13, 5.9085e-13, 1.5080e-12, 6.5305e-12,\n",
      "        1.0272e-11, 6.5640e-13, 7.3243e-12, 1.0830e-11, 4.7648e-12, 3.4381e-12,\n",
      "        2.0333e-12, 3.2776e-12, 1.7736e-11, 8.9478e-12, 9.0749e-12, 1.2168e-11,\n",
      "        1.1615e-11, 1.5218e-11, 1.6040e-12, 3.3224e-12, 3.0424e-12, 7.4561e-12,\n",
      "        1.9745e-11, 1.3822e-11, 1.6004e-12, 7.8495e-13, 2.3013e-12, 9.9861e-12,\n",
      "        1.0325e-12, 3.0485e-15, 4.5429e-12, 4.3283e-12, 1.6357e-12, 5.0759e-13,\n",
      "        1.7371e-11, 3.6553e-12, 4.6906e-12, 2.5103e-12, 1.8446e-12, 7.5746e-13,\n",
      "        2.7584e-12, 1.1369e-12, 4.6813e-12, 6.9657e-13, 3.1222e-12, 8.3433e-12,\n",
      "        1.0297e-12, 1.2525e-11, 2.7298e-13, 1.9287e-12, 1.5141e-13, 3.9749e-12,\n",
      "        2.6173e-12, 7.6310e-12, 6.6721e-12, 1.7784e-12, 5.6650e-12, 7.4801e-14,\n",
      "        1.9591e-12, 2.4719e-12, 3.1243e-12, 7.1497e-14, 2.8910e-12, 3.3837e-12,\n",
      "        2.6929e-11, 1.1868e-12, 1.0471e-12, 2.9702e-12, 9.2511e-12, 3.5082e-11,\n",
      "        1.5154e-11, 1.5791e-12, 2.2393e-12, 1.9825e-12, 7.6765e-11, 5.5887e-12,\n",
      "        3.2548e-13, 1.8453e-12, 5.2201e-12, 1.7658e-12, 2.4126e-12, 7.3147e-12,\n",
      "        3.1037e-11, 1.2148e-11, 1.8048e-12, 2.2303e-11, 2.5712e-12, 1.4456e-12,\n",
      "        5.3475e-12, 6.1605e-13, 6.0632e-14, 6.2489e-12, 6.8312e-12, 2.5702e-11,\n",
      "        1.0462e-11, 2.9658e-12, 1.0604e-11, 6.2321e-12, 1.3191e-14, 1.5092e-12,\n",
      "        4.7302e-12, 4.2192e-13, 1.2552e-12, 8.1937e-12, 3.1376e-12, 2.2048e-11,\n",
      "        7.6893e-13, 3.7682e-12, 4.5437e-12, 1.8334e-12, 5.7165e-12, 3.6200e-12,\n",
      "        3.7851e-12, 3.7379e-11, 2.8030e-11, 4.1017e-12, 1.7101e-13, 1.2712e-12,\n",
      "        3.0927e-12, 7.7896e-13, 8.2799e-12, 7.5557e-13, 6.1898e-13, 2.6944e-12,\n",
      "        1.9864e-12, 8.5020e-12, 1.0321e-11, 5.7922e-12, 1.7354e-11, 4.0193e-13,\n",
      "        2.5442e-12, 8.6696e-12, 3.1517e-12, 1.4906e-12, 1.7725e-13, 6.1610e-12,\n",
      "        1.6005e-11, 3.8501e-12, 4.7158e-13, 6.7194e-12, 1.5964e-13, 1.3767e-11,\n",
      "        2.2382e-11, 1.4672e-11, 4.7876e-13, 1.0160e-11, 2.1203e-11, 4.0365e-12,\n",
      "        1.0876e-12, 5.9186e-12, 5.8134e-12, 7.8412e-12, 1.2331e-12, 2.2009e-11,\n",
      "        5.4995e-12, 2.7944e-11, 5.3454e-12, 2.0733e-12, 1.4781e-11, 2.0843e-12,\n",
      "        1.0299e-12, 1.4408e-12, 2.0422e-12, 3.8544e-12])}, 7: {'step': tensor(640.), 'exp_avg': tensor([[ 1.2566e-07,  3.2732e-08,  9.0539e-08,  ...,  1.3250e-08,\n",
      "          7.4335e-08,  8.6493e-08],\n",
      "        [ 1.1137e-06,  2.9017e-07,  6.9398e-07,  ...,  4.8991e-08,\n",
      "          7.4165e-07,  7.3783e-07],\n",
      "        [ 1.1151e-07,  2.5794e-09, -4.2022e-09,  ..., -9.6168e-10,\n",
      "          9.3131e-08,  6.8213e-08],\n",
      "        ...,\n",
      "        [ 3.4921e-10, -8.2750e-10, -4.8024e-10,  ...,  2.3001e-10,\n",
      "          1.0850e-09,  1.2359e-09],\n",
      "        [ 7.3767e-10,  9.3391e-10,  1.0861e-09,  ..., -1.1666e-10,\n",
      "          9.2814e-10,  4.9383e-10],\n",
      "        [ 6.7630e-08, -2.1009e-09,  4.4079e-08,  ..., -1.2981e-08,\n",
      "          3.6506e-08, -1.5083e-09]]), 'exp_avg_sq': tensor([[1.4498e-14, 1.5839e-14, 5.0375e-14,  ..., 6.6779e-15, 1.4938e-14,\n",
      "         1.3482e-14],\n",
      "        [6.1664e-13, 4.0997e-13, 2.1751e-12,  ..., 1.8019e-13, 8.4430e-13,\n",
      "         4.8878e-13],\n",
      "        [4.8374e-14, 4.0932e-14, 1.3848e-13,  ..., 3.3416e-14, 5.9025e-14,\n",
      "         6.2445e-14],\n",
      "        ...,\n",
      "        [2.8122e-17, 5.9125e-18, 6.5488e-17,  ..., 2.2330e-17, 6.8434e-17,\n",
      "         8.7046e-18],\n",
      "        [3.9603e-16, 5.1783e-16, 2.0968e-15,  ..., 4.5793e-16, 1.2330e-15,\n",
      "         3.5764e-16],\n",
      "        [1.8422e-15, 2.7413e-15, 9.2220e-15,  ..., 1.0721e-15, 1.5787e-15,\n",
      "         2.2937e-15]])}, 8: {'step': tensor(640.), 'exp_avg': tensor([ 1.1191e-06,  9.4762e-06,  6.8218e-07, -4.7875e-07, -2.7600e-07,\n",
      "         5.0639e-07,  1.1257e-06, -1.8037e-06, -1.8635e-07, -2.2309e-06,\n",
      "         5.4053e-07, -1.8229e-06,  4.3426e-07, -1.6510e-06, -1.3532e-06,\n",
      "        -1.2728e-07,  3.3960e-06,  1.1086e-06, -2.3478e-06, -4.6349e-07,\n",
      "         6.8523e-08,  1.1419e-07, -4.9367e-08, -9.9290e-07, -2.3393e-06,\n",
      "        -2.7983e-09, -2.2318e-06, -1.3789e-06, -2.2984e-06,  7.6822e-07,\n",
      "        -9.3105e-09,  7.4344e-08,  5.7421e-08,  2.2989e-08, -7.5614e-08,\n",
      "        -4.1875e-07, -1.0471e-07,  3.7121e-07, -4.4256e-06, -9.8466e-08,\n",
      "         5.9324e-08,  1.0688e-06, -8.8326e-08,  4.8814e-07,  5.4874e-07,\n",
      "         3.0422e-06,  1.3144e-06, -3.1359e-06, -2.2693e-06,  2.6372e-06,\n",
      "        -4.2232e-07,  5.6422e-07,  1.1386e-07,  4.2218e-08, -2.1233e-06,\n",
      "        -5.3119e-10,  2.6668e-07,  9.4163e-07, -2.0769e-09, -1.5626e-08,\n",
      "        -1.4887e-07,  4.1403e-09,  1.2166e-07,  4.9888e-06,  8.9205e-07,\n",
      "        -1.5288e-06, -8.3446e-06, -3.4307e-07, -1.7691e-06, -6.1725e-06,\n",
      "         1.8852e-08, -3.6112e-06,  2.5108e-07, -1.2040e-06, -4.5866e-06,\n",
      "        -9.4552e-24,  3.0033e-06,  1.7741e-09, -3.7296e-06, -5.0500e-07,\n",
      "        -5.8634e-06,  1.3443e-06, -2.6638e-06, -3.8389e-07, -1.9292e-06,\n",
      "         1.6651e-07,  7.4697e-06, -6.0170e-07, -1.0233e-06, -3.3132e-06,\n",
      "         3.7184e-07, -6.2828e-07, -6.3074e-06, -4.7593e-08,  6.1573e-06,\n",
      "        -6.3305e-07, -1.4828e-06, -9.8221e-09,  1.8125e-06, -3.7539e-08,\n",
      "         3.1922e-06, -3.1807e-06,  1.9808e-06,  2.9792e-07, -3.8905e-07,\n",
      "        -2.4774e-08, -2.9098e-06, -7.1359e-06,  3.1186e-06, -2.7478e-12,\n",
      "         1.2231e-06,  8.0076e-06,  3.1552e-07,  3.9766e-06,  7.9894e-07,\n",
      "         7.3236e-09, -7.1085e-07, -4.7754e-06,  0.0000e+00,  7.8884e-08,\n",
      "         1.9436e-06,  2.6777e-07, -9.7991e-08,  1.3779e-06, -2.1906e-06,\n",
      "         4.9618e-09,  1.1388e-08,  2.5014e-07]), 'exp_avg_sq': tensor([1.2617e-12, 5.0119e-11, 5.3577e-12, 1.6824e-12, 2.8940e-13, 1.4265e-12,\n",
      "        3.2720e-12, 3.7050e-11, 2.5439e-11, 2.6691e-11, 4.7994e-12, 1.0866e-11,\n",
      "        2.9001e-13, 3.3498e-12, 7.3877e-12, 2.1145e-13, 1.5075e-11, 9.1093e-12,\n",
      "        1.1910e-11, 2.9155e-13, 1.7443e-13, 1.1586e-13, 1.1292e-12, 7.2425e-13,\n",
      "        2.5285e-11, 1.4253e-13, 9.4004e-12, 5.9103e-12, 1.8125e-11, 8.1026e-13,\n",
      "        1.2364e-14, 1.4476e-12, 4.6071e-14, 2.8753e-14, 1.4815e-12, 2.3477e-12,\n",
      "        9.0761e-14, 7.5665e-13, 3.7891e-11, 3.5642e-13, 3.6356e-13, 8.8852e-12,\n",
      "        1.9183e-14, 1.1327e-13, 9.9722e-13, 9.0518e-12, 2.6228e-12, 3.3166e-11,\n",
      "        1.3935e-11, 1.5155e-11, 1.4835e-11, 8.0715e-13, 8.5292e-14, 2.6855e-13,\n",
      "        2.4914e-12, 3.7042e-13, 2.3037e-12, 5.0517e-12, 6.2847e-15, 2.1602e-13,\n",
      "        3.0667e-12, 3.6838e-14, 1.4293e-12, 7.4774e-11, 1.1637e-11, 7.3392e-12,\n",
      "        3.8552e-11, 2.0653e-13, 1.2614e-11, 4.4743e-11, 1.4234e-13, 2.5606e-11,\n",
      "        8.6665e-13, 2.0089e-11, 5.6727e-11, 3.5991e-17, 2.0397e-11, 3.5761e-15,\n",
      "        3.9914e-11, 9.9456e-13, 6.6037e-11, 2.6232e-12, 1.9212e-11, 3.8546e-12,\n",
      "        1.9562e-11, 9.9563e-13, 4.7300e-11, 2.2892e-12, 4.1388e-12, 1.8829e-11,\n",
      "        1.3196e-12, 1.4030e-12, 4.2968e-11, 5.4853e-15, 4.3507e-11, 6.9725e-13,\n",
      "        9.2632e-12, 2.1599e-14, 5.6336e-12, 2.8658e-13, 2.3940e-11, 1.7571e-11,\n",
      "        1.1954e-11, 4.4590e-13, 4.0284e-13, 3.1689e-14, 1.8192e-11, 7.3729e-11,\n",
      "        8.8607e-12, 8.4183e-17, 1.6618e-11, 1.0695e-10, 1.2340e-13, 2.7891e-11,\n",
      "        8.8176e-12, 5.1890e-13, 1.4485e-12, 5.5142e-11, 0.0000e+00, 1.9015e-13,\n",
      "        1.0133e-11, 4.4466e-13, 1.8534e-13, 4.9531e-12, 3.1468e-12, 2.7298e-15,\n",
      "        5.3624e-14, 2.2659e-13])}, 9: {'step': tensor(640.), 'exp_avg': tensor([[-3.1563e-05,  6.5733e-04, -1.0518e-02, -1.4710e-05, -2.1449e-05,\n",
      "         -4.5590e-06, -2.1587e-04,  5.2367e-04, -3.0275e-03,  6.0844e-05,\n",
      "          9.2831e-05, -3.5741e-05,  1.1081e-03,  2.0643e-04, -3.0177e-04,\n",
      "          1.9598e-03, -2.1060e-03,  5.7964e-04,  4.8787e-03,  1.8401e-03,\n",
      "          2.9681e-03, -8.8597e-04,  2.7597e-03,  2.4542e-03,  2.5796e-03,\n",
      "          2.6424e-04, -2.7110e-03],\n",
      "        [-7.5011e-06,  3.6966e-04, -9.4346e-03, -1.2523e-05, -1.7182e-05,\n",
      "          8.3613e-07, -1.0631e-04,  2.8566e-04, -3.1523e-03,  2.8121e-05,\n",
      "          7.9978e-06, -6.2175e-06,  3.2087e-03, -8.5499e-04, -2.9960e-03,\n",
      "         -2.0697e-03, -7.8742e-04,  1.2176e-03, -1.3524e-03, -2.6587e-03,\n",
      "          2.7388e-03,  1.3300e-03,  1.9727e-03,  1.5819e-03, -2.0517e-03,\n",
      "          2.6819e-03, -7.0446e-04],\n",
      "        [ 1.0529e-04,  4.3521e-04, -9.6378e-03,  8.2184e-08,  3.2463e-06,\n",
      "         -2.3538e-06,  2.4811e-05,  2.8528e-04, -3.2840e-03,  1.0266e-05,\n",
      "          3.6226e-05, -1.7266e-05, -1.1045e-03, -1.1648e-03,  4.1040e-04,\n",
      "         -5.6942e-04, -3.2336e-03,  2.2015e-04,  3.7796e-04, -3.2286e-03,\n",
      "         -1.0986e-04,  3.5139e-03, -1.1771e-03, -2.5210e-03,  3.6972e-03,\n",
      "          1.4160e-03,  7.6578e-04],\n",
      "        [ 1.6225e-05,  3.4347e-04, -1.1517e-02, -2.0932e-06, -1.0553e-05,\n",
      "         -2.1396e-07, -6.6171e-05,  2.3485e-04, -4.3210e-03,  6.1467e-05,\n",
      "          1.9498e-05, -1.4628e-05, -1.8420e-03, -1.9387e-03, -3.6585e-03,\n",
      "         -4.7309e-04, -5.3609e-03, -4.4627e-03,  4.6117e-05, -1.3142e-03,\n",
      "         -4.0174e-04, -1.6341e-03,  3.2339e-03,  1.1164e-03, -2.4487e-04,\n",
      "         -1.7163e-03,  1.2457e-03],\n",
      "        [-1.0730e-04,  5.7211e-04, -8.0616e-03, -1.2761e-05, -2.2982e-05,\n",
      "         -3.0732e-06, -2.6651e-04,  4.4483e-04, -2.8427e-03,  5.1423e-05,\n",
      "          8.2092e-05, -2.7829e-05,  6.4718e-04,  1.9779e-03, -1.0072e-03,\n",
      "         -6.2721e-04, -1.0945e-03,  2.4528e-03, -2.1781e-04, -2.9764e-03,\n",
      "          2.3845e-04,  2.5544e-03,  8.2937e-04, -2.4742e-03, -9.2382e-04,\n",
      "         -2.0907e-03, -1.9046e-03],\n",
      "        [ 5.0291e-05,  5.7507e-04, -1.2781e-02, -8.8366e-06, -1.9219e-06,\n",
      "         -3.3742e-06, -5.2288e-05,  4.2614e-04, -4.1279e-03,  8.1987e-05,\n",
      "          8.9919e-05, -2.1512e-05,  4.7044e-04, -2.0141e-03, -6.0003e-04,\n",
      "          1.3843e-04, -2.3649e-03, -2.2253e-03, -1.7393e-03, -6.2253e-04,\n",
      "         -4.5527e-04,  2.1933e-03,  1.8563e-03, -2.1864e-03,  1.3158e-03,\n",
      "          7.1862e-03, -8.4105e-05],\n",
      "        [ 9.9101e-05,  1.9507e-04, -8.2107e-03,  2.0619e-06,  2.0974e-06,\n",
      "         -7.9116e-08,  6.4441e-05,  1.0438e-04, -3.0096e-03,  3.4820e-05,\n",
      "          7.7823e-06, -5.0959e-06, -5.6633e-04, -4.0783e-03,  7.7808e-04,\n",
      "          1.3953e-03, -2.2824e-03, -2.0563e-03, -3.8606e-03, -2.4004e-04,\n",
      "          2.1207e-03,  2.2039e-03, -1.1682e-03,  1.9246e-03, -1.2768e-03,\n",
      "         -1.2306e-04, -2.7667e-04],\n",
      "        [-7.0428e-05,  6.6518e-04, -1.0033e-02, -2.5215e-05, -3.5018e-05,\n",
      "          1.0912e-06, -2.4793e-04,  5.1395e-04, -3.8677e-03,  2.6756e-05,\n",
      "          5.1423e-06, -7.1421e-06, -2.4960e-03, -3.7145e-03, -6.8320e-04,\n",
      "         -2.7968e-03, -8.5981e-04, -5.8613e-03, -1.3873e-03,  1.3933e-03,\n",
      "         -1.0185e-03,  7.2051e-04, -1.5152e-03,  5.8932e-03, -1.9097e-03,\n",
      "         -6.1131e-04, -4.9566e-04],\n",
      "        [-6.3592e-05,  7.4155e-05, -6.3298e-03,  4.2144e-06, -9.1488e-06,\n",
      "         -1.6157e-06, -1.2282e-04,  7.0157e-05, -2.0390e-03,  5.6365e-06,\n",
      "          3.8094e-05, -1.2112e-05,  3.5994e-03, -1.0570e-03,  2.5656e-03,\n",
      "          2.6661e-04,  1.3170e-03,  1.3918e-03, -8.3783e-04,  1.0995e-03,\n",
      "         -4.0082e-04,  2.4974e-04,  1.7998e-03, -1.4782e-03, -1.6799e-03,\n",
      "         -5.5055e-04, -3.3516e-03],\n",
      "        [ 6.8777e-06,  4.0998e-04, -9.5424e-03,  9.6024e-07,  5.7926e-06,\n",
      "         -2.9798e-06, -2.8686e-05,  2.7257e-04, -3.5174e-03,  3.8557e-05,\n",
      "          5.1354e-05, -2.1965e-05,  1.2229e-03, -1.7826e-03, -1.7266e-04,\n",
      "         -1.1346e-03, -2.5465e-04, -3.4575e-03, -7.4845e-04,  2.8659e-04,\n",
      "          8.1758e-04, -6.0085e-04, -1.5878e-03, -8.5544e-04,  2.9976e-03,\n",
      "         -5.9129e-04, -6.0358e-03],\n",
      "        [-9.3186e-06,  1.3338e-04, -1.2457e-03, -4.8904e-06, -5.0519e-06,\n",
      "         -4.8475e-08, -3.9997e-05,  1.0074e-04, -5.6436e-04,  5.0664e-06,\n",
      "          1.6802e-06, -7.2197e-07, -7.4001e-04, -5.1125e-04,  6.6053e-04,\n",
      "          2.6423e-05, -4.7440e-04, -2.4543e-04, -4.3667e-04, -6.4998e-04,\n",
      "          5.7636e-04,  2.1830e-04,  1.4523e-04, -1.0831e-04, -7.8278e-04,\n",
      "         -1.1605e-04, -8.5024e-04],\n",
      "        [-3.7304e-05,  1.0807e-04, -3.5524e-03,  3.0990e-06, -1.3677e-06,\n",
      "         -1.3895e-06, -5.4410e-05,  7.3038e-05, -1.2032e-03,  2.6228e-05,\n",
      "          1.5386e-05, -7.2466e-06, -1.3455e-04, -1.9656e-03, -8.9789e-04,\n",
      "         -1.2457e-03, -9.8130e-04, -2.2034e-04, -1.6231e-03,  1.9561e-04,\n",
      "          8.4266e-04, -6.1523e-04,  1.4193e-03,  3.6081e-04,  1.9841e-03,\n",
      "          1.1590e-03,  3.6467e-04],\n",
      "        [ 7.3414e-05,  2.1147e-04, -4.6595e-03,  2.3571e-06,  5.9387e-06,\n",
      "         -1.4320e-06,  5.0029e-05,  1.2262e-04, -1.4448e-03,  1.5784e-05,\n",
      "          4.0155e-05, -8.7906e-06,  2.0663e-04, -6.3323e-04, -1.7279e-03,\n",
      "          4.3545e-04,  1.8148e-03, -1.6468e-04, -9.1937e-04,  6.1690e-04,\n",
      "          2.8518e-03, -8.5539e-04,  1.1994e-03, -7.9297e-04,  5.9380e-04,\n",
      "          1.4366e-03,  6.2703e-04],\n",
      "        [ 9.0832e-05,  1.7613e-04, -3.1037e-03,  6.6040e-07,  8.7378e-06,\n",
      "         -4.1568e-07,  8.9719e-05,  9.7863e-05, -1.2011e-03, -1.7912e-05,\n",
      "          9.3901e-06, -1.5092e-06, -6.1356e-04,  2.1285e-03, -2.2354e-05,\n",
      "         -1.8144e-03,  6.5129e-04, -1.3469e-03, -1.4897e-03, -9.3690e-04,\n",
      "         -2.0664e-03,  3.8571e-04, -4.2531e-04,  2.8355e-04,  7.7704e-04,\n",
      "          7.6976e-04, -7.3538e-05],\n",
      "        [-1.1877e-04,  3.6363e-04, -5.1008e-03, -4.3076e-06, -2.6522e-05,\n",
      "         -2.2359e-06, -2.7647e-04,  2.7931e-04, -1.9911e-03,  3.0963e-05,\n",
      "          5.2647e-05, -2.4041e-05, -2.4677e-03, -5.6260e-04,  7.5101e-04,\n",
      "          1.8346e-03, -3.0809e-03, -3.8170e-04,  9.6341e-04,  8.2787e-04,\n",
      "          2.6181e-03, -1.1186e-03, -2.3179e-03, -1.3977e-03, -2.2307e-03,\n",
      "          2.4015e-03, -1.7202e-03],\n",
      "        [ 1.0342e-04,  6.2168e-04, -1.1911e-02, -5.8639e-06, -1.2010e-05,\n",
      "         -8.0226e-07, -4.6096e-05,  4.1081e-04, -4.1407e-03, -1.7827e-06,\n",
      "          3.1118e-05, -9.9877e-06,  5.6577e-03, -1.3201e-03,  2.6345e-03,\n",
      "         -2.1373e-03,  2.1472e-03,  3.0057e-06, -5.0625e-04, -4.7447e-04,\n",
      "          2.0245e-03, -8.6050e-04, -5.7527e-03, -1.1005e-03,  2.6614e-03,\n",
      "         -6.2481e-03,  2.0924e-03],\n",
      "        [-2.0868e-04,  3.0531e-04, -5.7285e-03,  4.2964e-07, -2.3105e-05,\n",
      "         -3.3846e-06, -3.2103e-04,  2.4137e-04, -1.9385e-03,  9.8885e-05,\n",
      "          5.3837e-05, -2.7340e-05, -8.6297e-04,  1.0878e-03, -1.3410e-03,\n",
      "         -4.3472e-03, -2.0051e-03, -2.5100e-04, -1.9519e-03, -7.1606e-04,\n",
      "          1.1369e-03,  3.7592e-03,  3.1380e-03, -3.6324e-04,  1.7916e-03,\n",
      "          4.7775e-04,  1.0332e-04],\n",
      "        [-5.5665e-05,  4.6824e-04, -7.9712e-03, -1.1081e-05, -2.0465e-05,\n",
      "         -1.8242e-06, -1.9118e-04,  3.6988e-04, -2.6508e-03,  5.5849e-05,\n",
      "          2.9086e-05, -1.4888e-05,  2.3751e-03, -8.9604e-05, -1.3067e-03,\n",
      "          2.4156e-03,  5.5606e-04, -1.7967e-03, -2.0100e-03, -1.3106e-03,\n",
      "         -9.0314e-04,  5.1973e-04, -7.2532e-04,  9.4847e-04, -2.9911e-03,\n",
      "         -3.4362e-04,  3.7058e-03],\n",
      "        [ 2.7178e-05,  1.3909e-04, -1.1368e-03, -3.6231e-06,  3.4225e-07,\n",
      "         -8.2702e-09,  1.2576e-05,  9.5587e-05, -5.3867e-04,  3.7716e-06,\n",
      "          2.3606e-06,  3.5118e-08,  4.8346e-04, -1.5554e-04,  2.4708e-04,\n",
      "         -8.1709e-05, -1.0200e-03, -3.4333e-04, -1.0964e-03, -1.2114e-04,\n",
      "          3.7353e-05, -9.8716e-04,  1.6309e-04, -6.3414e-04, -1.2812e-04,\n",
      "         -3.3095e-04, -4.0282e-04],\n",
      "        [-7.6520e-05,  4.4833e-04, -7.7836e-03, -2.7963e-06, -1.1299e-05,\n",
      "         -3.7531e-06, -1.8755e-04,  3.3267e-04, -2.1289e-03,  6.2843e-05,\n",
      "          7.5424e-05, -2.7077e-05,  1.5494e-03, -5.3764e-04,  2.8733e-03,\n",
      "         -1.8009e-03, -4.6101e-04, -5.8139e-04,  1.8676e-03,  1.3616e-03,\n",
      "          1.9592e-03,  2.7565e-04,  1.6498e-03,  1.5168e-03, -3.4306e-04,\n",
      "          4.2668e-03,  2.0156e-03],\n",
      "        [-4.0609e-05,  3.6060e-04, -6.6948e-03, -1.3276e-05, -2.9141e-05,\n",
      "          1.1618e-06, -2.0293e-04,  2.8266e-04, -2.2726e-03,  2.5392e-05,\n",
      "          1.0802e-05, -5.0316e-06,  5.1411e-04, -1.7818e-03, -2.6875e-03,\n",
      "          4.6927e-04, -1.7834e-03, -3.8728e-03,  3.1500e-03,  5.4101e-05,\n",
      "         -3.4769e-04,  9.1266e-04,  4.6055e-03,  3.5090e-04, -4.9877e-04,\n",
      "         -4.9822e-04, -3.6687e-04],\n",
      "        [-2.1871e-05,  3.9311e-04, -6.7439e-03, -5.7097e-06, -1.9812e-05,\n",
      "         -1.6174e-06, -1.6950e-04,  2.9025e-04, -2.1708e-03,  1.7736e-05,\n",
      "          2.6870e-05, -1.7446e-05,  3.8939e-05, -4.5014e-03, -2.7955e-04,\n",
      "          2.1772e-03, -8.2161e-04,  2.6116e-03, -5.5062e-04, -1.5702e-03,\n",
      "          2.0301e-04,  1.5018e-03,  1.0673e-03,  3.8241e-03,  1.6458e-03,\n",
      "         -6.3957e-04, -6.9098e-04],\n",
      "        [-4.4335e-05,  3.3245e-04, -5.8519e-03, -9.6459e-06, -1.8669e-05,\n",
      "         -8.2243e-07, -1.4988e-04,  2.5203e-04, -2.1757e-03,  1.2704e-06,\n",
      "          6.1510e-06, -1.0288e-05, -2.3714e-03,  3.2349e-04, -7.5330e-04,\n",
      "          2.6901e-04,  1.2189e-03, -1.8339e-03, -1.8730e-03, -3.1172e-03,\n",
      "          3.0249e-03, -1.1000e-03,  2.3980e-04, -7.2893e-04,  5.5588e-04,\n",
      "          9.7985e-04, -1.1236e-03],\n",
      "        [-1.6822e-04,  2.6109e-04, -5.2679e-03, -8.0185e-06, -2.5616e-05,\n",
      "         -1.6246e-06, -2.7356e-04,  2.3100e-04, -1.6488e-03,  1.5555e-05,\n",
      "          1.5783e-05, -1.5525e-05,  1.2601e-03,  9.3853e-04,  1.1053e-03,\n",
      "          9.8883e-04, -1.6506e-03,  4.4125e-04, -1.8456e-03, -6.6634e-04,\n",
      "         -8.0704e-04, -1.3641e-03,  1.2833e-03,  7.0238e-04,  1.5213e-03,\n",
      "         -8.6173e-04,  1.5373e-03],\n",
      "        [-2.1262e-04,  5.3306e-04, -1.1726e-02,  1.8580e-06, -2.3572e-05,\n",
      "         -6.9610e-06, -3.7271e-04,  3.9466e-04, -3.4813e-03,  1.8068e-04,\n",
      "          1.3521e-04, -4.5679e-05,  5.2584e-05,  5.2769e-03,  3.6605e-03,\n",
      "          4.4712e-03, -1.6044e-03,  7.7836e-04, -3.9478e-03, -1.8407e-03,\n",
      "          1.6053e-03,  2.0304e-03,  4.9628e-03,  2.4685e-04,  1.8032e-03,\n",
      "         -2.5161e-03,  2.7352e-04],\n",
      "        [-1.1105e-04,  3.7033e-04, -1.2784e-02, -1.1196e-05, -3.3832e-05,\n",
      "          5.3817e-07, -2.4880e-04,  3.0423e-04, -4.5579e-03,  2.5498e-05,\n",
      "         -7.1595e-06, -4.2354e-06, -2.0866e-03,  2.2275e-03,  2.1005e-03,\n",
      "         -5.1703e-03, -3.7873e-03, -7.1574e-04,  8.3506e-04, -1.5123e-03,\n",
      "         -2.0667e-04, -2.4816e-03, -3.1408e-04,  5.6410e-03, -7.7439e-04,\n",
      "         -2.0492e-04, -2.5498e-03],\n",
      "        [-8.5489e-06, -2.2972e-05,  7.4576e-04, -1.5803e-06, -1.5098e-06,\n",
      "          4.2658e-07, -7.5020e-06, -1.0066e-05,  2.4654e-04,  3.0303e-06,\n",
      "         -3.8697e-06,  1.6055e-06,  3.8336e-04,  4.7025e-04, -7.1660e-04,\n",
      "          7.0745e-05,  1.4337e-04, -1.7006e-05, -7.1595e-05,  2.7758e-04,\n",
      "         -1.2405e-04,  3.8488e-04, -1.5207e-04, -3.8978e-05, -2.7070e-05,\n",
      "          1.1544e-05, -4.6087e-05],\n",
      "        [-6.0222e-05,  5.6665e-05, -1.1463e-03, -2.1747e-06, -9.6307e-06,\n",
      "         -2.4380e-07, -9.3728e-05,  5.3922e-05, -4.4102e-04,  7.4024e-06,\n",
      "          6.6535e-06, -3.3040e-06,  4.9485e-04, -2.5111e-04, -4.5804e-04,\n",
      "          3.7923e-04, -7.1244e-05, -1.8146e-05, -3.5000e-04, -2.0563e-04,\n",
      "         -1.9621e-04, -9.1414e-04, -5.2877e-04,  4.7152e-05,  2.3721e-04,\n",
      "         -1.5274e-05, -1.3185e-04],\n",
      "        [-2.4710e-04,  6.9246e-04, -1.2817e-02, -2.2403e-05, -5.2379e-05,\n",
      "         -1.8448e-06, -4.9189e-04,  5.7798e-04, -4.5975e-03,  1.0361e-05,\n",
      "          2.6796e-05, -2.3906e-05,  4.3803e-05, -2.0244e-03, -1.0184e-03,\n",
      "         -3.6549e-03,  3.6692e-03, -1.5232e-03,  1.6916e-03, -3.4990e-03,\n",
      "         -1.7272e-03, -5.2009e-04,  1.4011e-03, -1.0977e-03, -3.4283e-03,\n",
      "         -1.2357e-03,  1.2847e-04],\n",
      "        [ 3.1783e-05,  4.2607e-04, -1.1946e-02, -7.7396e-07, -6.8536e-06,\n",
      "         -1.9673e-06, -6.8681e-05,  2.8502e-04, -4.0562e-03,  2.5436e-05,\n",
      "          2.3419e-05, -2.1791e-05, -1.3973e-03, -2.3093e-03, -2.6494e-03,\n",
      "         -1.1360e-03, -1.9222e-04, -1.9134e-04, -5.6413e-03,  3.9998e-03,\n",
      "          2.9075e-03, -4.0394e-04, -3.1717e-05,  1.6611e-03,  2.9597e-03,\n",
      "         -1.1205e-03,  1.8177e-04],\n",
      "        [-8.4622e-06,  3.7647e-04, -9.2370e-03, -3.5457e-06, -4.8234e-06,\n",
      "         -2.6558e-06, -7.8600e-05,  2.7049e-04, -2.7182e-03,  7.3679e-05,\n",
      "          3.1113e-05, -1.9072e-05, -7.8032e-04,  7.0741e-05,  9.5427e-04,\n",
      "          2.2428e-03,  4.6353e-04, -1.1275e-03, -1.2245e-03,  1.8273e-04,\n",
      "          3.8584e-03,  3.1927e-03, -1.8439e-03,  2.0451e-03,  6.8357e-04,\n",
      "          7.4538e-04,  1.4984e-03],\n",
      "        [ 8.4374e-05,  5.3874e-04, -1.0690e-02, -1.2285e-05, -1.5811e-05,\n",
      "          2.2520e-07, -9.3409e-05,  3.9589e-04, -4.0055e-03,  4.4073e-05,\n",
      "          2.3869e-05, -1.0051e-05, -1.0933e-03, -2.0053e-03, -1.4450e-03,\n",
      "         -2.2620e-03, -4.0309e-03,  1.5746e-03,  7.9241e-04,  9.4952e-04,\n",
      "         -2.0382e-03,  3.5381e-03, -1.4091e-03, -4.5453e-03, -2.1544e-03,\n",
      "         -1.8669e-03,  2.6854e-03]]), 'exp_avg_sq': tensor([[4.4928e-08, 6.6841e-08, 4.0563e-05, 4.2386e-10, 8.3934e-10, 8.8392e-11,\n",
      "         4.7177e-08, 4.4345e-08, 3.0567e-06, 9.9289e-09, 1.5611e-08, 5.1052e-10,\n",
      "         3.2238e-06, 3.1823e-06, 2.7296e-06, 3.9686e-06, 3.7499e-06, 3.5088e-06,\n",
      "         1.5806e-05, 4.5929e-06, 4.0089e-06, 3.7423e-06, 4.3027e-06, 4.0771e-06,\n",
      "         4.6233e-06, 3.1278e-06, 5.7966e-06],\n",
      "        [2.3286e-08, 3.4429e-08, 2.0469e-05, 2.0898e-10, 4.1521e-10, 1.7815e-11,\n",
      "         2.1881e-08, 2.1495e-08, 2.0368e-06, 1.9411e-09, 2.8214e-09, 1.3900e-10,\n",
      "         2.7715e-06, 1.9449e-06, 3.8549e-06, 2.8153e-06, 1.7313e-06, 2.2770e-06,\n",
      "         1.6772e-06, 3.1536e-06, 1.8865e-06, 1.6514e-06, 1.8057e-06, 1.6734e-06,\n",
      "         3.1281e-06, 2.8337e-06, 1.8892e-06],\n",
      "        [3.8617e-08, 5.1346e-08, 1.0087e-04, 5.5143e-10, 1.5757e-09, 1.9809e-10,\n",
      "         4.7268e-08, 3.7088e-08, 9.3103e-06, 1.1338e-08, 7.2952e-09, 7.5814e-10,\n",
      "         8.3590e-06, 6.4403e-06, 6.1504e-06, 5.5704e-06, 7.4906e-06, 8.2018e-06,\n",
      "         6.5213e-06, 8.7603e-06, 7.4546e-06, 8.7859e-06, 7.7924e-06, 1.5518e-05,\n",
      "         8.9741e-06, 5.9021e-06, 6.1929e-06],\n",
      "        [2.9324e-08, 4.5304e-08, 4.2383e-05, 3.5703e-10, 5.5824e-10, 2.3599e-11,\n",
      "         2.6002e-08, 2.8682e-08, 5.0998e-06, 6.2086e-09, 3.0034e-09, 4.5254e-10,\n",
      "         5.1134e-06, 2.5038e-06, 6.7788e-06, 2.7131e-06, 7.8583e-06, 5.1244e-06,\n",
      "         2.3269e-06, 3.1537e-06, 3.0474e-06, 4.0225e-06, 5.5912e-06, 2.5780e-06,\n",
      "         2.6679e-06, 4.0953e-06, 3.1979e-06],\n",
      "        [2.5398e-08, 4.6019e-08, 1.9963e-05, 2.8696e-10, 4.2163e-10, 3.9118e-11,\n",
      "         2.5514e-08, 3.0826e-08, 2.1443e-06, 6.2267e-09, 4.1618e-09, 3.4908e-10,\n",
      "         1.8663e-06, 2.8268e-06, 2.1157e-06, 1.6707e-06, 1.8432e-06, 4.8723e-06,\n",
      "         1.4168e-06, 3.0671e-06, 2.2494e-06, 2.5580e-06, 1.8292e-06, 4.0500e-06,\n",
      "         2.2814e-06, 2.8905e-06, 2.9421e-06],\n",
      "        [4.8006e-08, 6.6650e-08, 5.4211e-05, 4.5517e-10, 8.5099e-10, 6.0564e-11,\n",
      "         4.1464e-08, 4.2453e-08, 4.9679e-06, 6.0979e-09, 2.3827e-08, 2.2308e-10,\n",
      "         3.4857e-06, 4.0119e-06, 3.5519e-06, 3.7356e-06, 3.8233e-06, 3.3627e-06,\n",
      "         3.4031e-06, 4.1932e-06, 5.1111e-06, 3.6626e-06, 3.5797e-06, 6.0723e-06,\n",
      "         3.4721e-06, 1.8336e-05, 3.0347e-06],\n",
      "        [1.3815e-08, 1.7422e-08, 1.5803e-05, 1.2448e-10, 1.9825e-10, 1.6680e-11,\n",
      "         1.2081e-08, 1.1847e-08, 1.8110e-06, 2.9359e-09, 1.6787e-09, 1.9328e-10,\n",
      "         1.4881e-06, 5.4536e-06, 1.3430e-06, 2.2188e-06, 1.5275e-06, 1.4700e-06,\n",
      "         4.6645e-06, 1.4185e-06, 1.1783e-06, 2.3076e-06, 1.9853e-06, 1.6654e-06,\n",
      "         2.5192e-06, 1.0680e-06, 1.0917e-06],\n",
      "        [4.7830e-08, 7.9719e-08, 3.7187e-05, 5.0139e-10, 8.7662e-10, 4.4712e-11,\n",
      "         4.7718e-08, 5.3312e-08, 4.6488e-06, 3.0731e-09, 6.7610e-09, 1.9833e-10,\n",
      "         5.4487e-06, 6.4743e-06, 2.8035e-06, 5.1781e-06, 2.8071e-06, 1.0746e-05,\n",
      "         2.9346e-06, 4.1580e-06, 4.4515e-06, 2.8786e-06, 4.1489e-06, 1.2813e-05,\n",
      "         5.5456e-06, 2.6744e-06, 2.9400e-06],\n",
      "        [1.4494e-08, 1.2914e-08, 1.7778e-05, 1.4472e-10, 4.3079e-10, 4.6874e-11,\n",
      "         1.9024e-08, 9.0425e-09, 1.7962e-06, 2.5677e-09, 2.1464e-09, 1.5220e-10,\n",
      "         4.1665e-06, 1.9602e-06, 5.5423e-06, 1.7247e-06, 3.8006e-06, 2.6184e-06,\n",
      "         1.6719e-06, 2.0023e-06, 2.6881e-06, 1.5615e-06, 2.0199e-06, 2.1981e-06,\n",
      "         4.1407e-06, 2.0564e-06, 7.3573e-06],\n",
      "        [2.6856e-08, 4.0969e-08, 3.9997e-05, 2.9626e-10, 5.9856e-10, 5.8993e-11,\n",
      "         2.4956e-08, 2.6245e-08, 4.5540e-06, 1.0145e-08, 1.4062e-08, 5.4358e-10,\n",
      "         2.7349e-06, 2.8749e-06, 3.0227e-06, 5.0358e-06, 3.1495e-06, 5.0320e-06,\n",
      "         2.2916e-06, 2.6179e-06, 2.6523e-06, 2.7985e-06, 4.1795e-06, 3.0844e-06,\n",
      "         3.6356e-06, 2.3245e-06, 1.6400e-05],\n",
      "        [1.7340e-09, 2.2159e-09, 4.5580e-06, 4.6009e-11, 2.8695e-10, 2.6717e-11,\n",
      "         5.8164e-09, 2.1348e-09, 7.3428e-07, 1.8406e-10, 1.0499e-09, 4.0520e-11,\n",
      "         1.6930e-06, 4.5847e-07, 4.2692e-07, 3.8840e-07, 5.6855e-07, 1.0187e-06,\n",
      "         1.1891e-06, 7.0548e-07, 5.3501e-07, 4.5909e-07, 4.2563e-07, 5.2879e-07,\n",
      "         2.2011e-06, 5.6992e-07, 2.2710e-06],\n",
      "        [3.6451e-09, 4.0533e-09, 6.5132e-06, 6.2060e-11, 2.1331e-10, 2.9454e-11,\n",
      "         6.7318e-09, 3.0729e-09, 5.5950e-07, 4.2660e-10, 1.1283e-09, 6.1045e-11,\n",
      "         8.9739e-07, 2.1626e-06, 8.7554e-07, 1.2145e-06, 6.5255e-07, 1.5992e-06,\n",
      "         1.7825e-06, 9.3311e-07, 6.2801e-07, 6.6710e-07, 2.1178e-06, 5.7968e-07,\n",
      "         1.2102e-06, 1.0436e-06, 1.0234e-06],\n",
      "        [5.3948e-09, 8.3265e-09, 3.8607e-06, 5.0967e-11, 7.1027e-11, 9.3791e-12,\n",
      "         4.7076e-09, 4.5731e-09, 3.7528e-07, 1.2629e-09, 1.2957e-09, 5.7406e-11,\n",
      "         4.4515e-07, 5.4771e-07, 8.9476e-07, 4.9286e-07, 1.2190e-06, 7.8197e-07,\n",
      "         7.3136e-07, 9.0361e-07, 1.4176e-06, 8.9842e-07, 6.4603e-07, 8.3603e-07,\n",
      "         4.9763e-07, 6.7570e-07, 5.8539e-07],\n",
      "        [4.6437e-09, 8.6848e-09, 1.5749e-06, 4.6298e-11, 7.4308e-11, 6.7760e-12,\n",
      "         4.0260e-09, 4.4892e-09, 2.1330e-07, 7.5713e-10, 3.2334e-10, 2.2922e-11,\n",
      "         3.7665e-07, 1.0815e-06, 3.0239e-07, 8.1106e-07, 4.3457e-07, 4.3128e-07,\n",
      "         5.8029e-07, 3.9198e-07, 1.4650e-06, 3.9364e-07, 4.2862e-07, 3.5947e-07,\n",
      "         3.4671e-07, 6.1384e-07, 3.8025e-07],\n",
      "        [2.3382e-08, 2.2724e-08, 1.1545e-05, 2.5729e-10, 6.1619e-10, 6.5203e-11,\n",
      "         3.1043e-08, 1.7809e-08, 1.3062e-06, 9.5823e-09, 1.0316e-08, 5.0638e-10,\n",
      "         3.6205e-06, 1.6201e-06, 1.8986e-06, 2.8208e-06, 4.0783e-06, 2.3909e-06,\n",
      "         3.2776e-06, 2.2494e-06, 2.5321e-06, 2.8848e-06, 5.0260e-06, 2.4416e-06,\n",
      "         5.4973e-06, 2.5870e-06, 2.5540e-06],\n",
      "        [7.0723e-08, 1.0596e-07, 5.5137e-05, 7.4819e-10, 1.6040e-09, 1.2088e-10,\n",
      "         6.9721e-08, 6.6346e-08, 6.0855e-06, 1.8064e-08, 7.0617e-09, 8.7953e-10,\n",
      "         1.0161e-05, 4.5551e-06, 7.5649e-06, 6.6129e-06, 8.0086e-06, 5.0971e-06,\n",
      "         4.4775e-06, 5.3640e-06, 4.7398e-06, 5.2438e-06, 2.3167e-05, 8.1665e-06,\n",
      "         6.2336e-06, 2.0519e-05, 6.4998e-06],\n",
      "        [2.0685e-08, 1.9714e-08, 1.0060e-05, 2.2899e-10, 6.9335e-10, 7.5489e-11,\n",
      "         3.3814e-08, 1.4929e-08, 1.0222e-06, 7.4558e-09, 2.1873e-08, 3.8933e-10,\n",
      "         1.7568e-06, 1.6562e-06, 1.5248e-06, 7.9316e-06, 2.0632e-06, 1.1037e-06,\n",
      "         2.9639e-06, 1.2325e-06, 1.4095e-06, 5.5651e-06, 4.8506e-06, 1.6145e-06,\n",
      "         1.8622e-06, 1.8021e-06, 1.7736e-06],\n",
      "        [2.4572e-08, 3.3590e-08, 2.2269e-05, 3.4784e-10, 8.0733e-10, 9.0208e-11,\n",
      "         2.9518e-08, 2.4538e-08, 2.0534e-06, 2.4702e-09, 5.0683e-09, 2.1454e-10,\n",
      "         2.7216e-06, 1.6705e-06, 1.8818e-06, 3.9295e-06, 2.2549e-06, 1.9168e-06,\n",
      "         2.1378e-06, 2.1599e-06, 3.3540e-06, 1.6687e-06, 2.1427e-06, 1.6296e-06,\n",
      "         5.9949e-06, 1.6159e-06, 4.8087e-06],\n",
      "        [1.8262e-09, 2.0878e-09, 5.0304e-06, 6.4853e-11, 4.1349e-10, 3.7424e-11,\n",
      "         7.5175e-09, 2.3597e-09, 6.3118e-07, 2.8760e-10, 1.6795e-09, 7.1004e-11,\n",
      "         1.0312e-06, 6.2635e-07, 1.2978e-06, 4.9226e-07, 2.7586e-06, 6.0399e-07,\n",
      "         2.0599e-06, 8.1897e-07, 6.9757e-07, 1.3584e-06, 5.4466e-07, 1.6256e-06,\n",
      "         8.7288e-07, 8.6160e-07, 8.2459e-07],\n",
      "        [2.4478e-08, 3.0347e-08, 1.5483e-05, 2.5543e-10, 4.9115e-10, 5.4967e-11,\n",
      "         2.8899e-08, 2.0543e-08, 1.0534e-06, 4.1539e-09, 3.8410e-09, 2.6068e-10,\n",
      "         1.9403e-06, 1.3874e-06, 4.5893e-06, 2.4247e-06, 1.7044e-06, 1.6225e-06,\n",
      "         2.3515e-06, 2.3518e-06, 1.8674e-06, 1.4877e-06, 2.1768e-06, 1.7233e-06,\n",
      "         2.1332e-06, 4.4994e-06, 2.8631e-06],\n",
      "        [1.8687e-08, 2.4868e-08, 1.0691e-05, 2.2073e-10, 4.5671e-10, 2.6344e-11,\n",
      "         2.4651e-08, 1.8076e-08, 1.1272e-06, 1.4691e-09, 1.9211e-09, 1.0627e-10,\n",
      "         1.4939e-06, 1.5344e-06, 2.9045e-06, 1.3120e-06, 1.2220e-06, 3.7009e-06,\n",
      "         3.7212e-06, 1.2131e-06, 1.6314e-06, 1.5178e-06, 6.0644e-06, 1.1758e-06,\n",
      "         1.8846e-06, 1.3900e-06, 1.2452e-06],\n",
      "        [1.7209e-08, 3.3277e-08, 1.2861e-05, 2.2844e-10, 4.1475e-10, 4.7472e-11,\n",
      "         2.1229e-08, 2.2656e-08, 1.1765e-06, 5.1451e-09, 1.7964e-09, 2.9483e-10,\n",
      "         1.8425e-06, 5.1290e-06, 1.5670e-06, 2.8609e-06, 1.4536e-06, 5.0719e-06,\n",
      "         1.7245e-06, 2.0515e-06, 1.9726e-06, 1.7771e-06, 1.7394e-06, 4.8856e-06,\n",
      "         2.5715e-06, 2.0425e-06, 1.5344e-06],\n",
      "        [9.3283e-09, 1.6799e-08, 8.4710e-06, 1.0746e-10, 1.8923e-10, 1.3546e-11,\n",
      "         1.0394e-08, 1.0588e-08, 9.9541e-07, 1.8441e-09, 1.0490e-09, 1.0573e-10,\n",
      "         2.2460e-06, 8.6471e-07, 8.1624e-07, 7.6673e-07, 1.6738e-06, 1.1829e-06,\n",
      "         1.6459e-06, 2.5769e-06, 2.2790e-06, 1.3301e-06, 7.2026e-07, 9.7484e-07,\n",
      "         8.1652e-07, 7.7683e-07, 9.4519e-07],\n",
      "        [1.1557e-08, 1.1497e-08, 9.4532e-06, 1.1809e-10, 2.9021e-10, 2.9120e-11,\n",
      "         1.4548e-08, 7.8519e-09, 7.8898e-07, 3.2321e-09, 1.6094e-09, 2.0327e-10,\n",
      "         9.4663e-07, 1.1512e-06, 1.1517e-06, 1.4340e-06, 9.8659e-07, 1.1395e-06,\n",
      "         2.1055e-06, 8.6799e-07, 1.7291e-06, 1.5010e-06, 1.3058e-06, 7.8834e-07,\n",
      "         1.0739e-06, 1.2911e-06, 1.8288e-06],\n",
      "        [6.9884e-08, 6.8472e-08, 6.1345e-05, 8.1048e-10, 1.4979e-09, 1.5749e-10,\n",
      "         7.1554e-08, 4.8201e-08, 4.5422e-06, 1.4619e-08, 1.4015e-08, 1.0750e-09,\n",
      "         4.6234e-06, 2.0463e-05, 1.0802e-05, 1.8304e-05, 4.1682e-06, 5.7754e-06,\n",
      "         7.8725e-06, 4.8893e-06, 4.2245e-06, 4.4396e-06, 1.3466e-05, 4.6405e-06,\n",
      "         4.8968e-06, 8.6750e-06, 4.1532e-06],\n",
      "        [6.9210e-08, 6.6212e-08, 8.4415e-05, 7.3504e-10, 1.9903e-09, 1.2727e-10,\n",
      "         7.9778e-08, 4.7674e-08, 9.2987e-06, 4.3869e-09, 7.3647e-09, 4.7253e-10,\n",
      "         1.0085e-05, 7.3678e-06, 5.2690e-06, 1.6220e-05, 6.3296e-06, 5.1295e-06,\n",
      "         5.3173e-06, 5.1353e-06, 6.6521e-06, 1.3215e-05, 6.4726e-06, 1.6726e-05,\n",
      "         6.5623e-06, 4.5921e-06, 8.4119e-06],\n",
      "        [4.1752e-09, 3.0560e-09, 2.2968e-05, 7.4337e-11, 3.4216e-10, 4.5683e-11,\n",
      "         7.8155e-09, 2.8453e-09, 1.9901e-06, 7.9185e-10, 1.4167e-09, 6.0348e-11,\n",
      "         6.5920e-06, 4.0842e-06, 9.1132e-06, 1.3073e-06, 1.9647e-06, 1.5206e-06,\n",
      "         1.7997e-06, 3.1808e-06, 1.3388e-06, 5.8019e-06, 1.9475e-06, 1.5437e-06,\n",
      "         2.2424e-06, 2.5108e-06, 1.4498e-06],\n",
      "        [2.7287e-09, 1.0907e-09, 1.1404e-05, 6.8461e-11, 4.2970e-10, 6.2050e-11,\n",
      "         9.8627e-09, 2.2031e-09, 1.2872e-06, 1.0715e-09, 4.8174e-09, 1.0446e-10,\n",
      "         2.3968e-06, 9.2184e-07, 1.8091e-06, 1.6521e-06, 8.4641e-07, 9.1368e-07,\n",
      "         1.2200e-06, 7.5919e-07, 3.8452e-06, 6.2526e-06, 3.0753e-06, 8.6228e-07,\n",
      "         6.8555e-07, 8.6256e-07, 1.0113e-06],\n",
      "        [7.2047e-08, 8.6229e-08, 7.5461e-05, 7.4900e-10, 1.6186e-09, 1.2067e-10,\n",
      "         8.3681e-08, 6.5575e-08, 8.3066e-06, 1.0198e-08, 8.3717e-09, 6.1207e-10,\n",
      "         5.2124e-06, 5.5394e-06, 4.4441e-06, 1.1014e-05, 1.9850e-05, 3.9687e-06,\n",
      "         5.3831e-06, 7.7450e-06, 8.7957e-06, 5.8551e-06, 4.5255e-06, 5.5330e-06,\n",
      "         1.1309e-05, 4.3062e-06, 4.5641e-06],\n",
      "        [3.6238e-08, 5.7107e-08, 4.4080e-05, 5.0521e-10, 6.8760e-10, 4.0910e-11,\n",
      "         3.5531e-08, 3.9063e-08, 4.2943e-06, 1.2486e-08, 3.6205e-09, 7.2839e-10,\n",
      "         4.3479e-06, 2.9317e-06, 5.1825e-06, 3.1552e-06, 2.7125e-06, 2.2741e-06,\n",
      "         1.0004e-05, 1.0262e-05, 3.4094e-06, 3.0188e-06, 2.6499e-06, 2.7239e-06,\n",
      "         4.3965e-06, 3.6997e-06, 2.5739e-06],\n",
      "        [2.5633e-08, 3.4908e-08, 3.5423e-05, 2.6329e-10, 4.9949e-10, 6.4602e-11,\n",
      "         2.4611e-08, 2.2300e-08, 2.6870e-06, 5.2790e-09, 3.0292e-09, 4.3004e-10,\n",
      "         2.5134e-06, 2.0997e-06, 3.7005e-06, 3.3039e-06, 2.1655e-06, 2.8982e-06,\n",
      "         2.6579e-06, 3.6315e-06, 4.6214e-06, 4.4657e-06, 3.6802e-06, 2.7726e-06,\n",
      "         6.3228e-06, 2.6584e-06, 2.4935e-06],\n",
      "        [4.9317e-08, 6.5145e-08, 4.7395e-05, 5.9640e-10, 1.1141e-09, 7.7340e-11,\n",
      "         5.2298e-08, 4.2501e-08, 5.7084e-06, 9.6471e-09, 6.2004e-09, 6.8557e-10,\n",
      "         4.9537e-06, 4.2119e-06, 3.8142e-06, 5.5232e-06, 6.9075e-06, 6.7729e-06,\n",
      "         3.0413e-06, 4.6905e-06, 6.8388e-06, 5.6310e-06, 5.2629e-06, 1.5824e-05,\n",
      "         4.8207e-06, 4.5570e-06, 6.2487e-06]])}, 10: {'step': tensor(640.), 'exp_avg': tensor([-0.0184, -0.0176, -0.0181, -0.0220, -0.0153, -0.0233, -0.0160, -0.0206,\n",
      "        -0.0128, -0.0181, -0.0028, -0.0070, -0.0097, -0.0076, -0.0121, -0.0251,\n",
      "        -0.0127, -0.0152, -0.0026, -0.0150, -0.0144, -0.0139, -0.0121, -0.0105,\n",
      "        -0.0239, -0.0246,  0.0016, -0.0023, -0.0243, -0.0227, -0.0162, -0.0214]), 'exp_avg_sq': tensor([1.3365e-04, 6.5778e-05, 2.8246e-04, 1.3672e-04, 6.9637e-05, 1.7389e-04,\n",
      "        5.5246e-05, 1.3835e-04, 6.1722e-05, 1.2431e-04, 1.6105e-05, 1.9734e-05,\n",
      "        1.4674e-05, 9.3134e-06, 5.8810e-05, 2.3820e-04, 4.4808e-05, 8.2586e-05,\n",
      "        2.1064e-05, 5.4031e-05, 5.0924e-05, 5.1670e-05, 3.2076e-05, 3.4157e-05,\n",
      "        2.2924e-04, 2.8068e-04, 6.1603e-05, 3.7892e-05, 2.4729e-04, 1.4090e-04,\n",
      "        9.3608e-05, 1.7314e-04])}, 11: {'step': tensor(640.), 'exp_avg': tensor([[ 3.1276e-05,  1.2794e-05,  1.5012e-06,  ...,  1.5383e-04,\n",
      "          1.0239e-04,  1.1454e-05],\n",
      "        [-5.8533e-04, -8.3685e-04, -6.2774e-04,  ..., -8.3929e-04,\n",
      "         -5.3692e-04, -8.3499e-04],\n",
      "        [ 5.1086e-05,  6.0845e-05,  5.1128e-05,  ...,  7.8283e-05,\n",
      "          3.4558e-05,  4.6342e-05],\n",
      "        ...,\n",
      "        [ 7.8669e-04,  4.9076e-04,  2.7413e-04,  ...,  7.5680e-04,\n",
      "          3.1618e-04,  4.1602e-04],\n",
      "        [ 4.9939e-04,  2.9051e-04,  5.8139e-05,  ...,  3.2466e-04,\n",
      "          2.3842e-04,  1.5628e-04],\n",
      "        [-9.7271e-03, -1.1551e-02, -9.3714e-03,  ..., -1.0073e-02,\n",
      "         -8.9924e-03, -9.7092e-03]]), 'exp_avg_sq': tensor([[8.6761e-08, 4.0737e-08, 1.2374e-08,  ..., 1.1362e-06, 4.4696e-07,\n",
      "         4.1547e-08],\n",
      "        [2.3825e-07, 4.4356e-07, 5.1568e-07,  ..., 5.7877e-07, 3.5185e-07,\n",
      "         7.0459e-07],\n",
      "        [2.8342e-07, 5.2951e-07, 5.5313e-07,  ..., 6.4147e-07, 2.8900e-07,\n",
      "         4.5924e-07],\n",
      "        ...,\n",
      "        [8.3505e-06, 4.8147e-06, 4.1310e-06,  ..., 1.1658e-05, 7.4638e-06,\n",
      "         6.8187e-06],\n",
      "        [3.0220e-06, 2.2502e-06, 5.6833e-07,  ..., 2.9232e-06, 2.8989e-06,\n",
      "         2.4957e-06],\n",
      "        [5.6294e-05, 5.7805e-05, 6.3181e-05,  ..., 5.2088e-05, 4.9009e-05,\n",
      "         7.5977e-05]])}, 12: {'step': tensor(640.), 'exp_avg': tensor([ 9.8411e-05, -1.1443e-03,  9.7391e-05, -1.5438e-02,  2.7289e-07,\n",
      "         8.2380e-04,  1.3302e-05,  8.8138e-05, -1.2169e-02, -2.9277e-05,\n",
      "        -3.2966e-03,  2.1927e-05,  1.2830e-08, -1.1691e-02,  1.0187e-03,\n",
      "        -6.1238e-03, -9.9998e-03,  3.7886e-04, -3.5994e-04, -2.6158e-04,\n",
      "        -1.0595e-02, -1.1679e-02,  1.9969e-03, -1.0065e-02,  1.4740e-04,\n",
      "         1.1650e-04, -1.0534e-02,  4.0889e-04,  3.6303e-04,  7.1816e-04,\n",
      "         4.3598e-04, -1.7120e-02]), 'exp_avg_sq': tensor([1.3727e-06, 2.8461e-06, 3.3441e-06, 3.4455e-04, 2.4276e-07, 2.7121e-05,\n",
      "        6.1439e-07, 3.1819e-06, 2.1143e-04, 7.3205e-08, 3.3607e-06, 3.4477e-07,\n",
      "        2.7591e-09, 1.1053e-04, 2.8372e-04, 9.6566e-06, 5.5945e-05, 2.7256e-05,\n",
      "        2.1283e-06, 3.3767e-08, 8.5731e-05, 7.9770e-05, 1.4720e-04, 6.8599e-05,\n",
      "        5.8614e-06, 2.4545e-06, 7.8824e-05, 9.8177e-06, 1.8618e-05, 4.3697e-05,\n",
      "        1.5353e-05, 2.6567e-04])}, 13: {'step': tensor(640.), 'exp_avg': tensor([[-2.0039e-05, -2.1191e-06, -3.1374e-05, -6.3363e-06, -3.6814e-05,\n",
      "         -8.9748e-07, -1.1234e-06, -2.2313e-05,  1.3047e-05, -2.4020e-05,\n",
      "         -7.7987e-06, -1.6422e-05, -1.7502e-06, -5.1826e-06, -3.7590e-05,\n",
      "         -2.4495e-06, -9.3495e-06, -8.7975e-06, -1.2702e-05, -8.0675e-06,\n",
      "         -3.3895e-07, -3.9954e-06,  3.0387e-07, -2.2397e-05, -1.4172e-05,\n",
      "         -7.6121e-06, -2.7545e-05, -7.2327e-06, -1.8176e-05, -3.4701e-06,\n",
      "          1.2276e-07,  1.1528e-07, -8.7126e-06, -2.5349e-07,  1.1830e-06,\n",
      "         -2.0841e-06, -1.1673e-05, -3.0875e-06, -1.8405e-05, -2.5740e-06,\n",
      "         -7.7706e-07,  8.6110e-07, -1.3035e-06, -2.1689e-05, -1.0669e-05,\n",
      "          9.8662e-07, -4.7670e-05, -1.0318e-05, -5.6424e-05, -3.1655e-05,\n",
      "          1.0077e-05, -1.7182e-05, -1.2160e-06, -1.7687e-05, -4.2282e-05,\n",
      "         -4.0758e-06,  1.9631e-06, -3.4942e-05,  6.1225e-09, -1.9668e-06,\n",
      "          6.7683e-07,  2.4925e-07,  8.6388e-07, -7.3240e-06,  5.9969e-06,\n",
      "         -2.3905e-05, -3.3908e-05, -2.1202e-05, -6.1304e-05, -3.6737e-05,\n",
      "         -1.6909e-06, -6.6156e-05,  1.0749e-08, -1.0909e-05, -3.6943e-05,\n",
      "         -4.0350e-22,  3.1817e-06, -2.0794e-07, -4.9535e-05, -1.0422e-05,\n",
      "         -5.4420e-05, -4.0003e-06, -2.3260e-05, -5.7168e-06, -1.1027e-05,\n",
      "         -5.6113e-06, -8.9517e-06, -2.5598e-06, -2.4679e-05, -1.5025e-05,\n",
      "         -1.1841e-06, -5.6061e-06, -5.6287e-05,  1.8685e-07, -3.5782e-06,\n",
      "         -2.4748e-05, -1.5734e-05,  1.0615e-07, -1.2125e-05,  1.0882e-06,\n",
      "          5.2522e-06, -1.7213e-05, -2.2428e-05, -7.3644e-07, -3.1239e-05,\n",
      "          1.8886e-06, -2.9597e-05, -4.9390e-05, -1.0581e-05,  3.6292e-11,\n",
      "          8.2463e-06, -2.5034e-06, -2.2582e-05, -7.1786e-06, -2.3281e-05,\n",
      "          4.9178e-06, -2.1668e-05, -5.1683e-05,  0.0000e+00, -1.8873e-06,\n",
      "         -5.0789e-06, -3.2324e-06, -4.0615e-05,  1.2006e-06, -3.0132e-05,\n",
      "         -6.6585e-07, -1.7272e-06, -8.1158e-07]]), 'exp_avg_sq': tensor([[9.5873e-09, 2.2296e-09, 5.4482e-09, 7.4329e-10, 1.7747e-08, 4.2394e-11,\n",
      "         9.5956e-10, 2.7850e-09, 7.4157e-10, 4.9973e-09, 3.9729e-09, 2.2669e-09,\n",
      "         3.3579e-11, 4.3279e-10, 1.2371e-08, 3.4673e-10, 4.3850e-09, 2.8824e-09,\n",
      "         3.4510e-10, 2.2123e-10, 1.2315e-11, 2.1490e-10, 4.2245e-10, 1.8655e-09,\n",
      "         8.4732e-10, 3.8878e-09, 4.5972e-09, 6.4165e-10, 8.4521e-09, 2.7837e-09,\n",
      "         1.8603e-11, 8.9834e-11, 1.1773e-09, 7.0098e-12, 3.3427e-10, 1.4780e-10,\n",
      "         1.2798e-09, 5.6969e-10, 8.8133e-10, 2.1647e-09, 1.7975e-10, 1.6831e-09,\n",
      "         6.8344e-12, 3.6756e-09, 2.2607e-09, 3.9881e-10, 3.4228e-08, 6.8696e-10,\n",
      "         1.4464e-08, 8.4710e-09, 4.2395e-10, 7.6225e-09, 1.1430e-11, 6.5561e-09,\n",
      "         2.6196e-09, 1.0556e-10, 9.9838e-10, 1.6068e-08, 8.0970e-13, 4.9885e-09,\n",
      "         1.1595e-09, 1.0907e-11, 1.6813e-10, 2.8264e-09, 6.4330e-10, 4.3494e-09,\n",
      "         1.0323e-09, 8.1218e-09, 2.5626e-08, 1.2978e-09, 1.3604e-10, 2.4980e-08,\n",
      "         8.1683e-10, 7.6867e-10, 3.3727e-09, 2.0996e-13, 3.3893e-09, 1.0429e-12,\n",
      "         1.4684e-08, 2.1551e-09, 9.3000e-09, 2.3519e-09, 1.1711e-09, 1.6168e-09,\n",
      "         2.5057e-09, 1.9573e-09, 2.9894e-09, 1.8127e-10, 1.0201e-08, 7.8226e-10,\n",
      "         1.2946e-10, 2.9208e-10, 3.4275e-09, 2.3962e-12, 2.6378e-09, 6.6754e-09,\n",
      "         4.1077e-09, 7.5335e-11, 2.6446e-09, 1.9808e-11, 5.5961e-10, 1.5651e-09,\n",
      "         9.3314e-09, 2.5097e-11, 6.6662e-09, 8.0632e-11, 2.7474e-09, 2.7856e-09,\n",
      "         2.2427e-09, 1.8061e-14, 1.6784e-09, 2.7437e-09, 2.6286e-09, 3.1587e-09,\n",
      "         3.8932e-09, 2.5101e-09, 7.4619e-09, 3.4051e-09, 0.0000e+00, 2.3241e-10,\n",
      "         2.1631e-09, 3.4045e-10, 2.0368e-08, 1.3173e-10, 2.5137e-09, 1.5429e-11,\n",
      "         3.0234e-10, 1.2472e-09]])}, 14: {'step': tensor(640.), 'exp_avg': tensor([-0.0001]), 'exp_avg_sq': tensor([1.7968e-07])}, 15: {'step': tensor(640.), 'exp_avg': tensor([[-6.7626e-05, -2.3544e-02, -2.2630e-03, -1.2824e-01, -3.9756e-04,\n",
      "         -1.6056e-03, -7.4620e-04, -2.8318e-04, -1.7347e-01, -4.6352e-04,\n",
      "         -6.3962e-02, -5.4005e-04, -4.7276e-10, -1.5341e-01, -3.8923e-04,\n",
      "         -1.3977e-01, -1.4651e-01, -1.0534e-03, -8.7564e-03, -2.1205e-03,\n",
      "         -1.4576e-01, -1.4773e-01, -4.7876e-03, -1.4634e-01, -2.2022e-04,\n",
      "         -9.7753e-04, -1.5198e-01, -5.6879e-04, -2.1329e-03, -9.6175e-04,\n",
      "         -1.1908e-03, -1.2377e-01]]), 'exp_avg_sq': tensor([[5.0723e-07, 7.1632e-04, 1.3313e-04, 9.4503e-03, 6.2333e-06, 9.7722e-05,\n",
      "         1.6502e-05, 4.1159e-05, 1.9275e-02, 2.2513e-06, 1.4075e-03, 7.5750e-06,\n",
      "         9.8521e-09, 9.3449e-03, 2.0593e-04, 5.5383e-03, 7.6787e-03, 2.3527e-04,\n",
      "         2.7561e-04, 3.9484e-06, 8.9538e-03, 6.7443e-03, 1.3793e-03, 8.9278e-03,\n",
      "         1.2117e-05, 4.0836e-05, 9.3256e-03, 1.5479e-05, 4.2644e-04, 1.4939e-04,\n",
      "         7.1853e-05, 5.3397e-03]])}, 16: {'step': tensor(640.), 'exp_avg': tensor([-0.0387]), 'exp_avg_sq': tensor([0.0062])}}, 'param_groups': [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-05, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}]}}\n"
     ]
    }
   ],
   "source": [
    "from utils import PPO_policy_update\n",
    "new_params = PPO_policy_update(PPO_Model=PPO_model, policy_net_update=PPO_params, value_net_update = None)\n",
    "print(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_nested_module_params(module, param_dict):\n",
    "    # Initialize an empty dictionary to hold the nested modules\n",
    "    nested_dict = {}\n",
    "\n",
    "    # Recursively populate the nested dictionary\n",
    "    def add_params_to_module(module, param_names):\n",
    "        for name, param in param_names.items():\n",
    "            if '.' in name:\n",
    "                # Split the name to get the module path\n",
    "                parts = name.split('.')\n",
    "                module_name = parts[0]\n",
    "                param_name = '.'.join(parts[1:])\n",
    "\n",
    "                # Create or get the nested module\n",
    "                if module_name not in nested_dict:\n",
    "                    nested_dict[module_name] = nn.ModuleDict()\n",
    "                \n",
    "                # If the nested module doesn't exist, create it\n",
    "                if param_name:\n",
    "                    if param_name not in nested_dict[module_name]:\n",
    "                        nested_dict[module_name][param_name] = param\n",
    "            else:\n",
    "                # If there is no '.' in the name, it's a direct parameter of the module\n",
    "                if name not in nested_dict:\n",
    "                    nested_dict[name] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: 2.5\n",
      "b1: 2.0\n",
      "w2: 2.5\n",
      "b2: 2.0\n",
      "Loss: 1.53125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Initialize weights and biases as nn.Parameter\n",
    "        self.w1 = nn.Parameter(torch.tensor(0.5))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.w2 = nn.Parameter(torch.tensor(0.5))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute hidden layer output\n",
    "        h = self.w1 * x + self.b1\n",
    "        # Compute output layer output\n",
    "        y = self.w2 * h + self.b2\n",
    "        return y\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "def criterion(y,t):\n",
    "    return 0.5 * (y-t)**2\n",
    "\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([1.0], requires_grad=False)  # Ensure x does not require gradients\n",
    "t = torch.tensor([2.0])\n",
    "\n",
    "# Forward pass\n",
    "y = model(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(y, t)\n",
    "\n",
    "\n",
    "# Print out the gradients\n",
    "with th.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        param.add_(2)\n",
    "        print(f\"{name}: {param.item()}\")\n",
    "\n",
    "# Print the computed loss\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import multiply_and_sum_tensors\n",
    "import torch as th\n",
    "def calculate_g(\n",
    "    policy_params,\n",
    "    log_probs: list[th.Tensor],\n",
    "    returns: th.Tensor) -> list[th.Tensor]:\n",
    "        grads = []\n",
    "        for pi in log_probs:\n",
    "            print(pi)\n",
    "            grad_tuple = th.autograd.grad(outputs=pi, inputs=policy_params, grad_outputs=th.ones_like(pi))\n",
    "            grads.append(grad_tuple)\n",
    "        print(grad_tuple)\n",
    "        return multiply_and_sum_tensors(scalar_tensor=returns, tensor_lists=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action1: tensor([0.9623], grad_fn=<AddBackward0>)\n",
      "mean and std: (tensor([0.6201], grad_fn=<ViewBackward0>), tensor([1.0505], grad_fn=<AddBackward0>))\n",
      "Log probabilities: [tensor(-1.0213, grad_fn=<MulBackward0>), tensor(-1.3880, grad_fn=<MulBackward0>)]\n",
      "[(tensor(-0.9519), tensor([[-0.0940, -0.1881]]), tensor([-0.0940]), tensor([[-0.5655]]), tensor([-0.6190])), (tensor(-0.9115), tensor([[-0.0209, -0.0314]]), tensor([-0.0105]), tensor([[-0.6015]]), tensor([-0.6072]))]\n",
      "Parameter containing:\n",
      "tensor(0., requires_grad=True) Parameter containing:\n",
      "tensor([[0.5406, 0.5869]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.1657], requires_grad=True) Parameter containing:\n",
      "tensor([[0.9186]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.2191], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from policies import GaussianMLPPolicy\n",
    "import torch as th\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "policy = GaussianMLPPolicy(input_size=2, output_size=1, hidden_layers=[1])\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "x2 = torch.tensor([2.0, 3.0])\n",
    "action1 = policy.get_action(x)[0]\n",
    "print(\"action1:\", action1)\n",
    "mean1, std1 = policy.forward(x)\n",
    "print(\"mean and std:\", policy.forward(x))\n",
    "# Compute log probabilities\n",
    "log_probs = [policy.get_log_prob(x, action1), policy.get_log_prob(x2, policy.get_action(x2)[0])]\n",
    "\n",
    "print(\"Log probabilities:\", log_probs)\n",
    "grad_list = [th.autograd.grad(outputs=pi, inputs=policy.parameters(), grad_outputs=th.ones_like(pi)) for pi in log_probs]\n",
    "print(grad_list)\n",
    "#grads = torch.autograd.grad(outputs=log_probs, inputs=policy.parameters(), grad_outputs=torch.ones_like(log_probs))\n",
    "\n",
    "print(*policy.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from policies import GaussianMLPPolicy\n",
    "testModel = GaussianMLPPolicy(input_size=27, hidden_layers=[512, 512, 256, 128], output_size=1)\n",
    "paramList = [param for param in testModel.parameters()]\n",
    "paramList[0] = paramList[0].unsqueeze(dim=0)\n",
    "paramList[1] = paramList[1].unsqueeze(dim=0)\n",
    "\n",
    "\n",
    "print(len([name for name, param in testModel.named_parameters()][2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "summationTerm = 0\n",
    "print(len(action1))\n",
    "for mean, std, act in zip(mean1, std1, action1):\n",
    "\n",
    "    summationTerm += (act-mean)**2/std**2 + 2*torch.log(std)\n",
    "\n",
    "total = -0.5 * (summationTerm + 1 * np.log(2*np.pi))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = -0.5 * (torch.sum((action1-mean1)**2/std1**2 + 2 * torch.log(std1)) + len(action1) * np.log(2 * np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.tensor([1,1,1,1])\n",
    "test2 = torch.tensor([2,2,2,2])\n",
    "torch.sum(test2+test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 2, 3]), tensor([7, 8]), tensor([ 9, 10, 22])]\n"
     ]
    }
   ],
   "source": [
    "from utils import multiply_tensors_in_place, subtract_lists_of_tensors, add_lists_of_tensors\n",
    "import torch\n",
    "test = [torch.tensor([1,2,3]), torch.tensor([7,8]), torch.tensor([9,10,22])]\n",
    "multiply_tensors_in_place(test, scalar=1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1,  0, -1]), tensor([-5, -6]), tensor([ -7,  -8, -20])]\n"
     ]
    }
   ],
   "source": [
    "u_r = [torch.tensor([1,1,1]), torch.tensor([1,1]), torch.tensor([1,1,1])]\n",
    "otherG = [torch.tensor([1,1,1]), torch.tensor([1,1]), torch.tensor([1,1,1])]\n",
    "momentum_term = subtract_lists_of_tensors(add_lists_of_tensors(u_r, otherG), test)\n",
    "print(momentum_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [.5 * gc + (1 - .5) * mt for gc, mt in zip(test, momentum_term)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1., 1., 1.]), tensor([1., 1.]), tensor([1., 1., 1.])]\n"
     ]
    }
   ],
   "source": [
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
