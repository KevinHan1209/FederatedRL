{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.HoverAviary import HoverAviary\n",
    "from gym_pybullet_drones.envs.MultiHoverAviary import MultiHoverAviary\n",
    "from gym_pybullet_drones.utils.utils import sync, str2bool\n",
    "from gym_pybullet_drones.utils.enums import ObservationType, ActionType, Physics\n",
    "\n",
    "from policies import GaussianMLPPolicy\n",
    "from server import Federated_RL\n",
    "\n",
    "DEFAULT_GUI = True\n",
    "DEFAULT_RECORD_VIDEO = True\n",
    "DEFAULT_OUTPUT_FOLDER = 'results'\n",
    "DEFAULT_COLAB = False\n",
    "DEFAULT_DYNAMICS = Physics('pyb') # pyb: Pybullet dynamics; dyn: Explicit Dynamics specified in BaseAviary.py\n",
    "DEFAULT_WIND = np.array([0, 0.05, 0]) # units are in induced newtons\n",
    "DEFAULT_OBS = ObservationType('kin') # 'kin' or 'rgb'\n",
    "DEFAULT_ACT = ActionType('one_d_rpm') # 'rpm' or 'pid' or 'vel' or 'one_d_rpm' or 'one_d_pid'\n",
    "DEFAULT_AGENTS = 2\n",
    "DEFAULT_MA = False\n",
    "DEFAULT_MASS = 0.037 # Actual default is 0.027\n",
    "\n",
    "DR = False\n",
    "MASS_RANGE = [0.027, 0.042] # Maximum recommended payload is 15g\n",
    "WIND_RANGE = 0.005 # Inspired by literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: 2\n",
      "Action size: 1\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.171763861893404\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.17346065572630612\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.005362994037568569\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.39753802590085113\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.36363732162007156\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.9476621747016907\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5340036662265137\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.336897565710252\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.1627451330423355\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.6949878879876834\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.21003997561239066\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.01786182075738907\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5640703590498615\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.19349280762841492\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.3062681257724762\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4016449059453464\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.4490075262245611\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.0871442556381226\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.484993041649269\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -2.048714175637513\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.3266823344410875\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.4911016686943243\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.1822826862335205\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.6394214627270147\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.5475388271047704\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.389696516969819\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.8680979587925683\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.7371792881963987\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.34991166141238184\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.11595755815505981\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.8696633209178705\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.107876385006403\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.464084323658297\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.4728215331927783\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.7393612861633301\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4456500146451501\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.13287061639140685\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.05789504572749138\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.21332595958217476\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.14316489174102073\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.4409632384777069\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.1324804186052795\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.19371566031900359\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.0861384868621826\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4825602377051693\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.10735967855563532\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.07199453562498093\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.21743769786590122\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.1578560851408614\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.5438176393508911\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.29101169633282764\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.25174300737618654\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 0\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.17557047248084837\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.07696146604641128\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.2796999514102936\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4145075735468517\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.08950070327936313\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.08522680401802063\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.3509406209792151\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.30351055076050704\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.474259614944458\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.2189101376146953\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.10563005854391537\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.31612440943717957\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.245572806965591\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.11417754269889337\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.3871590197086334\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5567534963955224\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.3792499094744618\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.4114788770675659\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.13775541156719212\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.3298462185804424\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5182957183712991\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.2907066833852604\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.3272547721862793\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.40821198644995205\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.23130459715692148\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.4125249981880188\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.40704420924515844\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.2643873997834015\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.43860989809036255\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5149921572842552\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.6744547854525275\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.2152485847473145\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.20099457952119104\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.17351949928145563\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.5393208861351013\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.40722787383469417\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.69539246147525\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.33370158436042985\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.16582134153351194\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.4629128575325012\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.2798291272184601\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.3302033698441423\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.996997594833374\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.3295449218200976\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.2639641628433524\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.5101801753044128\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.3453024532986254\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.14361079582266548\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.2229486107826233\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.19308084511361692\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.8815989043509622\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.40343133431199707\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.509691820061412\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.16440026304430153\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.6954819764418714\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 1\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.11257502871254059\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.23997248558804457\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.44809787920640226\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.11151342335218999\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.01801810972392559\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.23662886203450018\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.16155406875217224\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.4966597557067871\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.1117022003184083\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.4355892958050578\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.2298050608251528\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.6317005260165645\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.06399895076579287\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.6945868246858979\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.227645951927871\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.35639879447483624\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.0034918009769171476\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.09641549681447824\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.5860000241000991\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.19364290116879512\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.3757158053997582\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.1478746305517934\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.48208339728316635\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.21789938839358694\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.3743481138735807\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.053387878306641\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.20023758010837142\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.010051199235022068\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.6981293226256702\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.22279493190187463\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.14122414588928223\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4581798537098473\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.565665479359883\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.7703707208531797\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.0219220610994573\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.5275366902351379\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5988621992882551\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.4935349438039407\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.07226146736572284\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.9653413189151685\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.07761583455486064\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.627543732684596\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.2128923323197978\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -2.795526728872192\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.0939281485394393\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.8507826115172732\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 2\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.15930120208110785\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -4.983227966599516\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.45154134968373294\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.6756506517470354\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.157684564590454\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.6502449678792962\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.5396819949074082\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.4024388790130615\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.6568207930601277\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.6876208049922617\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.620685875415802\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.197493999094426\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.9050015284872155\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.05611211434006691\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.1955294026858141\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.9407903264135833\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.35048506523116485\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.3705389615534225\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4860392968402781\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -3.3592534624856407\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.4323708750284071\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.9069884657312559\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.554083414363084\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1.727161183957992\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.007512860000133514\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.21449151760883703\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -12.904035650714011\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.1747116363428782\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.2943044882893684\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.006877098232507706\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.15066384983807424\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -4.861874041880493\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5544176333064921\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -18.604394061061594\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.37670643933215275\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -3.832559336826353\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.547580450195076\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -50.97648950218719\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.24053782933093298\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -128.54357955952932\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5711019489081277\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -322.35362855532816\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.5647175631045442\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -128.07502872977622\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -0.3334123336853533\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -864.1243037568504\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 3\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -1.2725259417522274\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -7174.511605035982\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -10.484148831577446\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -9.298666980636213\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.35017237067222595\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -4.520181107114242\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -100.46471563577246\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -3.4161146002698186\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -33.28361878850366\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -2.1045205653949735\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -517.818954152312\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -6.556075414201246\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1944.519730125789\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -4.770590717020014\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -3088.43342874395\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -13.762175289731214\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -14599.834948535414\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -5.024349631807733\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -51508.64155188551\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -7.687030419717598\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -173792.05447101148\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 1\n",
      "Acquiring global rollout\n",
      "Total Reward: -22.142553161215915\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -492778.40027863893\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 0\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -12.982852043821385\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -0.7358295979708026\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 0.352425754070282\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 1\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -5.4810530080964215\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -620.6120068276517\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 2\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -13.146441188131915\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -575792.2025143522\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 3\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -4.948979696900379\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -1962955.7075923234\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 4\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -4.461544847505846\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -4643148.827327833\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 5\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -11.076572240174677\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -5933747.236788906\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 6\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -4.293501188615508\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -15387980.360807909\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 7\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -6.12480079176706\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -40173719.22187485\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 8\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -4.51954135869629\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -183316110.85257483\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n",
      "GLOBAL ITERATION: 4\n",
      "LOCAL ITERATION: 9\n",
      "\n",
      "TRAINING AGENT: 2\n",
      "Acquiring global rollout\n",
      "Total Reward: -7.540498708682501\n",
      "Episode length: 5\n",
      "\n",
      "Acquiring local rollout\n",
      "Total Reward: -256555478.28540698\n",
      "Episode length: 5\n",
      "\n",
      "Importance sampling weight: 1.7999999523162842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<server.Federated_RL at 0x1334e8320>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on classic gym control environment first\n",
    "envs = [gym.make('MountainCarContinuous-v0') for _ in range(DEFAULT_AGENTS)]\n",
    "env = envs[0]\n",
    "# Get the state size\n",
    "state_space = env.observation_space\n",
    "state_size = state_space.shape[0]\n",
    "# Get the action size\n",
    "action_space = env.action_space\n",
    "action_size = action_space.shape[0]\n",
    "\n",
    "layers = [16,16]\n",
    "\n",
    "print(\"State size:\", state_size)\n",
    "print(\"Action size:\", action_size)\n",
    "\n",
    "policy = GaussianMLPPolicy(input_size=state_size, output_size=action_size, hidden_layers=layers) # Will need some smarter way to initialize the policy within the model in the future\n",
    "# ASSUMING ONE ALGORITHM SO FAR. WILL IMPLEMENT GENERAL STRUCTURE FOR DIVERSIFIED ALGORITHMS LATER\n",
    "\n",
    "#### Train the model #######################################\n",
    "model = Federated_RL(policy=policy,\n",
    "                    envs=envs,\n",
    "                    num_agents = DEFAULT_AGENTS,\n",
    "                    global_iterations=5,\n",
    "                    local_iterations=10,\n",
    "                    max_episode_length=5\n",
    "                    )\n",
    "\n",
    "model.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below is unit testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 15, 18, 14, 12, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# verify returns\n",
    "rewards = [2,-3,4,2,7,0,5]\n",
    "dones = [0 for _ in range(len(rewards))]\n",
    "returns = []\n",
    "gamma = 1\n",
    "R = 0\n",
    "for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "    if done:\n",
    "        R = 0\n",
    "    R = reward + gamma * R\n",
    "    returns.insert(0, R)\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y, t)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Zero gradients, backward pass, optimizer step\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print out the gradients\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Initialize weights and biases as nn.Parameter\n",
    "        self.w1 = nn.Parameter(torch.tensor(0.5))\n",
    "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
    "        self.w2 = nn.Parameter(torch.tensor(0.5))\n",
    "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute hidden layer output\n",
    "        h = self.w1 * x + self.b1\n",
    "        # Compute output layer output\n",
    "        y = self.w2 * h + self.b2\n",
    "        return y\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "def criterion(y,t):\n",
    "    return 0.5 * (y-t)**2\n",
    "\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([1.0], requires_grad=False)  # Ensure x does not require gradients\n",
    "t = torch.tensor([2.0])\n",
    "\n",
    "# Forward pass\n",
    "y = model(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(y, t)\n",
    "\n",
    "# Zero gradients, backward pass, optimizer step\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients\n",
    "print(\"Gradients with respect to weights and biases:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.grad.item()}\")\n",
    "\n",
    "# Print the computed loss\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import multiply_and_sum_tensors\n",
    "import torch as th\n",
    "def calculate_g(\n",
    "    policy_params,\n",
    "    log_probs: list[th.Tensor],\n",
    "    returns: th.Tensor) -> list[th.Tensor]:\n",
    "        grads = []\n",
    "        for pi in log_probs:\n",
    "            print(pi)\n",
    "            grad_tuple = th.autograd.grad(outputs=pi, inputs=policy_params, grad_outputs=th.ones_like(pi))\n",
    "            grads.append(grad_tuple)\n",
    "        print(grad_tuple)\n",
    "        return multiply_and_sum_tensors(scalar_tensor=returns, tensor_lists=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action1: tensor([0.9623], grad_fn=<AddBackward0>)\n",
      "mean and std: (tensor([0.6201], grad_fn=<ViewBackward0>), tensor([1.0505], grad_fn=<AddBackward0>))\n",
      "Log probabilities: [tensor(-1.0213, grad_fn=<SumBackward1>), tensor(-1.3880, grad_fn=<SumBackward1>)]\n",
      "[(tensor(-0.9519), tensor([[-0.0940, -0.1881]]), tensor([-0.0940]), tensor([[-0.5655]]), tensor([-0.6190])), (tensor(-0.9115), tensor([[-0.0209, -0.0314]]), tensor([-0.0105]), tensor([[-0.6015]]), tensor([-0.6072]))]\n",
      "Parameter containing:\n",
      "tensor(0., requires_grad=True) Parameter containing:\n",
      "tensor([[0.5406, 0.5869]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.1657], requires_grad=True) Parameter containing:\n",
      "tensor([[0.9186]], requires_grad=True) Parameter containing:\n",
      "tensor([-0.2191], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from policies import GaussianMLPPolicy\n",
    "import torch as th\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "policy = GaussianMLPPolicy(input_size=2, output_size=1, hidden_layers=[1])\n",
    "\n",
    "# Sample data\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "x2 = torch.tensor([2.0, 3.0])\n",
    "action1 = policy.get_action(x)[0]\n",
    "print(\"action1:\", action1)\n",
    "mean1, std1 = policy.forward(x)\n",
    "print(\"mean and std:\", policy.forward(x))\n",
    "# Compute log probabilities\n",
    "log_probs = [policy.get_log_prob(x, action1), policy.get_log_prob(x2, policy.get_action(x2)[0])]\n",
    "\n",
    "print(\"Log probabilities:\", log_probs)\n",
    "grad_list = [th.autograd.grad(outputs=pi, inputs=policy.parameters(), grad_outputs=th.ones_like(pi)) for pi in log_probs]\n",
    "print(grad_list)\n",
    "#grads = torch.autograd.grad(outputs=log_probs, inputs=policy.parameters(), grad_outputs=torch.ones_like(log_probs))\n",
    "\n",
    "print(*policy.parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0213, grad_fn=<SumBackward1>)\n",
      "tensor(-1.3880, grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "grad requires non-empty inputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcalculate_g\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mcalculate_g\u001b[0;34m(policy_params, log_probs, returns)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pi \u001b[38;5;129;01min\u001b[39;00m log_probs:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pi)\n\u001b[0;32m---> 10\u001b[0m     grad_tuple \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     grads\u001b[38;5;241m.\u001b[39mappend(grad_tuple)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(grad_tuple)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/drones/lib/python3.12/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mValueError\u001b[0m: grad requires non-empty inputs."
     ]
    }
   ],
   "source": [
    "print(\"result:\", calculate_g(policy.parameters(), log_probs, th.tensor([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.3031, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "summationTerm = 0\n",
    "for mean, std, act in zip(mean1, std1, action1):\n",
    "\n",
    "    summationTerm += (act-mean)**2/std**2 + 2*torch.log(std)\n",
    "\n",
    "total = -0.5 * (summationTerm + 2 * np.log(2*np.pi))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.3031, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.5 * (torch.sum((action1-mean1)**2/std1**2 + 2 * torch.log(std1)) + len(action1) * np.log(2 * np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(torch.tensor([1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
